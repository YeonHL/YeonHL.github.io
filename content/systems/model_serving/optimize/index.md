---
title: 모델 서빙 최적화
description:
date: 2025-09-20T19:03:00
lastmod: 2025-09-20
slug: optimize
comments: true
math: false
categories:
  - Systems
tags:
  - Model-Serving
keywords:
  - Model-Serving
  - MLOps
---
## 개요

### 섹션 1.3: 모델 로딩 및 관리

특히 대규모 언어 모델(LLM)과 같은 최신 모델들은 수십에서 수백 기가바이트에 달할 수 있어, 모델 로딩 자체가 심각한 병목 현상을 유발할 수 있습니다. 예를 들어, 로딩에 30초 이상이 소요된다면 실시간 서비스에서는 용납될 수 없는 지연입니다.3 이를 해결하기 위해 정교한 모델 로딩 및 관리 전략이 필요합니다.

#### 모델 형식 표준화

모델을 서빙하기 전에, 표준화되고 최적화된 형식으로 패키징해야 합니다. ONNX와 같은 형식은 여러 프레임워크 간의 상호 운용성을 보장하며, TensorRT와 같은 도구는 특정 NVIDIA GPU 하드웨어에 맞게 모델을 최적화하여 성능을 극대화합니다.6

#### 로딩 전략

- **웜 풀 (Warm Pools):** 모델이 이미 메모리에 로드된 사전 준비된 서버 인스턴스 풀을 유지하여, 갑작스러운 트래픽 급증이나 신규 배포 시 콜드 스타트(cold-start) 지연 시간 없이 요청을 처리할 수 있도록 합니다.3
- **지능형 캐싱 (Intelligent Caching):** 동일하거나 매우 유사한 입력에 대한 추론 결과를 캐싱하여 반복적인 계산을 피합니다. 20-40%의 캐시 적중률(cache hit rate)만으로도 컴퓨팅 비용을 크게 절감하고 응답 시간을 개선할 수 있습니다.3
- **연결 관리 (Connection Management):** Keep-alive 설정 및 연결 풀링(connection pooling)을 통해 모든 요청에 대해 새로운 연결을 설정하는 오버헤드를 줄여 네트워크 효율성을 최적화합니다.3

### 4.1: 효율적인 추론을 위한 모델 압축 기술

모델의 크기와 계산 요구 사항을 압축을 통해 줄이는 것은 추론 비용을 낮추고, 지연 시간을 개선하며, 자원이 제한된 하드웨어(예: 엣지 디바이스)에서의 배포를 가능하게 하는 중요한 전략입니다.

| 최적화 기법                            | 작동 원리                                                                  | 주요 이점                                                | 핵심 트레이드오프/과제                                              | 최적 사용 사례                                                     |
| -------------------------------------- | -------------------------------------------------------------------------- | -------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------ |
| **양자화 (Quantization)**              | 모델 가중치/활성화 값의 수치 정밀도를 낮춤 (예: FP32 → INT8).              | 모델 크기 축소, 추론 속도 향상, 메모리/에너지 소비 감소. | 잠재적인 정확도 손실, 양자화된 모델의 평가 복잡성.                  | 엣지 디바이스 배포, 클라우드 추론 비용 절감.                       |
| **지식 증류 (Knowledge Distillation)** | 작은 '학생' 모델이 큰 '교사' 모델의 동작(소프트 레이블)을 모방하도록 학습. | 큰 모델의 능력을 가진 작고 효율적인 모델 생성.           | 추가적인 학습 복잡성, 'Capacity Gap' 리스크, 교사 모델 품질 의존성. | 강력한 교사 모델이 있을 때, 특정 작업에 특화된 효율적인 모델 생성. |
| **가지치기 (Pruning)**                 | 모델에서 중요도가 낮은 가중치나 연결을 제거하여 희소(sparse)하게 만듦.     | 모델 크기 감소, 특정 하드웨어에서 추론 속도 향상.        | 정확도 손실 가능성, 희소 행렬 연산을 지원하는 하드웨어 필요.        | 모델 크기가 매우 중요한 자원 제약 환경.                            |

표 3: LLM 추론 최적화 기술: 트레이드오프 분석. 이 표는 엔지니어와 아키텍트에게 실용적인 의사 결정 프레임워크를 제공하여, 다양한 압축 전략을 프로젝트의 특정 제약 조건에 따라 비교할 수 있게 합니다. 이 정보는 37-44의 내용을 종합하여 구성되었습니다.

#### 4.1.1: 양자화: 정밀도와 성능의 트레이드오프

양자화는 모델의 가중치 및/또는 활성화 값의 수치 정밀도를 낮추는 기술입니다(예: 32비트 부동소수점에서 8비트 정수로). 이는 모델 크기를 줄이고, 메모리 사용량을 감소시키며, 저정밀도 연산을 지원하는 하드웨어에서 계산을 가속화할 수 있습니다. Hugging Face는 `bitsandbytes`, `AutoGPTQ`와 같은 라이브러리를 통해 다양한 양자화 방식을 지원합니다.

주요 트레이드오프는 잠재적인 모델 정확도 손실입니다. 핵심은 <u>양자화된 모델을 평가하여 대상 사용 사례에 대해 성능이 수용 가능한 수준을 유지하는지 확인</u>하는 것입니다. 양자화 인식 학습(Quantization-Aware Training, QAT)과 같은 기술은 이러한 정확도 손실을 완화하는 데 도움이 될 수 있지만, 학습 복잡성을 추가합니다.

#### 4.1.2: 지식 증류: 작은 모델로 지능 전달

더 작은 "학생" 모델이 더 크고 유능한 "교사" 모델의 행동을 모방하도록 학습됩니다. 학생 모델은 단순히 정답 레이블로부터 학습하는 것을 넘어, 교사의 "소프트 레이블"(출력에 대한 전체 확률 분포)로부터 학습하여 더 미묘한 지식을 포착합니다.

주요 과제는 "**용량 격차(Capacity Gap)**"입니다. 교사 모델이 너무 복잡하면 매우 작은 학생 모델이 <u>효과적으로 학습할 수 있는 충분한 용량을 갖지 못해 성능이 저하</u>될 수 있습니다. 반대로, 일부 연구에서는 잘 학습된 학생 모델이 교사 모델의 학습 과정에 존재했던 <u>노출 편향(exposure bias)과 같은 문제를 피함으로써 교사를 능가할 수도 있음</u>을 보여줍니다. 이 과정은 또한 학습 복잡성과 비용을 추가합니다.

## 파트 V: 모델 서빙의 미래 궤적

이 마지막 파트는 차세대 모델 서빙 아키텍처를 형성하고 있는 새로운 패러다임과 동향을 살펴봅니다.

### 5.1: 분산 지능: 엣지 AI 및 연합 서빙 아키텍처

추론을 중앙 집중식 클라우드에서 엣지(온디바이스)로 이동시키거나, 데이터를 이동시키지 않고 분산된 데이터에서 모델을 학습시키기 위해 연합 학습을 사용하는 것이 새로운 패러다임으로 부상하고 있습니다.

- **엣지 AI(Edge AI):**
  - **동기:** 낮은 지연 시간의 실시간 의사 결정, 데이터 프라이버시 강화(데이터가 디바이스에 머무름), 클라우드 의존성 및 에너지 소비 감소를 가능하게 합니다.
  - **아키텍처:** 일반적으로 엣지 디바이스(예: 스마트폰, 센서), 엣지 서버(게이트웨이), 그리고 클라우드(모델 학습 및 관리용)의 3계층 아키텍처를 포함합니다. 자원이 제한된 하드웨어에서 실행하기 위해 고도로 최적화되고 압축된 모델이 필요합니다.
- **LLM을 위한 연합 학습(Federated Learning, FL):**
  - **동기:** 민감한 데이터를 중앙 집중화하지 않고 <u>여러 클라이언트(예: 병원, 은행)에 걸쳐 공유 모델을 학습함으로써 데이터 프라이버시 및 희소성 문제를 해결</u>합니다.
  - **과제:** LLM의 엄청난 크기는 전통적인 FL(전체 모델 가중치 교환)을 비현실적으로 만듭니다. 핵심은 FL을 <u>LoRA와 같은 파라미터 효율적 미세 조정(PEFT)과 결합</u>하는 것입니다. 여기서 작고 가벼운 어댑터 업데이트만 교환하여 통신 및 계산 비용을 대폭 절감합니다. 다른 과제로는 클라이언트 간 데이터 이질성, 보안, 통신 효율성 등이 있습니다.
## 섹션 3: 효율성의 추구: 다층적 최적화 접근법

학습된 AI 모델을 프로덕션 환경에 그대로 배포하는 것은 비효율적이고 비용이 많이 드는 경우가 대부분입니다. 모델의 크기가 클수록 <u>더 많은 메모리</u>를 차지하고, <u>연산량이 많을수록 추론 속도는 느려집니다.</u> 따라서 모델 서빙의 성능을 극대화하고 비용을 최소화하기 위해서는 배포 전후 단계에서 다층적인 최적화 과정이 필수적입니다. 최적화는 크게 <span style="background:#fff88f">모델 자체를 변경하는 '모델 수준 최적화', 추론 실행을 가속화하는 '추론 엔진 최적화', 그리고 런타임 환경을 개선하는 '서빙 수준 최적화'</span>로 나눌 수 있습니다.

### 3.1. 모델 수준 최적화: 배포 전 모델의 물리적 크기 축소

이 단계의 목표는 모델의 성능 저하를 최소화하면서 모델의 크기와 연산 복잡도를 줄이는 것입니다.

- **양자화 (Quantization):** 모델의 <span style="background:#fff88f">가중치와 활성화 값의 수치 정밀도를 낮추는 기술</span>입니다. 예를 들어, 32비트 부동소수점(FP32)을 8비트 정수(INT8)로 변환하는 것입니다.
  - **영향:** 모델의 메모리 점유 공간을 약 4분의 1로 줄이고, INT8 연산을 지원하는 하드웨어(예: NVIDIA Tensor Core)에서 추론 속도를 크게 향상시킬 수 있습니다.
  - **방법:**
    - **학습 후 양자화 (Post-Training Quantization, PTQ):** 이미 <u>학습된 모델에 적용</u>하는 비교적 간단한 방법입니다. <span style="background:#fff88f">구현이 쉽지만, 정밀도 손실로 인한 모델 정확도 하락이 발생</span>할 수 있습니다.
    - **양자화 인식 학습 (Quantization-Aware Training, QAT):** 모델 <u>학습 과정에서 양자화로 인한 오차를 미리 시뮬레이션하여 모델이 이에 적응하도록 학습</u>시킵니다. 더 복잡하지만 일반적으로 <span style="background:#fff88f">더 높은 정확도를 유지</span>할 수 있습니다.
- **프루닝 (Pruning / Sparsity):** 신경망에서 <u>중요도가 낮은 연결(가중치)을 식별하고 제거하여 가중치 값을 0으로 만드는 기술</u>입니다.
  - **영향:** <u>불필요한 파라미터를 제거하여 모델 크기를 줄이고</u>, 희소성(Sparsity)을 활용할 수 있는 하드웨어에서는 <span style="background:#fff88f">연산량을 감소시켜 추론 속도를 높일 수</span> 있습니다. 이는 마치 나무의 잔가지를 쳐내어 더 가볍고 효율적으로 만드는 것과 같습니다.
- **지식 증류 (Knowledge Distillation):** 크고 복잡하지만 성능이 좋은 '교사 모델(Teacher Model)'의 지식을 작고 효율적인 '학생 모델(Student Model)'에게 전달하여 학습시키는 기법입니다. 학생 모델은 교사 모델의 예측 결과(Soft Label)를 모방하도록 학습합니다.
  - **영향:** 교사 모델의 <span style="background:#fff88f">성능을 상당 부분 유지하면서도 훨씬 작고 빠른 모델</span>을 만들 수 있어, 리소스가 제한된 <u>엣지 디바이스 배포나 비용에 민감한 클라우드 서빙에 매우 유용</u>합니다.

### 3.2. 추론 엔진 최적화: 특화된 컴파일러의 힘

모델 수준 최적화가 끝난 모델이라도, 이를 실행하는 방식에 따라 성능은 크게 달라질 수 있습니다. 추론 엔진은 최적화된 모델을 특정 하드웨어에서 가장 효율적으로 실행하기 위한 특화된 컴파일러 및 런타임입니다.

- **심층 분석: NVIDIA TensorRT:** NVIDIA GPU를 위한 대표적인 고성능 추론 옵티마이저 및 런타임입니다. TensorRT는 학습된 모델(예: PyTorch, TensorFlow 모델)을 입력받아, 대상 GPU 아키텍처에 맞는 고도로 최적화된 '엔진' 파일을 생성합니다.
- **핵심 최적화 원리:**
  1. **그래프 최적화 (Layer & Tensor Fusion):** TensorRT는 모델의 <u>연산 그래프를 분석하여 여러 개의 연속적인 레이어(예: Convolution → Bias → ReLU)를 하나의 최적화된 커널로 병합</u>합니다. 이를 **레이어 퓨전(Layer Fusion)** 이라 합니다. 이 과정은 <span style="background:#fff88f">GPU 커널을 호출하는 오버헤드를 줄이고, 중간 결과를 GPU 메모리에 다시 쓰고 읽는 과정을 생략하여 메모리 대역폭을 절약</span>합니다. 결과적으로 <u>모델 그래프가 극적으로 단순화되어 성능이 향상</u>됩니다.
  2. **정밀도 보정 (Precision Calibration):** 모델을 FP16이나 INT8과 같은 낮은 정밀도로 자동 변환하면서 정확도 손실을 최소화합니다. 특히 INT8로 변환할 때는 <u>대표적인 샘플 데이터를 사용하여 각 레이어의 값 분포를 분석하고, 정보 손실이 가장 적은 최적의 스케일링 팩터를 결정</u>하는 '**보정(Calibration)**' 과정을 거칩니다.
  3. **커널 자동 튜닝 (Kernel Auto-Tuning):** TensorRT는 배포 대상 GPU의 특정 아키텍처(예: Ampere, Hopper)에 <span style="background:#fff88f">가장 최적화된 커널 구현체를 라이브러리에서 자동으로 선택</span>합니다. 이를 통해 동일한 모델이라도 각기 다른 GPU 하드웨어에서 최상의 성능을 발휘하도록 보장합니다.
  4. **유연성:** 만약 모델의 특정 레이어가 TensorRT에서 지원되지 않는 커스텀 연산일 경우, 해당 부분은 원래의 <span style="background:#fff88f">프레임워크(예: PyTorch)에서 실행하도록 남겨두고 지원되는 부분만 최적화하는 하이브리드 실행을 지원</span>합니다. 이는 복잡하고 새로운 모델 아키텍처에도 TensorRT의 이점을 부분적으로 적용할 수 있게 해주는 강력한 기능입니다.

### 3.3. 서빙 수준 최적화: 런타임의 향상

최적화된 모델과 엔진이 준비되었더라도, 실제 서빙 환경에서 여러 요청을 어떻게 처리하느냐에 따라 최종 성능이 결정됩니다.

- **인플라이트 배칭 (In-flight Batching / Continuous Batching):** 정적 배치와 달리, GPU가 유휴 상태가 되지 않도록 들어오는 <span style="background:#fff88f">요청들을 동적으로 묶어 처리</span>하는 고급 기술입니다. 배치가 꽉 찰 때까지 기다리지 않고, 처리 중인 배치가 끝나면 즉시 대기 중인 요청들로 새로운 배치를 구성하여 GPU 파이프라인을 지속적으로 채웁니다. 이는 GPU 활용률과 전체 처리량을 극대화하는 데 매우 효과적입니다.
- **PagedAttention & KV 캐시 관리:** LLM 추론 시 메모리 소모가 가장 큰 부분은 이전 토큰들의 Key-Value 쌍을 저장하는 KV 캐시입니다. PagedAttention은 <u>운영체제의 페이징 기법과 유사하게 KV 캐시를 관리</u>하여 <span style="background:#fff88f">메모리 단편화를 줄이고, 더 많은 요청을 동시에 처리할 수 있게 해 처리량을 높입니다.</span> 또한, 대화가 길어질 때 KV 캐시를 <u>CPU 메모리나 특수 하드웨어로 오프로딩</u>하여 <span style="background:#fff88f">GPU 메모리 부담을 줄이는 기술</span>도 사용됩니다.
- **추측 추론 (Speculative Inference):** 작고 빠른 '드래프트 모델'을 사용하여 <u>여러 개의 미래 토큰을 미리 생성하고, 크고 정확한 '검증 모델'이 이 후보들을 한 번에 검증</u>하는 방식입니다. 만약 드래프트 모델의 예측이 맞았다면, <span style="background:#fff88f">여러 토큰을 한 번의 연산으로</span> 얻게 되므로 TPOT를 획기적으로 개선할 수 있습니다.

이러한 다층적 최적화 기법들은 서로 독립적이지 않으며, 상호 보완적으로 작용하여 최종적인 서빙 성능을 결정합니다. 아래 표는 각 기법의 특징과 영향을 요약하여 최적화 전략 수립에 도움을 줍니다.

**표 2: 모델 및 추론 최적화 기법 요약**

| 기법                | 주요 메커니즘               | 지연 시간 영향     | 처리량 영향      | 모델 크기 영향    | 정확도 영향                  | 구현 복잡도              |
| ----------------- | --------------------- | ------------ | ----------- | ----------- | ----------------------- | ------------------- |
| **양자화 (PTQ/QAT)** | 가중치/활성화의 수치 정밀도 감소    | 감소 (++)      | 증가 (++)     | 대폭 감소 (+++) | 약간 감소 (-) / 거의 없음 (QAT) | 낮음 (PTQ) / 높음 (QAT) |
| **프루닝**           | 불필요한 모델 가중치 제거        | 감소 (+)       | 증가 (+)      | 감소 (++)     | 약간 감소 (-)               | 중간                  |
| **지식 증류**         | 작은 학생 모델이 큰 교사 모델을 모방 | 대폭 감소 (+++)  | 대폭 증가 (+++) | 대폭 감소 (+++) | 감소 (--)                 | 높음                  |
| **TensorRT 컴파일**  | 그래프 퓨전, 커널 자동 튜닝 등    | 대폭 감소 (+++)  | 대폭 증가 (+++) | 약간 감소 (+)   | 거의 없음                   | 중간                  |
| **인플라이트 배칭**      | 동적 요청 그룹화             | 약간 증가 (-)    | 대폭 증가 (+++) | 영향 없음       | 영향 없음                   | 높음 (프레임워크 수준)       |
| **추측 추론**         | 작은 모델로 예측, 큰 모델로 검증   | TPOT 감소 (++) | 증가 (++)     | 영향 없음       | 영향 없음                   | 높음 (프레임워크 수준)       |


## 파트 1: 고성능 추론 최적화: 처리량 극대화 및 지연 시간 최소화

본 보고서의 첫 번째 파트에서는 모델 추론의 원시 성능을 향상시키는 데 사용되는 핵심 기술을 심층적으로 분석합니다. 단순히 '더 빠르게 만드는 것'을 넘어, 모든 프로덕션급 서빙 시스템의 결정적인 제약 조건인 지연 시간(latency), 처리량(throughput), 그리고 비용(컴퓨팅/메모리) 간의 근본적인 상충 관계를 탐구합니다.

### 동적 및 연속 배치: 효율성의 초석

이 섹션에서는 단순한 요청 그룹화에서부터 최신 대규모 언어 모델(LLM)에 요구되는 고도로 정교하고 메모리를 인지하는 기술에 이르기까지 배치(batching) 전략의 진화를 분석합니다.

#### 동적 배치의 메커니즘

서버 측 배치 기술, 특히 NVIDIA Triton과 같은 프레임워크에서의 동적 배치는 비동기적으로 도착하는 개별 추론 요청을 수집하여 더 큰 단일 배치로 그룹화한 후 모델에 전달하여 처리하는 방식으로 작동합니다. 이러한 접근 방식은 GPU에서 다수의 작은 배치를 순차적으로 실행하는 것보다 단일의 큰 배치를 실행하는 것이 훨씬 효율적이기 때문에 매우 중요하며, 이를 통해 처리량을 극적으로 향상시킬 수 있습니다.

#### 지연 시간과 처리량의 균형

동적 배치의 핵심 과제는 <u>더 큰 배치를 형성하기 위해 대기하는 것(처리량 증가)과 요청을 즉시 처리하는 것(지연 시간 최소화) 사이의 균형</u>을 맞추는 것입니다. Triton의 `max_queue_delay_microseconds`와 같은 핵심 구성 매개변수는 이러한 균형을 조절하는 데 중요한 역할을 합니다. 이 매개변수는 스케줄러가 지정된 시간 동안 요청을 지연시켜, 더 많은 요청이 도착하여 더 크고 효율적인 배치를 형성할 수 있도록 허용합니다. 성능 분석 도구를 사용하여 이 값을 정밀하게 조정함으로써, 서비스 수준 협약(SLA)에서 요구하는 지연 시간 예산을 초과하지 않으면서 처리량을 극대화하는 최적의 지점을 찾을 수 있습니다. 예를 들어, 기본 동적 배치 구성이 이미 지연 시간 예산 내에 있다면, 최대 배치 크기를 늘리거나 `max_queue_delay_microseconds` 값을 0이 아닌 값으로 설정하여 지연 시간을 약간 희생하는 대신 처리량을 높이는 시도를 할 수 있습니다. 또한, 우선순위 큐(`priority_levels`)를 설정하여 중요도가 높은 요청이 낮은 우선순위의 요청을 우회하여 먼저 처리되도록 할 수도 있어, 다양한 서비스 요구사항에 대응할 수 있습니다.

#### LLM의 등장과 연속 배치의 부상

전통적인 동적 배치는 대규모 언어 모델(LLM)에 적용하기에는 한계가 명확합니다. LLM은 <u>가변 길이의 입력과 출력</u>을 가지므로, '배치'는 더 이상 균일한 텐서가 아닙니다. 짧은 프롬프트로 구성된 배치와 긴 문서로 구성된 배치는 근본적으로 다릅니다. 이러한 가변성은 심각한 메모리 낭비와 불필요한 패딩(padding)을 유발하여 GPU 활용도를 저하시킵니다.

이러한 문제는 단순한 성능 저하를 넘어, 메모리가 제한된 GPU에서 대규모 모델을 서빙하는 데 있어 실존적인 위협이 되었습니다. 특히, LLM의 핵심 구성 요소인 <u>KV 캐시에 필요한 메모리는 시퀀스 길이에 따라 기하급수적으로 증가</u>하기 때문입니다. 이러한 배경에서 **연속 배치(Continuous Batching)** 또는 인플라이트 배치(in-flight batching)라는 새로운 기술이 등장했습니다. 연속 배치는 전체 배치가 형성될 때까지 기다리는 대신, 개별 시퀀스가 완료되는 즉시 현재 처리 중인 배치에 새로운 요청을 추가하는 방식입니다. 이를 통해 유휴 시간과 패딩을 제거하고 GPU를 지속적으로 가동시킬 수 있습니다. 이 기술은 종종 **PagedAttention**과 같은 기법과 함께 사용되는데, 이는 운영 체제의 가상 메모리 페이징과 유사한 방식으로 메모리 집약적인 KV 캐시를 보다 효율적으로 관리합니다.

배치 기술의 발전은 인공지능 모델의 진화 과정을 그대로 반영합니다. 초기 이미지 분류용 CNN과 같은 모델들은 입력 크기가 고정되어 있었습니다. 이러한 모델들에게 동적 배치는 완벽한 해결책이었습니다. N개의 요청을 그룹화하여 `(N, *)` 형태의 텐서로 쌓아 처리하면 되었고, 주된 고민은 대기 시간과 배치 크기 간의 단순한 트레이드오프였습니다. 그러나 트랜스포머와 LLM의 등장은 가변 길이 시퀀스라는 새로운 문제를 제시했습니다. 10개의 요청으로 구성된 배치가 10개의 다른 시퀀스 길이를 가질 수 있게 되면서, 가장 긴 시퀀스에 맞춰 모든 시퀀스를 패딩해야 하는 전통적인 방식은 막대한 계산 낭비를 초래했습니다. 이러한 비효율성은 단순히 성능 문제를 넘어, 메모리가 제한된 GPU에서 대규모 모델을 서비스하는 것 자체를 불가능하게 만들었습니다. 따라서 문제의 본질은 '어떻게 요청을 그룹화할 것인가'에서 '매우 파편화되고 동적인 메모리를 관리하면서 어떻게 GPU를 지속적으로 작업으로 채울 것인가'로 전환되었습니다. 이는 곧 연속 배치와 PagedAttention과 같은 기술의 개발로 이어졌습니다. 결론적으로, 동적 배치에서 연속 배치로의 전환은 점진적인 개선이 아니라, 지배적인 모델 아키텍처가 CNN에서 LLM으로 이동함에 따라 발생한 근본적인 아키텍처의 적응입니다. 이는 <u>상태 없는(stateless) 요청 스케줄링에서 상태를 가지며(stateful) 메모리를 인지하는 워크로드 오케스트레이션으로의 패러다임 전환</u>을 의미합니다.




### 섹션 2.3: 성능 최적화 기법

처리량을 극대화하고 지연 시간을 최소화하기 위한 다양한 엔지니어링 기법이 존재합니다.

#### 동적 배치 (Dynamic Batching)

- **개념:** 동적 배치는 서버 측에서 짧은 시간 동안 도착하는 개별 추론 요청들을 수집하여 하나의 더 큰 배치로 묶은 다음 GPU로 보내는 기술입니다.3
- **이점:** GPU는 병렬 처리에 매우 효율적이므로, 더 큰 데이터 배치에서 훨씬 더 효율적으로 작동합니다. 이 기법은 많은 모델에서 처리량을 3배에서 10배까지 향상시킬 수 있습니다.3
- **트레이드오프:** 이 방식은 서버가 더 많은 요청이 도착하기를 기다리는 동안 배치의 첫 번째 요청들에 약간의 지연 시간을 추가합니다. 이 지연 시간은 구성 가능한 지연 매개변수(예: Triton의 `max_queue_delay_microseconds`)를 통해 제어됩니다.31

#### 요청 큐잉 및 우선순위 지정

우선순위 큐를 구현하여 높은 가치를 가지거나 지연 시간에 민감한 요청이 나중에 도착하더라도 표준 요청보다 먼저 처리되도록 할 수 있습니다.32 이는 중요한 비즈니스 트랜잭션과 관련된 요청을 일반적인 요청보다 우선적으로 처리해야 할 때 유용합니다.

#### 다단계 캐싱

단순한 결과 캐싱을 넘어, 복잡한 파이프라인의 속도를 높이기 위해 중간 단계의 피처 계산 결과나 부분적인 모델 출력을 캐싱하는 것을 포함합니다.3 예를 들어, 여러 모델을 통과해야 하는 요청의 경우, 첫 번째 모델의 출력을 캐싱하여 후속 모델들이 이를 재사용할 수 있습니다.

### 데이터 경로 최적화: CPU와 가속기 간 통신

이 섹션에서는 자주 간과되는 병목 지점, 즉 호스트 CPU의 메모리에서 가속기(GPU/TPU)의 메모리로 데이터를 이동하는 데 걸리는 시간을 다룹니다.

#### 데이터 전송 병목 현상

고해상도 이미지나 긴 문서와 같이 입력 데이터가 큰 추론 워크로드의 경우, <u>PCIe 버스를 통해 데이터를 전송하는 데 소요되는 시간이 전체 지연 시간의 상당 부분을 차지</u>하여 값비싼 가속기를 유휴 상태로 만들 수 있습니다. 이러한 병목 현상을 해결하기 위해 데이터 이동을 체계적으로 캐싱하거나 전송 횟수를 줄이는 접근법이 제안되었습니다.

#### Pinned (페이지-고정) 메모리

일반적인 CPU 메모리(`malloc`)는 운영 체제가 이동시킬 수 있는 "페이지 교환 가능(pageable)" 메모리입니다. GPU로의 DMA(Direct Memory Access) 전송이 발생하기 전에, GPU 드라이버는 이 데이터를 운영 체제가 이동시킬 수 없는 특별한 "고정된(pinned)" 버퍼로 먼저 복사해야 합니다. `cudaMallocHost`를 사용하여 메모리를 직접 고정 메모리로 할당함으로써, 이러한 추가적인 호스트 간 복사를 제거하고 지연 시간을 줄일 수 있습니다.

#### Zero-Copy 메모리

Zero-Copy는 한 단계 더 나아간 개념으로, 특히 NVIDIA Jetson 시리즈와 같이 통합 메모리 아키텍처를 가진 시스템에서 효과적입니다. Zero-Copy를 사용하면 GPU는 명시적인 복사 없이 <u>호스트의 고정 메모리에 있는 데이터에 직접 접근</u>할 수 있습니다. 데이터가 물리적으로 이동하는 것이 아니라 GPU에 해당 데이터에 대한 포인터가 제공되는 방식입니다. 이는 CPU와 GPU가 물리적 메모리를 공유하는 통합 GPU에서 중복 복사를 완전히 피할 수 있어 매우 효율적입니다.

#### 통합 메모리 (Unified Memory)

이는 CUDA가 제공하는 더 진보된 추상화 계층으로, <u>CPU와 GPU 모두에서 접근할 수 있는 단일 통합 메모리 공간을 생성</u>합니다. CUDA 드라이버는 데이터를 접근하는 프로세서로 필요에 따라 자동으로 데이터를 마이그레이션하여, Zero-Copy의 용이성과 데이터 지역성(locality)의 성능 이점을 결합합니다. 이는 프로그래밍을 단순화하면서 내부적으로 데이터 이동을 지능적으로 관리합니다.

Pinned Memory와 Zero-Copy 같은 기술들은 CPU와 GPU 메모리의 물리적 분리 및 상대적으로 느린 PCIe 버스라는 하드웨어적 한계를 완화하기 위한 소프트웨어 최적화 기법입니다. 그러나 일부 전문가들은 이것만으로는 충분하지 않다고 주장합니다. 전통적인 x86 CPU 중심 아키텍처 자체가 전처리, 후처리, 네트워크 I/O의 병목 지점으로 작용하여 가속기 활용률을 50% 미만으로 낮게 유지시킨다는 것입니다.

이러한 문제 인식은 새로운 목적 기반 AI 추론 아키텍처의 등장을 촉진했습니다. NeuReality와 같은 회사는 전처리, 후처리 및 네트워킹 기능을 직접 통합하여 전통적인 호스트 CPU를 완전히 우회하고 데이터 경로를 간소화하는 "서버-온-칩(server-on-a-chip)" 시스템(예: NR1 칩)을 개발하고 있습니다. 이는 레거시 아키텍처에서 데이터 전송을 최적화하는 것에서 벗어나, AI 추론을 위해 특별히 설계된 <u>새로운 데이터 중심 아키텍처</u>를 만드는 패러다임 전환을 나타냅니다. 결론적으로, Zero-Copy와 Pinned Memory는 오늘날 시스템에 필수적인 전술이지만, 장기적인 추세는 AI 가속기를 범용 CPU의 주변 장치로 취급하는 대신, 데이터 처리를 위한 특수 구성 요소로 둘러싸인 시스템의 중심으로 두는 근본적인 서버 아키텍처의 재설계 방향으로 나아가고 있습니다.

## 마치며


> 본 포스트는 Google Gemini의 응답을 기반으로 저의 의견을 반영하여 다시 작성했습니다.