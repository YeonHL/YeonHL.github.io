---
title: 모델 서빙
description:
date: 2025-09-22T19:03:00
lastmod: 2025-09-22
slug: optimize
comments: true
math: false
categories:
  - Systems
tags:
  - Model-Serving
keywords:
  - Model-Serving
  - MLOps
---
## 개요

### 4.3: 서빙 엔드포인트 보안: 개인정보보호, 거버넌스 및 적대적 방어

LLM이 중요한 애플리케이션에 통합됨에 따라 새로운 공격 표면이 생겨났습니다. 모델 서빙 API를 보호하는 것은 매우 중요하며, 새로운 종류의 취약점에 대한 방어를 포함합니다.

- **OWASP Top 10 for LLMs:** 이는 LLM 관련 리스크를 이해하기 위한 중요한 프레임워크를 제공합니다. 서빙 API에 대한 주요 취약점은 다음과 같습니다.
  - **LLM01: 프롬프트 인젝션(Prompt Injection):** 공격자가 모델이 <u>원래 지침을 무시하도록 입력을 조작하여 데이터 유출이나 무단 작업을 유발</u>할 수 있습니다. 완화 방법에는 <u>입/출력 필터링, 권한 제어, 그리고 보안을 위해 프롬프트에 의존하지 않는 것</u>이 포함됩니다.
  - **LLM02: 민감 정보 노출(Sensitive Information Disclosure):** 모델이 <u>학습 데이터나 연결된 시스템에서 기밀 데이터를 의도치 않게 노출</u>합니다. 완화 방법에는 <u>데이터 정제 및 접근 제어</u>가 포함됩니다.
  - **LLM04: 모델 서비스 거부 / LLM10: 무제한 자원 소비(Unbounded Consumption):** 공격자가 <u>자원 집약적인 쿼리를 보내 비용을 증가시키거나 서비스를 저하</u>시킵니다. 완화 방법에는 <u>엄격한 속도 제한, 입력 유효성 검사, 자원 모니터링</u>이 포함됩니다.
  - **LLM05: 부적절한 출력 처리(Insecure Output Handling):** 다운스트림 시스템이 <u>악성 코드(예: XSS, SSRF)를 포함할 수 있는 LLM의 출력을 맹목적으로 신뢰하고 실행</u>합니다. 완화 방법은 <u>LLM 출력을 신뢰할 수 없는 사용자 입력으로 취급하고 유효성을 검사</u>하는 것입니다.
- **적대적 공격(Adversarial Attacks):** 이는 <u>오분류를 유발하도록 설계된 미묘하고 악의적인 입력</u>입니다. <u>적대적 학습(모델을 이러한 예제로 학습시키는 것)과 같은 방어는 견고성을 향상</u>시킬 수 있지만, 역설적으로 <u>멤버십 추론(공격자가 특정 데이터 포인트가 학습 데이터에 있었는지 확인하는 공격)과 같은 다른 공격의 위험을 증가</u>시킬 수 있습니다. 이는 서로 다른 보안 및 개인정보보호 목표 간의 복잡한 트레이드오프를 보여줍니다.


## 섹션 5: 모델 서빙 시스템 보안 강화

인공지능 모델을 프로덕션 환경에 배포하는 것은 강력한 비즈니스 가치를 창출하는 동시에, 새로운 보안 위협에 시스템을 노출시키는 일이기도 합니다. AI 시스템의 <span style="background:#fff88f">공격 표면(attack surface)은 전통적인 소프트웨어 시스템보다 넓고 복잡</span>합니다. <u>코드와 인프라의 취약점뿐만 아니라, 학습 데이터, 모델 자체의 수학적 특성, 그리고 전체 MLOps 파이프라인이 공격 대상</u>이 될 수 있습니다. 따라서 모델 서빙 시스템을 보호하기 위해서는 전통적인 인프라 보안을 넘어, 머신러닝 고유의 위협에 대응할 수 있는 다층적인 보안 전략이 필수적입니다. 본 섹션에서는 '설계 기반 보안(Secure by Design)' 원칙에 입각한 기초 보안 프랙티스와, 새롭게 부상하는 적대적 머신러닝(Adversarial Machine Learning) 공격에 대한 방어 전략을 제시합니다.

### 5.1. 설계 기반 보안: 기초 보안 프랙티스

안전한 모델 서빙 시스템은 개발 초기 단계부터 보안을 고려하여 설계되어야 합니다. 이는 MLOps 수명주기의 모든 단계에 보안 원칙을 내재화하는 것을 의미합니다.

- **인프라 보안:** 모델이 실행되는 <span style="background:#fff88f">컴퓨팅, 네트워크, 스토리지 자원을 보호</span>하는 것은 가장 기본적인 보안 계층입니다. <u>방화벽, 네트워크 분리(network segmentation), 전송 계층 보안(TLS)과 같은 암호화 프로토콜을 사용</u>하여 전송 중인 데이터를 보호해야 합니다.
- **접근 제어:** 강력한 <span style="background:#fff88f">인증(authentication) 및 인가(authorization) 메커니즘을 구현</span>해야 합니다. 특히 <u>역할 기반 접근 제어(Role-Based Access Control, RBAC)는 사용자와 서비스가 자신의 기능 수행에 필요한 최소한의 모델과 데이터에만 접근하도록 제한</u>함으로써 '최소 권한의 원칙'을 실현하는 데 핵심적인 역할을 합니다.
- **데이터 및 모델 보호:** 학습 데이터와 학습된 모델 아티팩트는 <u>저장 시(at rest)와 전송 시(in transit) 모두 암호화</u>되어야 합니다. 또한, <u>모델 난독화(obfuscation)나 암호화 기술을 적용하여 모델 자체의 지적 재산을 보호</u>할 수 있습니다.
- **안전한 CI/CD 파이프라인:** 배포 파이프라인은 악성 코드나 조작된 모델이 프로덕션 환경으로 유입될 수 있는 잠재적 경로입니다. <u>코드 서명(code signing), 아티팩트 무결성 검증, 정적/동적 취약점 스캔 등을 파이프라인에 통합</u>하여 신뢰할 수 있는 코드와 모델만이 배포되도록 보장해야 합니다.

### 5.2. 적대적 공격: 머신러닝 특화 공격 방어

적대적 AI는 악의적인 행위자가 모델을 속이거나 조작하기 위해 특수하게 제작된 입력을 사용하는 공격 기법을 총칭합니다. 이러한 공격은 전통적인 소프트웨어 버그가 아닌, 모델이 학습한 패턴의 취약점을 이용합니다.

프로덕션에 배포된 모델이 직면할 수 있는 주요 공격 벡터는 다음과 같습니다.

- **회피 공격 (Evasion Attacks):** 원본 입력에 인간이 감지하기 어려운 <u>미세한 노이즈를 추가하여 모델이 오분류하도록 만드는 공격</u>입니다. 예를 들어, 자율주행차의 이미지 인식 모델이 미세하게 조작된 정지 표지판 이미지를 속도 제한 표지판으로 오인하게 만드는 경우입니다.
- **모델 추출 공격 (Model Stealing / Extraction):** 공개된 API에 반복적으로 쿼리를 보내고 그 입출력 쌍을 분석하여, 대상 모델과 <u>유사한 성능을 내는 대체 모델을 복제하는 공격</u>입니다. 이는 기업의 핵심 지적 재산을 탈취하는 행위입니다.
- **모델 역전 공격 (Model Inversion):** 모델의 예측 결과를 분석하여 학습 데이터에 포함된 민감한 정보를 역으로 추론하는 공격입니다. 예를 들어, 안면 인식 모델의 출력에서 특정 개인의 얼굴 이미지를 복원하려는 시도가 이에 해당하며, 심각한 프라이버시 침해를 유발할 수 있습니다.
- **프롬프트 주입 공격 (Prompt Injection):** LLM을 대상으로 하는 공격으로, 악의적으로 조작된 프롬프트를 입력하여 모델이 안전 장치를 우회하거나, 기밀 정보를 누설하거나, 유해한 콘텐츠를 생성하도록 유도하는 공격입니다.

이러한 공격에 대한 방어 전략으로는 모델 학습 단계에서 <u>의도적으로 적대적 예제를 포함시켜 모델의 강건성(robustness)을 높이는</u> '**적대적 학습(adversarial training)**', <u>입력 데이터에 대한</u> **유효성 검사 및 정제(sanitization)**, 그리고 <u>API에 대한</u> **요청 비율 제한(rate limiting) 및 이상 쿼리 모니터링**을 통해 의심스러운 활동을 탐지하는 방법 등이 있습니다.

머신러닝 시스템의 보안을 고려할 때, 모델의 유용성과 보안 강화 조치 사이에는 본질적인 긴장 관계가 존재합니다. 예를 들어, 적대적 학습은 모델의 강건성을 높이지만, <u>정상적인 데이터에 대한 예측 정확도를 약간 저하</u>시킬 수 있습니다. API 요청 비율을 제한하여 모델 추출 공격을 방어하는 조치는 <u>정상적인 고트래픽 사용자의 서비스 이용에 불편</u>을 줄 수 있습니다. 이처럼 보안을 강화하는 많은 조치가 모델의 성능이나 효용성을 일부 저해할 수 있습니다. 따라서 어떤 수준의 보안을 적용할지는 기술팀과 비즈니스 이해관계자가 함께 참여하여, 잠재적 <u>위협의 심각성과 비즈니스 영향을 고려한 리스크 기반의 의사결정</u>을 통해 신중하게 결정되어야 합니다.


## 섹션 5: 게이트 강화: AI 모델 서빙의 보안

AI 모델은 기존 소프트웨어와는 다른 독특한 공격 표면을 가지고 있어, 이에 특화된 보안 전략이 필요합니다. 이 섹션에서는 AI 모델을 위협하는 주요 공격 유형을 분석하고, 이를 방어하기 위한 다층적 전략을 제시합니다.

### 5.1. 위협 환경: AI 모델에 대한 공격

- **적대적 공격 (Adversarial Attacks):** 모델이 오작동하도록 의도적으로 조작된, 눈으로는 거의 구별할 수 없는 미세한 노이즈가 추가된 입력을 만들어 모델을 속이는 공격입니다.61
  - **회피 공격 (Evasion Attacks, 예: FGSM, PGD):** 가장 일반적인 유형으로, 추론 시점에 입력 데이터(예: 이미지)를 약간 변형하여 모델의 탐지를 피하거나 잘못된 클래스로 분류하도록 유도합니다.61
  - **포이즈닝 공격 (Poisoning Attacks):** 공격자가 학습 데이터셋에 악의적인 데이터를 주입하여, 학습된 모델 자체의 성능을 저하시키거나 특정 입력에 대해 오작동하도록 백도어를 심는 공격입니다.
- **모델 추출 / 탈취 (Model Extraction / Stealing):** 공격자가 MLaaS(Machine Learning as a Service)와 같이 유료로 제공되는 API에 반복적으로 쿼리를 보내고, 입력과 출력의 쌍을 수집하여 내부 모델의 아키텍처나 가중치를 역공학적으로 알아내거나 복제하는 공격입니다.64
- **탈옥 / 프롬프트 인젝션 (Jailbreaking / Prompt Injection):** LLM에 특화된 위협으로, 공격자가 교묘하게 작성된 프롬프트를 사용하여 모델의 안전 필터를 우회하고, 유해하거나 의도하지 않은 답변(예: 비윤리적 콘텐츠 생성)을 유도하는 공격입니다.62

### 5.2. 다층적 방어 전략

단일 방어 기법만으로는 진화하는 공격을 모두 막을 수 없으므로, 여러 방어선을 구축하는 심층 방어(Defense-in-Depth) 전략이 필요합니다.

- **적대적 훈련 (Adversarial Training):** 현재까지 알려진 가장 효과적인 방어 기법 중 하나입니다. 학습 과정에서 의도적으로 적대적 예제를 생성하고, 모델이 이 예제들을 올바르게 분류하도록 명시적으로 학습시킵니다. 이를 통해 모델이 적대적 섭동에 대해 더 강건(robust)해집니다.61
- **방어적 증류 (Defensive Distillation):** 지식 증류 기법을 방어에 활용하는 방법입니다. '교사 모델'이 출력하는 부드러운 확률 분포(Softmax 출력의 온도를 높여 조절)를 정답 레이블로 사용하여 '학생 모델'을 학습시킵니다. 이 과정은 모델의 결정 경계를 평탄하게 만들어, 작은 섭동에 덜 민감하게 만드는 효과가 있습니다.61
- **입력 정제 및 변환 (Input Sanitization and Transformation):** 모델에 입력을 전달하기 전에 JPEG 압축, 노이즈 추가, 리사이징 등의 변환을 적용하여 잠재적인 적대적 노이즈를 제거하거나 약화시키는 방법입니다.61
- **탐지 기반 방어 (Detection-Based Defenses):** 입력 데이터가 적대적인지 아닌지를 판별하는 별도의 탐지 모델을 학습시키는 방식입니다. 적대적 입력으로 의심되면 해당 요청을 거부하거나 추가적인 분석을 수행할 수 있습니다.61
- **인증된 방어 (Certified Defenses):** 특정 크기(epsilon-ball) 내의 어떠한 섭동에 대해서도 모델의 예측이 변하지 않음을 수학적으로 보장하는 기법입니다. 가장 강력한 수준의 방어를 제공하지만, 계산 비용이 매우 높고 모델의 표현력이나 정확도를 제한할 수 있다는 단점이 있습니다.61

**표 4: 적대적 공격 및 방어 매트릭스**

| 공격 유형 \ 방어 전략    | 적대적 훈련                                                   | 방어적 증류                                                       | 입력 정제                                                            | 탐지 기반 방어                                                  | 인증된 방어                                           |
| ------------------------ | ------------------------------------------------------------- | ----------------------------------------------------------------- | -------------------------------------------------------------------- | --------------------------------------------------------------- | ----------------------------------------------------- |
| **회피 공격 (FGSM/PGD)** | **높음:** 직접적인 대응책으로 가장 효과적임.                  | **중간:** 그래디언트 기반 공격을 어렵게 만들지만, 우회될 수 있음. | **낮음~중간:** 일부 노이즈를 제거할 수 있으나, 적응형 공격에 취약함. | **중간:** 알려진 공격 패턴은 잘 탐지하나, 새로운 공격에 취약함. | **높음:** 특정 섭동 범위 내에서 수학적 보장을 제공함. |
| **모델 추출**            | **낮음:** 직접적인 방어 효과는 미미함.                        | **낮음:** 모델 복잡도를 약간 높일 수 있으나 근본적 해결책은 아님. | **영향 없음**                                                        | **중간:** 비정상적인 쿼리 패턴을 탐지하여 방어할 수 있음.       | **영향 없음**                                         |
| **프롬프트 인젝션**      | **중간:** 다양한 탈옥 프롬프트를 학습 데이터에 포함시켜 대응. | **낮음**                                                          | **중간:** 입력 프롬프트에서 의심스러운 패턴을 필터링.                | **높음:** 유해성 탐지 분류기를 별도로 두어 응답을 검열.         | **영향 없음**                                         |



## 마치며


> 본 포스트는 Google Gemini의 응답을 기반으로 저의 의견을 반영하여 다시 작성했습니다.