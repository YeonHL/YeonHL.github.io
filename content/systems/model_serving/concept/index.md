---
title: 모델 서빙의 개념
description: 모델 서빙은 어떤 문제를 해결하기 위해 등장했고, 무엇을 목표로 할까요?
date: 2025-09-16T19:03:00
lastmod: 2025-09-18
slug: concept
comments: true
math: false
categories:
  - Systems
tags:
  - Model-Serving
keywords:
  - Model-Serving
  - MLOps
---
## 개요

인공지능(AI) 모델 서빙은 단순히 학습된 모델을 배포하는 단계를 넘어, AI의 잠재적 가치를 실제 비즈니스 성과로 전환하는 핵심적인 공학 분야로 자리 잡았습니다. 모델 서빙의 본질적인 과제는 <span style="background:#fff88f">실제 운영 환경의 다양한 제약 조건 하에서 빠르고, 안정적이며, 비용 효율적인 예측 서비스를 제공하는 것</span>입니다. 이는 단순히 <u>모델을 API 뒤에 배치하는 것을 넘어, 분산 시스템, MLOps, 하드웨어 최적화 원칙을 통합하는 총체적인 접근을 요구</u>합니다. 성공적인 모델 서빙은 <u>예측의 품질뿐만 아니라, 예측이 전달되는 속도와 신뢰성, 그리고 이를 유지하는 데 드는 비용까지 모두 고려</u>하는 다차원적인 최적화 문제입니다.


## 모델 서빙의 시작

### 초기 배포: 스크립트와 사일로

초기 머신러닝 배포는 개발 과정의 마지막 단계에서 고려되는 부차적인 작업이었습니다. 데이터 과학자들은 훈련된 모델 결과물(예: `pickle` 파일)을 <u>엔지니어링 팀에 전달하는 방식으로 작업을 마무리</u>했습니다. 이 시기의 배포는 임시방편으로 작성된 스크립트, 수동 프로세스, 그리고 표준화되지 않은 환경의 조합으로 이루어져 여러 문제점을 내포하고 있습니다.

- **재현성 부족**: 일관되지 않은 환경과 수동적인 단계들은 모델이나 그 <u>예측 결과를 신뢰성 있게 재현하는 것을 거의 불가능</u>합니다.
- **확장성 문제**: 모델의 수와 데이터의 복잡성이 증가함에 따라 <u>수동 프로세스는 본질적으로 확장 불가능</u>합니다.
- **오류 위험 증가 및 비효율성**: 데이터 과학팀과 운영팀 간의 수동적인 인계 과정은 소통의 단절, 오류 발생 위험 증가, 그리고 느린 출시 주기로 이어졌습니다. 모델 개발과 소프트웨어 개발 사이의 간극은 주요 병목 현상의 원인이었습니다.

이러한 초기 단계의 어려움은 머신러닝이 학문적 탐구에서 실용적인 애플리케이션으로 전환하기 위한 배포에서 체계적이고 원칙에 기반한 접근법의 필요성을 절실하게 만들었다.

### MLOps: 철학적, 문화적 전환

MLOps는 개발(DevOps), 데이터(DataOps), 모델(ModelOps)을 통합하여 ML의 고유한 복잡성에 맞춰 CI/CD(지속적 통합/지속적 배포), 버전 관리, 자동화와 같은 DevOps 원칙을 적용합니다. 코드뿐만 아니라 데이터와 모델을 일급 시민(first-class citizens)으로 취급하고 관리합니다. 

목표는 데이터 수집, 모델 개발, 테스트, 배포, 모니터링, 거버넌스에 이르는 <u>전체 ML 생명주기에 걸쳐 반복 가능하고, 신뢰할 수 있으며, 확장 가능한 워크플로우를 만드는 것</u>입니다. 이를 통해 시장 출시 시간을 단축하고, 생산성을 향상시키며, 효율적인 배포를 달성할 수 있습니다.

이는 **결과물 중심(artifact-centric)** 관점에서 **시스템 중심(system-centric)** 관점으로의 전환입니다. 초기에는 '훈련된 모델 파일' 자체가 최종 결과물로 간주되었고 "이 저장된 모델 파일을 어떻게 서버에서 실행할까?"라는 질문이 이 시대의 핵심 과제였습니다. 그러나 이러한 접근은 의존성 관리, 환경 불일치, 수동 오류와 같은 문제들을 야기했습니다.

이에 대한 해답은 <u>데이터 검증, 훈련, 모델 검증, 배포, 모니터링</u>에 이르는 **전체 프로세스를 자동화**하는 것입니다. 이 자동화된 파이프라인, 즉 <span style="background:#fff88f">'시스템'이 진정한 의미의 지속 가능하고, 버전 관리되며, 확장 가능한 자산</span>입니다.

- **핵심 원칙:**
  - **자동화:** CI/CD(지속적 통합/지속적 배포) 파이프라인을 통해 모델의 테스트, 검증, 배포 과정을 자동화하여 수동 개입과 인적 오류를 줄입니다.
  - **실험 추적 및 모델 레지스트리:** 모든 실험, 데이터셋, 모델 아티팩트를 버전 관리하여 재현성을 보장하고, 승인된 모델을 중앙에서 관리하여 거버넌스를 강화합니다.
  - **모니터링:** 프로덕션 환경에서 시스템 성능(지연 시간, 에러율)과 모델 품질(예측 정확도, 드리프트)을 지속적으로 추적합니다.
  - **협업:** 데이터 과학자, ML 엔지니어, 운영팀이 공통된 프레임워크와 도구를 사용하여 원활하게 협업할 수 있는 환경을 제공합니다.


## 모델 서빙의 특징

### 훈련과 추론의 분리

머신러닝 프로세스를 훈련과 추론으로 나눌 수 있습니다.

- **훈련 (귀납/귀추):** 이는 '발견'의 과정입니다. 데이터에서 <u>패턴을 관찰하여 일반적인 규칙을 추론(귀납)하거나 가설을 형성(귀추)</u>하는 단계입니다. 이 단계는 <u>계산 집약적이고, 실험적이며, 반복적</u>인 특징을 가집니다.
- **추론 (연역):** 이는 '적용'의 과정입니다. 이미 알려진 <u>규칙(훈련된 모델)을 특정 사례(새로운 데이터)에 적용하여 결론(예측)에 도달</u>하는 단계입니다. 이 단계는 <span style="background:#fff88f">빠르고, 효율적이며, 신뢰할 수 있어야</span> 합니다.

이러한 분리는 엔지니어링 과정에서 전혀 다른 두 워크로드를 최적화합니다. <u>훈련 파이프라인은 강력하고 값비싼 하드웨어(GPU/TPU 등)에서 배치(batch) 지향적</u>인 방식으로 실행될 수 있으며, <u>추론 서비스는 더 가볍고 비용 효율적이며 고가용성을 갖춘 인프라</u>에서 <span style="background:#fff88f">낮은 지연 시간(latency)에 최적화</span>되어 배포될 수 있습니다.


### 재현성

<span style="background:#fff88f">예측은 그것을 생성한 과정이 재현 가능할 때에만 신뢰</span>할 수 있습니다. ML 서빙에서의 **재현성**이란, <u>동일한 입력(코드, 데이터, 구성)이 주어졌을 때 시스템이 동일한 모델을 생성하고, 결과적으로 동일한 예측을 산출</u>하는 것을 의미합니다. 재현성을 확보하기 위한 핵심 메커니즘은 다음과 같습니다.

- **컨테이너화 (Docker):** 모델, 의존성, 그리고 서빙 애플리케이션을 <u>단일하고 불변하는 컨테이너 이미지로 패키징</u>하여 실행 환경이 어디에서나 동일함을 보장합니다.
- **버전 관리 (코드, 데이터, 모델):** 코드와 함께 <u>데이터와 모델을 버전 관리되는 결과물</u>로 취급하는 것(Git, DVC 등의 도구 사용)은 모든 예측에 대한 완전하고 감사 가능한 계보(lineage)를 만드는 데 필수적입니다.
- **코드형 인프라 (Infrastructure as Code, IaC):** <span style="background:#fff88f">배포 환경(서버, 네트워크 등)을 코드로 정의</span>(Terraform, CloudFormation 등)함으로써 배포 환경 자체의 재현성을 보장합니다.
- **FAIR 원칙:** 찾을 수 있고(Findable), 접근 가능하며(Accessible), 상호 운용 가능하고(Interoperable), 재사용 가능한(Reusable) **FAIR 원칙**은 데이터와 모델을 본질적으로 재현성을 지원하는 방식으로 관리하기 위한 높은 수준의 철학적 프레임워크를 제공합니다.

### 서비스

모델 서빙 인프라는 모델이 애플리케이션과 최종 사용자의 필요를 충족시킬 수 있도록 지원합니다. 이를 구현하는 주요 메커니즘은 API(Application Programming Interface)입니다. 

API는 근본적인 모델과 인프라의 복잡성을 추상화하는 잘 정의된 계약 역할을 합니다. API 소비자는 모델이 심층 신경망인지 단순한 로지스틱 회귀인지 알 필요 없습니다.  요청을 어떻게 형식화하고 응답을 어떻게 해석하는지 알면 됩니다. 이러한 관심사의 분리(separation of concerns)는 현대 소프트웨어 아키텍처의 기본 원칙입니다.


### 정확성

모델 서빙은 전통적인 소프트웨어와 차이가 있습니다. 전통적 소프트웨어의 '정확성'은 정적인 논리에 의해 결정되는 내재적이고 고정된 속성입니다. 반면, 머신러닝(ML) 모델의 '정확성'은 <u>확률적이며, 끊임없이 변화하는 실제 세계의 데이터에 대한 성능</u>으로 결정되는 동적인 품질입니다. 모델 서빙은 이러한 동적인 관계를 지속적으로 적응하고, 모니터링하며, 관리해야 합니다.

- **전통적 소프트웨어 (결정론적):** 명시적으로 인간이 작성한 논리에 따라 작동합니다. 동일한 입력과 상태가 주어지면 _항상_ 동일한 출력을 생성한다. 그 행동은 코드로 완전히 결정됩니다.
- **ML 모델 (확률론적):** 데이터로부터 학습된 패턴에 따라 작동합니다. <span style="background:#fff88f">결정론적 확실성이 아닌 확률적 평가를 제공</span>한다. 그 출력은 가장 가능성 있는 결과에 대한 추정치인 예측이며, 불확실성을 내포합니다. 동일한 입력에 대해서도 모델의 예측은 확률 분포로부터의 샘플로 간주될 수 있습니다.

발생하는 문제점과 그 대응 방법에도 차이가 있습니다.

- **"버그" 대 "드리프트":** 전통적 소프트웨어의 버그는 내부 로직의 결함, 즉 의도된 결정론적 행동으로부터의 이탈이며, 코드의 수정으로 해결합니다. 반면, ML 모델이 "부정확한" 예측을 하는 것은 <u>반드시 버그가 아닐 수</u> 있습니다. 모델은 훈련된 대로 정확하게 작동하고 있더라도, 실제 세계가 변하여 <u>학습된 패턴이 더 이상 유효하지 않기 때문</u>에 "실패"할 수 있습니다. 이는 **개념 드리프트(Concept Drift)** 라 합니다.
- **유지보수 철학:** 전통적 소프트웨어의 유지보수는 버그를 수정하고 기능을 추가하는 것을 포함합니다. 반면 ML 모델의 유지보수는 **지속적인 적응**의 철학을 포함합니다. 성능 저하에 대한 주된 "수정" 방법은 모델의 코드를 변경하는 것이 아니라, <span style="background:#fff88f">새로운 현실을 반영</span>하는 새로운 데이터로 모델을 **재훈련**하는 것입니다.

| 특성              | 전통적 소프트웨어 서빙     | ML 모델 서빙                                  |
| ----------------- | -------------------------- | --------------------------------------------- |
| **핵심 로직**     | 결정론적 (코딩된 규칙)     | 확률론적 (학습된 패턴)                        |
| **데이터 의존성** | 로직이 우선, 데이터는 입력 | 데이터가 로직의 핵심; "데이터가 새로운 코드"  |
| **진실의 원천**   | 코드베이스                 | 코드 + 데이터 + 모델 결과물                   |
| **실패 모드**     | 버그 (코드의 논리적 오류)  | 성능 저하 (개념 드리프트, 데이터 드리프트 등) |
| **유지보수**      | 코드 패치 및 업데이트      | 지속적인 모니터링, 재훈련, 버전 관리          |
| **출력**          | 결정론적 결과              | 내재적 불확실성을 가진 예측                   |
| **핵심 과제**     | 로직의 복잡성, 확장성      | 재현성, 데이터 품질, 드리프트 관리            |


## 모델 서빙 구성 요소

모델 서빙은 하드웨어, 소프트웨어, 워크플로우를 결합하여 AI 이니셔티브의 기반이 되는 '디지털 스캐폴딩(digital scaffolding)'을 세우는 것과 같습니다. 성공적인 모델 서빙 아키텍처는 동시 다발적으로 발생하는 여러 과제, 즉 <u>성능 최적화, 신뢰성 보장, 운영 확장성 등을 해결</u>해야 합니다.

### 모델 서버 (Model Server)

모델 서버는 서빙 시스템의 핵심으로, 학습된 ML 모델을 호스팅하고 제공하는 역할을 전담합니다. GPU나 CPU와 같은 하드웨어 <span style="background:#d3f8b6">리소스를 효율적으로 관리</span>하고, 들어오는 <span style="background:#d3f8b6">추론 요청을 처리</span>합니다. 모델 서버는 <u>모델을 메모리에 로드하고, 입력 데이터를 받아 추론을 실행한 후, 그 결과를 반환</u>하는 모든 과정을 책임집니다.

TensorFlow Serving, TorchServe, NVIDIA Triton Inference Server와 같은 전문 프레임워크는 종종 이 모델 서버의 역할을 수행하며, 특정 <u>프레임워크에 최적화된 성능과 기능을 제공</u>합니다.

### API 게이트웨이 (API Gateway)

API 게이트웨이는 모든 클라이언트 요청에 대한 단일화된 진입점(single point of entry) 역할입니다. 이는 전체 ML 서비스로 들어가는 '정문'과 같습니다. API 게이트웨이는 클라이언트가 복잡할 수 있는 백엔드 마이크로서비스와 <u>단순하게 상호작용할 수 있도록</u> 돕습니다.

API 게이트웨이의 주요 책임은 <span style="background:#d3f8b6">API 호출을 관리하고 라우팅하며, API 키나 OAuth 토큰을 이용한 인증 및 권한 부여를 처리</span>하는 것입니다. 또한, 서비스 남용을 방지하기 위해 <u>요청 비율 제한(rate limiting) 및 스로틀링(throttling)을 강제</u>하고, 필요한 경우 <u>HTTP에서 gRPC로 프로토콜을 변환하는 기능</u>도 수행합니다. 

### 로드 밸런서 (Load Balancer)

로드 밸런서는 들어오는 <u>네트워크 트래픽을 여러 모델 서버 인스턴스에 분산시켜 단일 서버가 과부하되지 않도록 보장</u>합니다. 로드 밸런서의 핵심 기능은 <u>서버 인스턴스의 상태를 주기적으로 확인(health check)</u>하고, <u>문제가 발생한 인스턴스로부터 트래픽을 자동으로 다른 정상 인스턴스로 재라우팅</u>하여 가용성과 내결함성을 향상시키는 것입니다. 로드 밸런서는 API 게이트웨이보다 낮은 수준에서 작동하며, 애플리케이션 수준의 로직보다는 <span style="background:#d3f8b6">네트워크 트래픽 분산에 중점</span>을 둡니다.


> <font color="#245bdb"><b>NOTE: API 게이트웨이와 로드 밸런서</b></font>
> 
> API 게이트웨이와 로드 밸런서는 모두 트래픽을 관리하지만, 해결하는 문제의 영역이 다릅니다. **로드 밸런서**는 서빙 인프라의 <span style="background:#d3f8b6">가용성을 보장</span>하는 반면, **API 게이트웨이**는 <span style="background:#d3f8b6">애플리케이션 로직에 대한 제어</span>를 제공합니다. 
> 
> 이 차이점은 아키텍처 설계에서 중요한 결정 지점을 만듭니다. 일반적인 대규모 아키텍처 패턴 중 하나는 여러 <u>API 게이트웨이 인스턴스 앞에 로드 밸런서를 배치</u>하는 것입니다. 이는 <u>게이트웨이 자체의 고가용성을 확보</u>하기 위함입니다. 만약 하나의 게이트웨이 인스턴스가 실패하면, 로드 밸런서가 트래픽을 정상적인 다른 인스턴스로 라우팅하여 시스템 전체의 다운타임을 방지합니다. 
> 
> 또 다른 패턴은 단일 게이트웨이를 진입점으로 삼고, 게이트웨이가 내부의 여러 마이크로서비스로 요청을 라우팅하며, 각 <u>마이크로서비스는 자체적인 내부 로드 밸런서</u>를 가질 수 있습니다. 이 패턴은 <u>외부 공격 표면을 단순화하고 정책 시행을 중앙 집중화하는 데 우선순위</u>를 둡니다. 
> 
> 따라서 로드 밸런서와 게이트웨이의 상대적 위치 선정은 임의적인 것이 아니라, <u>제어 플레인(게이트웨이)의 고가용성</u>을 확보할 것인지, 아니면 <u>외부 인터페이스를 단순화하고 제어를 중앙화</u>할 것인지에 대한 아키텍처적 트레이드오프의 결과입니다.

### 모델 레지스트리 (Model Registry)

모델 레지스트리는 학습된 ML 모델과 관련 아티팩트를 저장, 관리, 추적하기 위한 중앙 집중식 버전 관리 리포지토리입니다. 이는 모든 프로덕션 후보 모델에 대한 '신뢰할 수 있는 단일 소스(source of truth)' 역할을 합니다. 모델 레지스트리의 핵심 책임은 다음과 같습니다.

- **모델 리니지(Lineage) 추적:** 모델을 생성한 특정 코드, 데이터, 실험 실행에 다시 연결하여 완전한 추적성을 보장합니다.
- **버전 관리 및 스테이징:** 모델 버전을 관리하고, '스테이징(Staging)', '프로덕션(Production)'과 같은 단계(stage)를 관리하여 모델의 생명주기를 제어합니다.
- **메타데이터 저장:** 성능 지표, 하이퍼파라미터, 모델 설명 등 중요한 메타데이터를 저장하여 거버넌스와 재현성을 강화합니다. 이는 MLOps의 핵심적인 기반입니다.

## 추론 요청 생명주기

클라이언트의 요청부터 응답까지, 하나의 추론 요청이 처리되는 과정은 여러 단계의 파이프라인으로 구성됩니다. 이 과정을 세밀하게 추적하면 시스템 내부의 동작을 명확히 이해할 수 있습니다.

1. **클라이언트 요청:** 클라이언트 애플리케이션은 API 게이트웨이의 엔드포인트로 REST나 gRPC와 같은 프로토콜을 통해 요청을 보냅니다.
2. **게이트웨이 및 로드 밸런서:** 게이트웨이는 요청을 인증하고, 설정된 속도 제한을 확인한 후 요청을 전달합니다. 그 후 로드 밸런서는 가용한 모델 서버 인스턴스 중 하나를 선택하여 요청을 보냅니다.
3. **요청 처리 파이프라인:** 모델 서버 내부에서 요청은 다음과 같은 다단계 프로세스를 거칩니다.

	- **입력 유효성 검사 및 정제 (Validation & Sanitization):** 서버는 요청 페이로드가 예상된 스키마와 일치하는지 검증하고, 오류나 악의적인 주입을 방지하기 위해 입력을 정제합니다.
	- **전처리 (Preprocessing):** 이미지나 원시 텍스트와 같은 원본 입력 데이터는 모델이 기대하는 숫자 텐서 형식(예: 이미지 리사이징, 텍스트 토큰화, 정규화)으로 변환됩니다. 이 로직은 모델 학습 파이프라인에서 사용된 로직과 반드시 일치해야 합니다.
	- **추론 (Inference):** 전처리된 텐서는 CPU 또는 GPU에 로드된 모델로 전달됩니다. 모델은 순전파(forward pass)를 수행하여 출력 텐서를 생성합니다.
	- **후처리 (Postprocessing):** 로짓(logits)이나 클래스 인덱스와 같은 모델의 원시 출력은 사람이 읽을 수 있고 애플리케이션에서 사용하기 쉬운 형식(예: 클래스 이름, 확률값이 포함된 JSON)으로 변환됩니다.

4. **응답 생성:** 최종적으로 형식화된 출력은 응답 본문으로 패키징되어 네트워크 스택을 통해 클라이언트에게 다시 전송됩니다.


## 모델 서빙의 목표

AI 모델 서빙의 최종 목표는 기술적 우수성을 넘어 비즈니스 가치를 창출하는 것입니다. 이를 위해서는 기술적 결정이 경제적 결과에 미치는 영향을 깊이 이해해야 합니다.

### 주요 지표

AI 모델 서빙은 세 가지 상충되는 목표 사이에서 균형을 잡아야 하는 '삼중고(Trilemma)'에 직면합니다.

1. **성능 (지연 시간/처리량):** 서비스가 얼마나 빠르고 반응성이 좋은가.
2. **정확도:** 모델의 예측이 얼마나 정확한가 (F1-점수, 정밀도, 재현율 등으로 측정).
3. **비용:** 하드웨어, 클라우드 리소스, 엔지니어링 노력을 포함한 총소유비용(TCO).

이 세 가지 목표는 서로 긴밀하게 연결되어 있어 하나를 개선하면 다른 하나가 저하되는 경우가 많습니다. 예를 들어, 더 크고 복잡한 모델은 일반적으로 더 높은 정확도를 보이지만, 추론에 더 많은 시간과 컴퓨팅 자원을 필요로 하므로 성능(지연 시간)이 저하되고 비용은 증가합니다. 반대로, 공격적인 양자화와 같은 최적화 기법은 모델 크기를 줄여 성능을 높이고 비용을 절감하지만, 정확도를 희생시킬 수 있습니다.

> <font color="#245bdb"><b>NOTE: 전략적 의사결정 프레임워크: 파레토 최적 전선</b></font>
> 
> 이러한 복잡한 상충 관계 속에서 '최고의' 단일 솔루션을 찾는 것은 거의 불가능합니다. 대신, '파레토 최적(Pareto Optimal)'이라는 개념을 활용하여 합리적인 의사결정 프레임워크를 구축할 수 있습니다.
> 
> - **파레토 최적의 이해:** 어떤 솔루션이 파레토 최적이라는 것은, <u>다른 목표를 악화시키지 않고서는 하나의 목표를 더 이상 개선할 수 없는 상태</u>를 의미합니다. 예를 들어, 현재 모델보다 정확도를 높이려면 반드시 비용이나 지연 시간이 증가해야만 하는 경우, 현재 모델은 파레토 최적 상태에 있다고 할 수 있습니다.
> - **상충 관계의 시각화:** 다양한 모델과 그 구성(예: 양자화 적용 여부, 하드웨어 종류)을 2차원 또는 3차원 그래프(예: X축-비용, Y축-지연 시간, 색상-정확도)에 표시하면, '파레토 최적 전선(Pareto Frontier)'을 시각적으로 확인할 수 있습니다. 이 전선 위에 있는 모든 점들은 기술적으로 '효율적인' 선택지들입니다. 전선 안쪽에 있는 점들은 비효율적인데, 왜냐하면 동일하거나 적은 비용 및 지연 시간으로 더 높은 정확도를 달성하는 다른 점이 전선 위에 존재하기 때문입니다.
> 
> 어떤 파레토 최적점을 선택할지는 비즈니스와 제품 요구사항에 달려 있습니다. 예를 들어, 생명이 달린 의료 영상 진단 모델은 비용이 아무리 많이 들더라도 정확도가 가장 높은 점을 선택해야 합니다. 반면, 무료 사용자에게 제공되는 비핵심적인 추천 기능은 정확도를 다소 희생하더라도 비용이 가장 저렴한 점을 선택하는 것이 합리적입니다.
> 
> 이 프레임워크는 "가장 정확한 모델이 무엇인가?"라는 질문을 "우리의 비즈니스 제약 조건 하에서 가장 효율적인 모델은 무엇인가?"라는 더 전략적인 질문으로 전환시킵니다. 데이터 과학팀이 종종 정적 테스트 데이터셋에서 최고의 정확도 점수를 내는 모델을 '최고'라고 여기는 경향이 있습니다. 하지만 프로덕션 환경은 <u>비용과 성능이라는 두 가지 치명적인 제약 조건</u>을 추가합니다. 99%의 정확도를 가졌지만 응답에 10초가 걸리고 시간당 10달러의 비용이 드는 모델은, 100ms의 지연 시간과 엄격한 예산이 요구되는 실시간 애플리케이션에서는 사실상 쓸모가 없습니다. 이 경우, 97%의 정확도를 가졌지만 50ms 내에 응답하고 시간당 1달러의 비용이 드는 모델이 훨씬 더 가치 있습니다. 2%의 정확도 하락은 200배의 속도 향상과 10배의 비용 절감을 위한 합리적인 트레이드오프입니다. 따라서 프로덕션에서 '최고의' 모델은 학술적인 정확도 리더보드의 최상단에 있는 모델이 아니라, 비용-성능-정확도라는 파레토 최적 전선 위에서 비즈니스 목표와 가장 잘 부합하는 지점에 위치한 모델입니다. 이는 데이터 과학과 MLOps 팀이 모델을 총체적으로 평가하기 위해 긴밀히 협력하는 문화적 변화를 요구합니다.
> 
> - **비즈니스 목표에서 시작:** 개발을 시작하기 전에 허용 가능한 지연 시간, 최소 정확도, 그리고 예산 한도를 명확히 정의해야 합니다.
> - **반복적인 최적화:** MLOps 원칙에 따라 다양한 모델, 하드웨어(CPU vs. 다양한 GPU), 최적화 기법을 체계적으로 실험하고 그 결과를 파레토 전선에 플로팅하여 최적의 조합을 찾아야 합니다.
> - **자원의 적정 규모화 (Right-Sizing):** 오토스케일링(특히 KPA의 Scale-to-Zero)과 MIG 같은 하드웨어 관리 기법을 적극 활용하여, 실제로 필요한 만큼의 리소스에 대해서만 비용을 지불하도록 시스템을 구성해야 합니다.
> - **모델 캐스케이딩 (Model Cascading):** 간단하고 저렴한 모델로 대부분의 쉬운 요청을 처리하고, 오직 어려운 요청만이 복잡하고 비싼 모델로 전달되도록 라우팅하는 고급 전략입니다. 이는 시스템 전체의 비용-성능 곡선을 최적화하는 데 매우 효과적일 수 있습니다.

### 비즈니스 가치 및 ROI

모델 서빙은 그 자체로 목적이 아니라, 실질적인 비즈니스 가치를 창출하기 위한 수단입니다. 모든 AI 이니셔티브의 성공은 조직의 목표에 얼마나 기여했는지로 측정됩니다. 이를 위해서는 모델의 예측이 비즈니스 결과로 이어지는 명확한 경로를 설정해야 합니다.

AI/ML 애플리케이션은 사기 탐지, 고객 이탈 감소, 실시간 추천, 예측 유지보수와 같은 기능을 통해 시장에서 결정적인 차별화 요소로 작용합니다. 성공적인 AI 프로젝트는 명확한 비즈니스 문제를 식별하는 것에서 시작하며, "잘못된 예측이 얼마나 큰 비용을 초래하는가?"라는 질문을 통해 프로젝트의 타당성을 검토합니다. 

비즈니스 가치는 AI 솔루션의 전체 수명 주기에 걸쳐 개념 단계부터 제품화까지 점진적으로 평가되어야 합니다. 가치를 창출하는 핵심 동인으로는 직원 생산성 향상, 매출 증대, 비용 효율성 개선, 고객 경험 향상 등이 있습니다. 예를 들어, 반복적인 작업을 자동화함으로써 수만 시간의 업무 시간을 절약하고 생산성을 25% 이상 향상시킬 수 있습니다.

이러한 가치를 체계적으로 실현하기 위해서는 우선순위 결정 기준이 필수적입니다. 높은 비즈니스 가치와 높은 실행 가능성을 가진 프로젝트, 즉 '빠른 성공(quick wins)'을 최우선으로 추진하고, 가치는 높지만 실행 가능성이 낮은 프로젝트는 미래 투자를 위해 전략적으로 평가해야 합니다.



#### FinOps

이는 MLOps와 FinOps의 결합으로 표현할 수 있습니다. LLM과 같이 복잡하고 자원 집약적인 모델의 등장은 재무 관리에 초점을 맞춘 FinOps 간의 긴밀한 통합을 요구합니다. 단순히 모델을 배포하는 것만으로는 충분하지 않으며, 비용 효율적이고 아키텍처적으로 최적화된 방식으로 배포해야 합니다. AI 프로젝트, 특히 LLM은 막대한 기술적, 재무적 부채를 유발할 수 있습니다. 

MLOps는 배포 파이프라인을 간소화하여 속도와 안정성을 보장합니다. 반면, FinOps는 근본적인 비용 효율성에 의문을 제기합니다. 해당 모델이 정말로 <u>비용을 절감하고 있는지, 아니면 "기존의 비효율성을 자동화"하고 있을 뿐인지, 선택된 인프라(예: 인스턴스 유형, 배포 전략)가 워크로드에 최적인지</u>를 묻습니다.

결론적으로, MLOps는 모델이 '실행될 수 있도록' 보장하고, FinOps는 <u>현재 구성으로 '실행되어야 하는지'를 보장</u>합니다. FinOps 없이는 MLOps 파이프라인이 과도하게 크고 비싼 GPU에 모델을 효율적으로 배포하여 ROI를 음수로 만들 수 있습니다. MLOps 없이는 FinOps가 승인한 예산이 느리고 신뢰할 수 없는 수동 배포로 인해 낭비될 수 있습니다. 따라서 모델 서빙의 진정한 ROI를 측정하기 위해서는 MLOps 지표(배포 빈도, 모델 성능)와 FinOps 지표(총소유비용(TCO), 추론당 비용, 자원 활용률)를 결합한 전체적인 관점이 필요합니다.

## 모니터링

중요한 모니터링 대상에는 <u>시스템 성능뿐만 아니라, 시간이 지남에 따라 발생하는 데이터 드리프트(data drift)와 모델 성능 저하도 포함</u>됩니다. 이러한 변화는 실제 환경의 변화에 대응하여 모델이 "엉망이 되는(go haywire)" 것을 방지하는 데 중요합니다.



## 파트 II: 모델 서빙의 아키텍처 진화

이 파트에서는 모델 서빙의 근본적인 기술적 변화를 상세히 다룹니다. 특히 거대 언어 모델(LLM)의 고유한 특성이 어떻게 전통적인 머신러닝(ML) 서빙 시스템의 가정을 무너뜨리고, 성능 목표와 시스템 아키텍처에 대한 전면적인 재고를 강요했는지 설명합니다.

### 2.1: 전통적 머신러닝 서빙: 기초적 관점

<u>전통적인 ML 서빙</u>은 구조화되거나 반구조화된 데이터에 대해 분류나 회귀와 같은 특정 판별적(discriminative) 작업을 수행하는 모델을 위해 설계되었습니다. 주요 목표는 <u>단일 예측에 대한 정확성, 낮은 지연 시간, 그리고 마이크로서비스로서의 관리 용이성</u>이었습니다.

이러한 모델들은 특정 문제를 해결하기 위해 처음부터 구축되는 경우가 많으며, 깊은 도메인 전문 지식과 피처 엔지니어링을 필요로 합니다. 추론은 일반적으로 단일의 병렬화 가능한 순방향 패스(forward pass)로 이루어집니다. 입력 크기가 고정되거나 제한적이어서 예측 가능한 계산 부하를 가집니다. 배포는 종종 모델을 마이크로서비스로 패키징하는 간단한 방식으로 이루어지며, 재학습도 쉽게 자동화할 수 있습니다. 주요 초점은 사기 탐지나 주택 가격 예측과 같이 명확하게 정의된 문제에 맞춰져 있습니다.

### 2.2: 패러다임 전환: 거대 언어 모델(LLM)의 고유한 요구사항

LLM은 근본적으로 다릅니다. 이들은 생성적(generative)이며, 방대한 양의 비정형 텍스트를 처리하고, 종종 처음부터 구축되기보다는 <u>거대한 파운데이션 모델(foundation model)을 기반으로 조정</u>됩니다. 이러한 차이점들은 확장성, 지연 시간, 메모리 관리 측면에서 전통적인 프레임워크가 처리할 수 없는 전례 없는 과제들을 야기합니다.

LLM은 방대한 파라미터 수로 특징지어지며, 이로 인해 메모리 집약적입니다. 이들은 특정 예측뿐만 아니라 텍스트 요약이나 코드 생성과 같은 광범위한 생성 작업을 위해 사용됩니다. 추론 과정은 단일 패스가 아니라 자기회귀 디코딩(autoregressive decoding)이라는 순차적이고 반복적인 과정입니다. 사용자 요청은 입력 및 출력 길이가 매우 다양하여 예측 불가능한 계산 부하와 메모리 사용량을 초래합니다. 이러한 변화는 <u>미세 조정(fine-tuning), 프롬프트 튜닝, 검색 증강 생성(RAG), 인간 피드백 기반 강화 학습(RLHF) 등</u> LLM 수명 주기에 맞춰진 MLOps의 진화된 형태인 LLMOps라는 전문 분야의 발전을 이끌었습니다.

아래 표는 전통적 ML 서빙과 LLM 서빙 패러다임의 주요 차이점을 요약합니다.

| 속성                      | 전통적 머신러닝                                 | 거대 언어 모델 (LLM)                                     |
| ------------------------- | ----------------------------------------------- | -------------------------------------------------------- |
| **주요 목적**             | 패턴 식별 및 예측 (판별적)                      | 언어 이해 및 생성 (생성적)                               |
| **데이터 유형**           | 구조적 및 반구조적 (테이블, 레이블 데이터)      | 비정형 텍스트 (책, 웹사이트, 기사)                       |
| **모델 아키텍처**         | 다양한 알고리즘 (결정 트리, SVM, 간단한 신경망) | 트랜스포머 아키텍처                                      |
| **추론 패턴**             | 단일, 병렬화 가능한 순방향 패스                 | 반복적, 순차적 자기회귀 디코딩                           |
| **주요 병목 현상**        | CPU 연산, 피처 엔지니어링                       | GPU 메모리 대역폭, VRAM 용량                             |
| **핵심 성과 지표 (KPIs)** | 정확도, 단일 요청 지연 시간                     | 처리량 (토큰/초), 첫 토큰까지의 시간 (TTFT), 토큰당 비용 |
| **배포 복잡성**           | 상대적으로 낮음 (마이크로서비스)                | 매우 높음 (특수 서빙 시스템 필요)                        |
| **일반적인 사용 사례**    | 사기 탐지, 고객 분류, 수요 예측                 | 챗봇, 텍스트 요약, 코드 생성, 콘텐츠 제작                |

표 1: 전통적 ML 서빙과 LLM 서빙 패러다임 비교. 이 표는 두 패러다임의 근본적인 차이점을 명확히 하여, LLM 서빙이 왜 더 어려운 문제인지를 보여줍니다. 이 정보는 5의 내용을 종합하여 구성되었습니다.

### 2.3: 심층 분석: 자기회귀 디코딩과 KV 캐싱이 서빙 목표를 재정의한 방식

LLM 추론의 두 가지 특정 특성, 즉 자기회귀 디코딩(autoregressive decoding)과 KV 캐싱(KV caching)은 LLM 서빙의 주요 기술적 과제의 근본 원인입니다. 이들은 성능 병목 현상을 순수하게 연산 집약적인 문제에서 심각한 메모리 집약적 및 지연 시간에 민감한 문제로 근본적으로 변화시켰습니다.

- **자기회귀 디코딩:** LLM은 한 번에 하나의 토큰을 생성하며, 각 새로운 토큰은 이전의 모든 토큰에 의존합니다. 이 순차적인 과정은 본질적으로 느리고 병렬화를 어렵게 만듭니다. 프롬프트를 처리하는 첫 번째 단계를 "프리필(prefill)"이라고 하며, 이후 토큰별 생성 단계를 "디코딩(decode)"이라고 합니다. 이 두 단계는 매우 다른 연산 프로필(연산 집약적 vs. 메모리 대역폭 집약적)을 가집니다.
- **KV 캐싱:** 매 단계마다 전체 시퀀스에 대한 어텐션 메커니즘을 재계산하는 것을 피하기 위해, 시스템은 <u>중간 어텐션 상태(키와 값)를 "KV 캐시"에 저장</u>합니다. 이 캐시는 생성되는 모든 토큰과 함께 크기가 커집니다. KV 캐시는 추론 중 <u>GPU 메모리의 주요 소비자이며, 종종 전체 메모리 사용량의 대부분을 차지</u>합니다.

이 두 가지 특성은 전통적인 ML에서는 볼 수 없었던 방식으로 처리량, 지연 시간, 활용률이라는 세 가지 핵심 지표 사이에 새로운 악순환의 상충 관계를 만들어냅니다. 이는 전체 분야가 단순한 성능 목표에서 벗어나 복잡하고 상호 의존적인 시스템을 공동으로 최적화하도록 강요했습니다. 이 과정은 다음과 같이 전개됩니다.

1. **목표 1: 처리량 극대화.** 이를 위한 명백한 방법은 한 번에 많은 요청을 처리하는 배치 크기를 늘리는 것입니다.
2. **KV 캐시 문제 발생.** 배치 크기가 커지면 더 많은 동시 요청을 의미합니다. 각 요청은 동적으로 커지는 KV 캐시를 가지고 있으며, 모든 KV 캐시에 필요한 총 메모리는 GPU의 VRAM을 빠르게 초과합니다. 이는 가능한 최대 배치 크기를 제한하여 처리량을 제한하는 결과로 이어집니다.
3. **자기회귀 문제 (지연 시간).** 큰 배치를 메모리에 맞출 수 있더라도, 요청들은 서로 다른 길이를 가집니다. 단순한 <u>"정적 배치(static batching)" 시스템에서는 전체 배치가 가장 긴 요청이 토큰 생성을 마칠 때까지 기다려야</u> 합니다. 이는 막대한 유휴 시간(GPU 비활용)을 발생시키고 모든 짧은 요청의 평균 지연 시간을 증가시킵니다. 이는 선두 차단(Head-of-Line, HOL) 블로킹으로 알려져 있습니다.
4. **결과적인 삼중고(Trilemma).**

   - **높은 처리량**(큰 배치)을 얻으려면 막대한 메모리가 필요하지만, HOL 블로킹으로 인해 **높은 지연 시간**과 **낮은 활용률**에 시달립니다.
   - **낮은 지연 시간**(작은 배치 또는 순차 처리)을 얻으려면 **처리량**과 GPU **활용률**이 매우 낮아집니다.
   - **높은 활용률**을 얻으려면 GPU를 작업으로 가득 채워야 하지만, 이는 다시 메모리 및 지연 시간 문제로 이어집니다.

결론적으로, 현대 LLM 서빙의 핵심 과제는 이 삼중고를 깨는 것입니다. 다음 파트에서 논의될 모든 주요 혁신 기술, 즉 PagedAttention, 연속 배치, 반복 수준 스케줄링 등은 이 상호 연결된 문제를 해결하기 위한 직접적인 시도입니다. 이 기술들은 메모리를 더 효율적으로 관리하고 작업을 더 세밀하게 스케줄링함으로써 지연 시간을 희생하지 않으면서 높은 활용률과 처리량을 달성하는 것을 목표로 합니다.

## 파트 III: LLM 서빙의 최전선: 핵심 기술 과제와 혁신

이 파트는 파트 II에서 확인된 삼중고를 해결하기 위해 설계된 최첨단 시스템과 알고리즘에 대한 심층 분석입니다. 최근 학술 논문에서 많은 내용을 참조하며, 아래 표는 이 분야의 주요 혁신을 요약합니다.

| 시스템 이름           | 주요 혁신 기술                                         | 해결한 주요 문제                       | 관련 학술 자료/출처                 |
| ---------------- | ------------------------------------------------ | ------------------------------- | --------------------------- |
| **Orca**         | 반복 수준 스케줄링 (연속 배치)                               | 선두 차단(HOL) 블로킹, 낮은 GPU 활용률      | Yu et al., OSDI '22 16      |
| **vLLM**         | PagedAttention                                   | KV 캐시 메모리 단편화                   | Kwon et al., SOSP '23 19    |
| **TensorRT-LLM** | PagedAttention, In-flight Batching, Quantization | 처리량 및 지연 시간 최적화                 | NVIDIA 19                   |
| **vAttention**   | CUDA VMM API 기반 메모리 관리                           | PagedAttention의 커널 재작성 부담 및 이식성 | Prabhu et al., arXiv '25 22 |
| **Llumnix**      | 동적 스케줄링, 요청 실시간 마이그레이션                           | 자원 단편화, 부하 불균형, SLO 위반          | Sun et al., OSDI '24 17     |
| **S-LoRA**       | 통합 페이징, 이기종 배치 커널                                | 수천 개의 LoRA 어댑터 동시 서빙            | Sheng et al., MLSys '24 16  |
| **DistKV-LLM**   | 분산 어텐션 및 KV 캐시 관리                                | 클라우드 환경의 대규모 컨텍스트 처리            | Qiu et al., arXiv '24 29    |

_표 2: 최첨단 LLM 서빙 시스템 및 혁신 요약. 이 표는 LLM 서빙 분야의 복잡한 환경을 구조화된 지도로 제공하여, 독자가 주요 시스템과 그 기여를 쉽게 파악할 수 있도록 돕습니다._

### 3.1: 메모리 병목 현상: 단편화에서 효율적 관리로

각 요청에 대한 KV 캐시의 동적이고 예측 불가능한 크기는 GPU VRAM에서 심각한 메모리 단편화를 유발하여 귀중한 자원을 낭비하고 배치 크기를 제한합니다. 이에 대한 해결책은 운영 체제에서 개념을 차용하는 것이었습니다. <u>전통적인 메모리 할당 방식은 각 요청에 대해 가능한 최대 시퀀스 길이에 해당하는 연속적인 메모리 블록을 예약</u>합니다. 대부분의 시퀀스는 이보다 짧기 때문에, 이는 <u>막대한 내부 단편화와 메모리 낭비</u>를 초래합니다.

#### 3.1.1: PagedAttention (vLLM): GPU를 위한 OS 페이징의 재창조

운영 체제의 가상 메모리에서 영감을 받은 PagedAttention은 KV 캐시를 고정된 크기의 "페이지" 또는 "블록"으로 분할합니다. 이 페이지들은 물리적 GPU 메모리에 비연속적으로 저장될 수 있습니다. "블록 테이블"은 토큰의 논리적 시퀀스를 이러한 비연속적인 물리적 블록에 매핑하는 역할을 합니다.

이 방식은 메모리가 <u>작은 블록 단위로 필요에 따라 할당</u>되므로 내부 단편화를 제거합니다. 이를 통해 훨씬 더 큰 배치 크기를 사용할 수 있게 되어 처리량을 극적으로 향상시킵니다. 또한 복잡한 샘플링 전략을 위한 효율적인 메모리 공유(쓰기 시 복사, copy-on-write)를 가능하게 합니다. PagedAttention을 구현한 vLLM은 TensorRT-LLM 및 HuggingFace TGI와 같은 서빙 시스템에서 사실상의 표준이 되었습니다.

#### 3.1.2: PagedAttention을 넘어서: vAttention 접근법과 커널 이식성

PagedAttention은 효과적이지만 단점도 있습니다. 비연속적인 메모리를 처리하기 위해 핵심적인 <u>어텐션 GPU 커널을 재작성</u>해야 하는데, 이는 어렵고 유지 관리 부담을 만듭니다. FlashAttention과 같은 새롭고 고도로 최적화된 어텐션 커널을 따라잡기 어렵습니다. 또한 사용자 공간에서 운영 체제 기능을 중복으로 구현하게 됩니다.

vAttention은 다른 접근법을 취합니다. KV 캐시를 <u>가상적으로는 연속적으로 유지</u>하면서, **저수준 CUDA 가상 메모리 관리(VMM) API**를 사용하여 이러한 가상 주소를 필요에 따라 <u>비연속적인 물리적 메모리 페이지에 매핑합</u>니다. 가상 메모리 레이아웃이 변경되지 않기 때문에, vAttention은 수정 없이 표준의 고도로 최적화된 어텐션 커널(예: FlashAttention)을 "즉시(out-of-the-box)" 사용할 수 있습니다. 이는 이식성과 성능을 향상시킵니다. 그러나 이 방식은 CUDA VMM API 호출의 <u>높은 지연 시간과 큰 최소 페이지 크기(2MB)</u>와 같은 자체적인 과제를 안고 있습니다. vAttention은 이러한 문제를 <u>선제적 할당 및 더 작은 페이지 크기(64KB)를 위한 커스텀 커널 드라이버</u>와 같은 최적화를 통해 해결합니다.

### 3.2: 처리량 과제: 동시 요청을 위한 고급 스케줄링

선두 차단(HOL) 블로킹을 극복하고 이기종 요청을 효율적으로 스케줄링하기 위해, 시스템은 거친 입도(요청 수준)에서 세분화된 입도(반복 수준) 스케줄링으로 이동했습니다.

#### 3.2.1: 연속 배치 및 반복 수준 스케줄링 (Orca)

FCFS(First-Come-First-Serve) 스케줄링은 짧은 요청이 긴 요청에 의해 차단되는 문제를 야기합니다. 정적 배치는 패딩을 필요로 하고, 배치 내의 모든 요청이 동일한 단계 수만큼 실행되도록 강제하여 연산 자원을 낭비합니다.

Orca는 "반복 수준 스케줄링"을 도입했으며, 이는 현재 **연속 배치(continuous batching)** 로 널리 알려져 있습니다. 전체 요청을 스케줄링하는 대신, 스케줄러는 _매 단일 토큰 생성 단계마다_ 결정을 내립니다. 실행 중인 배치에 새로운 요청을 추가하거나 완료된 요청을 제거할 수 있습니다. 이 방식은 패딩의 필요성을 제거하고 GPU 유휴 시간을 최소화하여 처리량과 활용률을 크게 향상시킵니다. 이는 모든 현대 서빙 시스템의 기초적인 최적화 기술입니다.

#### 3.2.2: 동적 스케줄링, 요청 마이그레이션 및 공정성

연속 배치를 사용하더라도 FCFS는 최적이 아닙니다. <u>요청은 길이, 우선순위, SLO(서비스 수준 목표) 요구사항</u> 면에서 이기종입니다. 긴 프리필 요청은 여전히 많은 짧은 디코딩 요청을 차단할 수 있습니다. 최근 연구는 더 지능적인 스케줄링에 초점을 맞추고 있습니다.

- **최단 작업 우선(SJF) 근사:** 일부 시스템은 작은 프록시 모델을 사용하여 요청의 출력 길이를 예측한 다음, <u>짧은 작업을 우선 처리</u>하여 평균 지연 시간을 줄입니다.
- **분리(Disaggregation):** 연산 집약적인 "프리필" 단계와 메모리 집약적인 "디코딩" 단계를 분리하여 <u>각각 다른 특화된 자원에서 스케줄링</u>합니다.
- **실시간 마이그레이션(Live Migration):** <u>자원 단편화와 부하 분산을 처리</u>하기 위해, Llumnix와 같은 시스템은 실행 중인 요청과 그 메모리 내 KV 캐시를 최소한의 다운타임으로 한 GPU 인스턴스에서 다른 인스턴스로 실시간 마이그레이션할 수 있습니다.
- **공정성 및 우선순위:** 서로 다른 <u>요청 우선순위(예: 실시간 vs. 배치)를 처리하고 클라이언트 간의 공정성을 보장하기 위한 스케줄러</u>가 개발되고 있습니다.

### 3.3: 개인화의 확장: 수천 개의 미세 조정 모델 서빙 (LoRA)

LoRA와 같은 파라미터 효율적 기법을 사용하여 수천 명의 다른 고객이나 작업을 위해 기본 LLM을 미세 조정하는 것이 일반화되고 있습니다. 이 수천 개의 "어댑터"를 공유 기본 모델 위에서 서빙하는 것은 독특하고 심각한 메모리 및 배치 문제를 야기합니다.

- **과제 1: 메모리 관리:** 모든 LoRA 어댑터 가중치를 GPU 메모리에 유지하는 것은 불가능합니다. 필요할 때 <span style="background:#d3f8b6">호스트 메모리에서 동적으로 가져와야</span> 합니다. 이는 각 요청의 KV 캐시와 결합되어 극심한 메모리 단편화와 I/O 오버헤드를 유발합니다.
- **과제 2: 배치:** 기본 모델 연산은 쉽게 배치 처리할 수 있습니다. 그러나 LoRA 관련 연산(저계급 어댑터 행렬과의 곱셈)은 <u>각 어댑터가 다른 랭크를 가질 수 있고 비연속적인 메모리에 저장</u>되기 때문에 이기종입니다. 이러한 <span style="background:#d3f8b6">연산을 효율적으로 배치 처리하려면 커스텀 CUDA 커널이 필요</span>합니다.
- **해결책:** S-LoRA와 같은 시스템은 통합 페이징(KV 캐시와 어댑터 가중치를 <u>단일 페이징 메모리 시스템에서 관리</u>), 이기종 배치를 위한 커스텀 커널, 그리고 최적의 성능을 위해 어댑터를 기본 모델과 병합/분리할 시기를 결정하는 크레딧 기반 알고리즘과 같은 기술을 개발하고 있습니다.

### 3.4: 다중모달의 복잡성: 텍스트를 넘어서는 모델 서빙

이미지, 비디오, 오디오를 처리하는 다중모달 거대 언어 모델(MLLM)의 등장은 특히 초기 인코딩 단계에서 메모리 소비와 지연 시간 측면에서 새로운 차원의 복잡성을 야기합니다.

- **인코딩 병목 현상:** 텍스트가 아닌 모달리티(예: 이미지 또는 비디오)를 LLM이 이해할 수 있는 형식으로 인코딩하는 초기 단계는 <u>연산 비용이 많이 들고, LLM의 어텐션 메커니즘에 매우 큰 입력을 생성</u>합니다. 이로 인해 <u>메모리 소비가 최대 100배까지 증가하고 첫 토큰까지의 시간(TTFT)이 크게 늘어날 수</u> 있습니다.
- **기존 최적화의 한계:** 긴 <u>텍스트 LLM을 위해 설계된 최적화(예: KV 캐시 압축)를 적용할 수 있지만, 이는 정확도를 저하</u>시킬 수 있으며 단일 요청 내의 다른 모달리티나 여러 모달리티의 고유한 특성을 고려하지 못합니다.
- **새로운 과제:** MLLM을 서빙하려면 <span style="background:#d3f8b6">새로운, 모달리티 인식 스케줄링이 필요</span>합니다. 큰 비디오 입력을 포함하는 요청은 FCFS 큐에서 텍스트 전용 요청과 동일하게 처리될 수 없습니다. 그렇게 하면 지연 시간에 민감한 채팅 애플리케이션에 심각한 HOL 블로킹을 유발할 수 있기 때문입니다. 이는 <span style="background:#d3f8b6">MLLM을 위해 특별히 설계된 새로운 시스템과 워크로드 추적을 필요</span>로 합니다.

### 5.2: AI 에이전트와 자율 시스템의 부상

다음 단계는 단순한 요청-응답 서빙을 넘어, 복잡한 목표를 이해하고, 계획을 세우며, 다른 모델을 포함한 도구를 사용하여 이를 실행할 수 있는 자율적인 AI 에이전트를 배포하는 것입니다.

- **서빙에 대한 시사점:**
  - 이는 복잡성을 크게 증가시킵니다. 에이전트를 서빙하는 것은 단일 모델을 서빙하는 것이 아니라, <u>모델, 도구, 상태 관리의 전체 시스템을 서빙</u>하는 것입니다.
  - 에이전트는 <u>동적인 다단계 워크플로우, 도구 호출, 장기 기억(RAG는 이의 기초 구성 요소임)을 처리</u>할 수 있는 더 정교한 서빙 인프라를 필요로 할 것입니다.
  - 에이전트 AI로의 추세는 이러한 자율 시스템의 백본으로서 견고하고 확장 가능하며 안전한 모델 서빙의 필요성을 더욱 강화할 것입니다.

### 5.3: 결론 및 전략적 권장 사항

이 보고서는 모델 서빙이 단순한 배포 단계에서 복잡하고 전략적인 분야로 진화한 여정을 요약했습니다. 이러한 진화는 판별적 ML에서 생성적 LLM으로의 전환에 의해 주도되었으며, 운영 체제, 분산 컴퓨팅, 비즈니스 전략의 아이디어를 융합하도록 강요했습니다.

주요 동향을 요약하면, 데이터 센터에서의 대규모 확장과 엣지에서의 초특화가 동시에 추진되고 있으며, 책임감 있는 AI와 거버넌스의 중요성이 증가하고 있고, 오픈 소스 모델과 더 접근하기 쉬운 서빙 도구를 통해 AI가 민주화되고 있습니다.

이러한 분석을 바탕으로 다음과 같은 전략적 권장 사항을 제시합니다.

1. **전체적인 관점 채택:** 조직은 모델 서빙을 기술적인 사후 고려 사항이 아니라 핵심 비즈니스 역량으로 취급해야 하며, MLOps, FinOps, 보안을 첫날부터 통합해야 합니다.
2. **특화된 인프라에 투자:** 기성 솔루션은 더 이상 LLM에 충분하지 않습니다. 경쟁력 있는 성능과 비용 효율성을 위해서는 vLLM과 같은 특화된 서빙 시스템 및 인프라에 대한 투자가 필요합니다.
3. **하이브리드 미래 준비:** 미래는 단지 클라우드에만 국한되지 않습니다. 클라우드, 엣지, 연합 환경에 걸친 하이브리드 및 분산된 모델 서빙의 세계에 대비해야 합니다.
4. **거버넌스 및 보안 우선순위 지정:** 모델이 더욱 강력해지고 자율적으로(에이전트) 변함에 따라, 견고한 거버넌스, 보안, 그리고 인간 참여형(human-in-the-loop) 감독의 필요성은 협상의 여지가 없습니다.





### 섹션 4. 적응의 철학: 동적인 현실에 맞서기

#### 4.1 개념 드리프트: 비영속성의 수용

개념 드리프트는 입력 피처와 목표 변수 간의 근본적인 관계가 시간이 지남에 따라 변하는 현상이다. 2020년에 스팸을 탐지하도록 훈련된 모델은 2025년에는 실패할 수 있는데, 이는 스팸을 구성하는 바로 그 _개념_ 자체가 진화했기 때문이다.

개념 드리프트를 수용한다는 것은 "완성된" 모델이라는 아이디어를 포기하는 것을 의미한다. 배포된 모델은 정적인 자산이 아니라, 세상에 대한 동적인 가설이며 <span style="background:#fff88f">지속적으로 검증</span>되어야 한다. 이는 안정적인 소프트웨어 구성 요소에 때때로 적용될 수 있는 "배포하고 잊어버리는" 사고방식과는 근본적으로 다르다.

#### 4.2 인식으로서의 모니터링: 시스템의 눈과 귀

전통적인 모니터링은 <u>CPU/메모리 사용량, 지연 시간, 오류율</u>과 같은 운영 메트릭에 중점을 둔다. MLOps 모니터링은 <u>이를 포함하면서도 더 나아가</u>야 한다. 그것은 모델을 위한 "인식" 시스템으로서 기능해야 한다.

드리프트 감지를 위한 필수 모니터링 신호는 다음과 같다.

- **모델 성능 메트릭:** <span style="background:#fff88f">실제 값(ground truth)</span>을 얻을 수 있는 경우, <u>정확도, 정밀도, 재현율, F1-score</u>와 같은 직접적인 측정 지표를 추적한다.
- **예측 드리프트:** <span style="background:#fff88f">모델 출력의 통계적 분포</span>를 모니터링한다. 대출 승인 모델이 갑자기 30%가 아닌 90%의 신청자를 승인하기 시작한다면, 무언가 변했을 가능성이 높다.
- **데이터 드리프트 및 품질:** <span style="background:#fff88f">입력 데이터의 통계적 분포와 무결성</span>을 모니터링한다. 여기에는 <u>null 값 비율, 데이터 유형 오류, 피처 분포의 변화(Kolmogorov-Smirnov 테스트, PSI 등) 추적</u>이 포함된다. 이는 종종 잠재적인 개념 드리프트의 가장 빠른 경고 신호이다.

#### 4.3 절충의 실용주의: 가능성의 예술

모델 서빙은 상충하는 목표들 사이의 끊임없는 균형 잡기 행위이다. 무한히 빠르고, 무한히 확장 가능하며, 완벽하게 정확하고, 무료인 이상적인 시스템은 존재하지 않는다.

- **지연 시간 대 처리량(Throughput):** <u>낮은 지연 시간</u>(한 사용자에 대한 빠른 응답)을 위해 최적화하는 것은 종종 요청을 개별적으로 처리하는 것을 포함하며, 이는 <u>전체 처리량을 제한</u>할 수 있다. <u>높은 처리량</u>(초당 많은 사용자)을 위해 최적화하는 것은 종종 요청을 일괄 처리하는 것을 포함하며, 이는 각 <u>개별 요청에 대한 지연 시간을 증가</u>시킨다.
- **비용 대 성능:** 더 강력한 하드웨어(예: GPU 대 CPU)는 더 나은 성능을 제공하지만 더 높은 비용이 든다. 선택은 <u>모델의 특정 요구 사항과 속도의 비즈니스 가치</u>에 따라 달라진다.
- **정확도 대 단순성/속도:** 더 <u>복잡한 모델이 더 정확할 수</u> 있지만, <u>실행 속도가 느리고 서빙 비용이 더 비싼 경우</u>가 많다. "실용적 정확성(practical accuracy)"의 원칙은 순위표에서 가장 높은 점수를 받은 모델이 아니라, <u>비즈니스 문제에 "충분히 좋은" 모델을 선택하도록 지시</u>한다.

이러한 적응의 철학은 MLOps를 단순한 기술 프랙티스의 집합을 넘어, 진정으로 적응적이고 지능적인 시스템을 만드는 모델로 격상시킨다. <span style="background:#fff88f">CI/CD, 드리프트 모니터링, 자동화된 재훈련</span>의 조합은 완전한 사이버네틱스(cybernetics) 피드백 루프를 형성한다. 시스템은 단순히 배포되는 것이 아니라, <u>모니터링을 통해 환경을 능동적으로 '인식'하고, 원하는 상태(메트릭 임계값)와 성능을 비교하며, 항상성(비즈니스 가치)을 유지하기 위해 수정 조치(재훈련/재배포)</u>를 취한다. 전통적인 CI/CD 파이프라인이 선형적인 "푸시" 시스템인 반면, MLOps는 라이브 서비스로부터 <u>통계를 수집하여 드리프트를 감지하고, 이를 파이프라인 재실행의 트리거로 사용</u>하는 피드백 경로를 추가함으로써 이 루프를 완성한다. 이것이 바로 피드백을 통해 자가 조절하는 사이버네틱스 시스템의 정의이다.

## 마치며


> 본 포스트는 Google Gemini의 응답을 기반으로 저의 의견을 반영하여 다시 작성했습니다.