[{"content":"이 블로그에서는 엔지니어로서 시스템을 구현하기 위해 알아야 할 지식과 제 생각을 정리하면서 커리어를 위한 노력들을 쌓아가려 합니다.\n시기에 따라 업데이트가 늦어지는 경우도 있겠지만, 매주 월요일, 목요일마다 포스트를 작성하는 것을 목표로 하고 있습니다.\n작성 날짜는 글 작성을 시작한 시점을 기준으로 작성했으며, 동일 주제에 대한 추가 내용이 있을 경우 새 글 작성과 별개로 기존 포스트에 업데이트하는 경우도 있습니다. 다만 새로운 내용을 방문자분들께 알리기 위해 가급적이면 별도의 글로 작성하려 합니다.\n카테고리 카테고리는 글의 성격에 따라 하위 항목 중 하나를 갖습니다:\n시스템 (Systems) 구현하려는 대상을 주제로 글을 작성합니다.\n시스템 설계 및 아키텍처 구현 과정에서의 고려사항 및 의사 결정 성능 최적화 및 확장성 논의 실제 시스템 구축 경험 및 노하우 이론 (Theories) 소프트웨어 엔지니어로서 알아야 할 지식을 주제로 글을 작성합니다.\n핵심 개념 및 정의 동작 원리 및 메커니즘 학술적인 이론 및 논문 내용 요약/분석 기본적인 개념 설명 및 심화 학습 가이드 스킬 (Skills) 기술/라이브러리/프레임워크를 주제로 글을 작성합니다.\n특정 기술/라이브러리/프레임워크 활용 가이드 실전 예제 및 코드 스니펫 팁과 트릭, 모범 사례 프로젝트 (Projects) 프로젝트를 진행하면서 고민한 내용이나 접근 과정을 주제로 글을 작성합니다.\n프로젝트의 계기 및 목표 요구사항 분석 및 설계 과정 기술 스택 선택 및 구현 과정 문제 해결 과정 및 교훈 프로젝트 결과물 및 회고 태그 태그는 글의 주제 및 관련 항목에 따라 작성합니다. 아래는 그 예시입니다:\n기술/라이브러리/프레임워크 (React, Node.js, Python, Django, Kubernetes, Docker, S3, MySQL, \u0026hellip;) 기술 스택 (Backend, Frontend, MLOps, DevOps, \u0026hellip;) 시스템 종류 (Server, Client, Model-Serving, Database, Observability, \u0026hellip;) 배포 환경 (Container, AWS, Azure, \u0026hellip;) 목표, 주제, 관심사 (Auth, CI-CD, Monitoring, Machine-Learning, \u0026hellip;) 핵심 컴포넌트 (LLM, Log, Trace) ","date":"2025-04-20T00:00:00Z","permalink":"https://yeonhl.github.io/p/introduce/","title":"블로그 소개"},{"content":"개요 모델을 성공적으로 배포하는 것은 MLOps 수명주기의 시작입니다. 프로덕션 환경에 배포된 모델은 끊임없이 변화하는 데이터와 외부 환경의 영향을 받기 때문에, 지속적인 모니터링 없이는 성능과 안정성을 보장할 수 없습니다. 머신러닝 시스템의 모니터링은 전통적인 소프트웨어 모니터링의 범위를 넘어, 모델 자체의 예측 품질과 입력 데이터의 통계적 특성까지 포괄해야 합니다.\nCI/CD, 드리프트 모니터링, 자동화된 재훈련의 조합은 피드백 루프를 형성합니다. 시스템은 단순히 배포되는 것이 아니라, 모니터링을 통해 환경을 능동적으로 \u0026lsquo;인식\u0026rsquo;하고, 원하는 상태(메트릭 임계값)와 성능을 비교하며, 항상성(비즈니스 가치)을 유지하기 위해 수정 조치(재훈련/재배포)합니다. 전통적인 CI/CD 파이프라인이 선형적인 \u0026ldquo;푸시\u0026rdquo; 시스템이라면, MLOps는 라이브 서비스로부터 통계를 수집하여 드리프트를 감지하고, 이를 파이프라인 재실행의 트리거로 사용하는 피드백 경로를 추가합니다.\n중요한 모니터링 대상에는 시스템 성능뿐만 아니라, 시간이 지남에 따라 발생하는 드리프트(drift)도 포함됩니다. 이러한 변화는 실제 환경의 변화에 대응하여 모델이 \u0026ldquo;엉망이 되는(go haywire)\u0026rdquo; 것을 방지하는 데 중요합니다.\n이번 포스트에서는 모델 서빙 단계에서 수집하는 지표들과 목적을 이해하고, 목표 설정 시 고려해야 할 점을 알아보겠습니다.\n목표 별 모니터링 지표 AI 모델 서빙은 세 가지 핵심 목표를 갖습니다.\n성능 (지연 시간/처리량): 서비스가 얼마나 빠르고 반응성이 좋은가. 정확도: 모델의 예측이 얼마나 정확한가 (F1-점수, 정밀도, 재현율 등으로 측정). 비용: 하드웨어, 클라우드 리소스, 엔지니어링 노력을 포함한 총소유비용(TCO). 지연 시간과 처리량은 전적으로 서빙 시스템의 특성에 따라 결정됩니다. 모델의 예측 품질은 학습 단계의 영향력이 가장 크지만, 서빙 단계의 영향력도 확대되고 있습니다.\n비용은 아키텍처와 관련이 깊습니다. 이에 대해서는 다음 포스트에서 아키텍처와 함께 알아보고, 이번 포스트에서는 나머지 두 목표를 먼저 알아보겠습니다.\n시스템 상태 여기에서도 구글의 사이트 신뢰성 엔지니어링(Site Reliability Engineering, SRE) 프랙티스에서 유래한 \u0026lsquo;네 가지 황금 신호(Four Golden Signals)\u0026lsquo;는 모든 서빙 인프라의 상태를 종합적으로 파악할 수 있는 핵심 지표입니다. 이 신호들은 시스템 수준의 문제를 감지하는 첫 번째 방어선 역할을 합니다.\n지연 시간 (Latency) 클라이언트가 요청을 보낸 후 응답을 받기까지의 전체 왕복 시간을 의미하며, 네트워크 오버헤드와 모델 실행 시간을 모두 포함합니다. 이는 사용자 경험에 직접적인 영향을 미치는 가장 중요한 지표 중 하나입니다.\n성공한 요청과 실패한 요청의 지연 시간을 구분하여 추적하는 것이 중요하며, 특히 p95, p99와 같은 백분위수(percentile)를 모니터링하여 일부 사용자가 겪는 최악의 경험을 파악해야 합니다.\nLLM의 경우, 사용자가 체감하는 응답성은 여러 단계로 나뉘어 측정됩니다.\n첫 토큰까지의 시간 (Time to First Token, TTFT) 사용자가 요청을 보낸 후 응답의 첫 번째 조각(토큰)이 생성될 때까지 걸리는 시간입니다. 이 지표는 챗봇과 같은 대화형 애플리케이션에서 사용자가 느끼는 \u0026lsquo;즉각적인 반응성\u0026rsquo;을 결정하는 가장 중요한 요소입니다.\n특히 긴 컨텍스트나 문서를 입력으로 사용하는 검색 증강 생성(RAG)과 같은 애플리케이션에서는 입력 프롬프트를 처리하는 데 상당한 시간이 소요되므로 TTFT가 전체 지연 시간에서 큰 비중을 차지하게 됩니다.\n초당 출력 토큰 수 (Output Tokens Per Second, OTPS) 첫 토큰이 생성된 후, 후속 토큰들이 생성되는 속도입니다. 이 지표는 응답이 얼마나 \u0026lsquo;매끄럽게\u0026rsquo; 생성되는지를 나타내며, 긴 형식의 콘텐츠를 생성하는 작업에서 중요합니다. 높은 OTPS는 사용자가 응답을 읽는 속도에 맞춰 자연스러운 스트리밍 경험을 제공합니다. 출력 토큰당 시간 (Time Per Output Token, TPOT) 으로도 나타냅니다.\n종단간 지연 시간 (End-to-End Latency, E2E) 요청 시작부터 최종 응답이 완료될 때까지 걸리는 총 시간으로, 네트워크 오버헤드, 전처리, 전체 생성 주기를 모두 포함합니다.\n처리량 (Throughput) 시스템이 주어진 시간 동안 처리할 수 있는 요청의 양, 즉 시스템의 용량을 나타내며 초당 요청 수(RPS, Requests Per Second) 또는 초당 쿼리 수(QPS, Queries Per Second)로 측정됩니다.\n시스템에 가해지는 부하의 양인 트래픽 (Traffic)을 모니터링하면 용량 계획을 수립하고 비정상적인 부하 패턴을 식별하는 데 도움이 됩니다.\n동시성 (Concurrency) 시스템이 동시에 처리할 수 있는 요청의 수입니다. 이 지표는 오토스케일링 시스템의 핵심적인 스케일링 기준으로 사용됩니다.\n오류율 (Error Rate) 실패하는 요청의 비율입니다. 오류율의 급격한 증가는 즉각적인 대응이 필요한 경고 신호로 간주해야 합니다.\n포화도 (Saturation) 시스템이 얼마나 \u0026lsquo;가득 찼는지\u0026rsquo;를 나타내는 지표로, CPU, 메모리, GPU 사용률과 같이 가장 제약이 심한 자원의 활용도를 측정합니다. 포화도는 미래의 문제를 예측하는 선행 지표입니다. 포화도가 높아지면 지연 시간이 증가하고 오류율이 상승하는 경향이 있습니다.\n예측 성능 모델의 예측 품질을 나타내는 지표입니다. 시스템 상태와 달리, 모델의 예측 성능은 \u0026lsquo;실제 값(ground truth)\u0026rsquo;, 즉 예측 대상의 실제 결과가 확인되어야만 정확하게 측정할 수 있습니다.\n실제 값은 즉시 확인되지 않고 지연되어 도착하거나, 경우에 따라서는 아예 획득이 불가능할 수도 있어 모델 성능을 직접적으로 모니터링하는 것은 상당한 도전 과제일 수 있습니다.\n모델 성능을 평가하는 핵심 지표는 해결하려는 과제의 종류에 따라 달라집니다.\n분류 (Classification): 정확도(Accuracy), 정밀도(Precision), 재현율(Recall), F1-점수(F1-Score), AUC-ROC(Area Under the Receiver Operating Characteristic Curve) 등이 사용됩니다. 회귀 (Regression): 평균 절대 오차(Mean Absolute Error, MAE), 평균 제곱 오차(Mean Squared Error, MSE), 평균 제곱근 오차(Root Mean Squared Error, RMSE) 등이 주로 사용됩니다. 서빙 시스템의 역할 확대 현대적인 서빙 시스템에서는 \u0026ldquo;정확도 스케일링(Accuracy Scaling)\u0026ldquo;이 중요해지고 있습니다. 이는 시스템에 과부하가 걸렸을 때, 지연 시간 SLO를 준수하기 위해 정확도가 약간 낮지만 더 빠른 모델 변형(variant)을 동적으로 사용하여 전체 시스템의 안정성을 유지하는 전략입니다.\n이는 서빙 시스템이 정적인 컴포넌트가 아니라, 실시간으로 성능과 정확도 간의 트레이드오프를 관리하는 동적인 시스템임을 시사합니다.\n선제적 탐지 목표: 드리프트 모델의 성능은 시간이 지남에 따라 자연스럽게 저하되는 경향이 있는데, 이는 프로덕션 환경에서 모델이 마주하는 실제 데이터(\u0026lsquo;추론 데이터\u0026rsquo;)가 모델을 학습시켰던 과거의 데이터(\u0026lsquo;학습 데이터\u0026rsquo;)와 달라지기 때문입니다. 이러한 현상을 \u0026lsquo;드리프트(drift)\u0026lsquo;라고 합니다.\n배포된 모델은 정적인 자산이 아니라, 세상에 대한 동적인 가설이며 지속적으로 검증되어야 합니다. 이는 쉽게 변경되지 않는 전통 소프트웨어의 결정론적인 특성과의 차이점입니다.\n개념 드리프트 (Concept Drift) 입력 피처와 목표 변수(target variable) 사이의 관계 자체가 변화하는 현상입니다. 예를 들어, 새로운 경쟁사의 마케팅 캠페인으로 인해 고객 이탈을 예측하는 주요 요인이 바뀌는 경우나, 2020년에 스팸을 탐지하도록 훈련된 모델이 2025년에는 실패하는 경우입니다. 이는 스팸을 구성하는 개념 자체가 진화했기 때문입니다.\n개념 드리프트는 직접 탐지하기 어려우며, 보통 모델 성능 지표의 하락을 통해 간접적으로 추론됩니다. 탐지를 위해 레이블이 필요합니다.\n직접 성능 모니터링: 가장 신뢰할 수 있는 방법입니다. 레이블이 지정된 프로덕션 데이터에 대한 모델 정확도, F1-점수 등을 추적합니다. 상당한 하락은 개념 드리프트를 나타냅니다. 드리프트 탐지 알고리즘: DDM(Drift Detection Method) 또는 ADWIN(Adaptive Windowing)과 같은 특수 알고리즘을 모델의 오류율이나 다른 성능 지표에 적용하여 시간 경과에 따른 변화를 탐지할 수 있습니다. NOTE: 개념 드리프트 감지 모니터링 신호\n드리프트 감지를 위한 필수 모니터링 신호는 다음과 같습니다.\n모델 성능 메트릭: 실제 값(ground truth)을 얻을 수 있는 경우, 정확도, 정밀도, 재현율, F1-score와 같은 직접적인 측정 지표를 추적합니다. 예측 드리프트: 모델 출력의 통계적 분포를 모니터링합니다. 대출 승인 모델이 갑자기 30%가 아닌 90%의 신청자를 승인하기 시작한다면, 드리프트 발생 가능성이 높습니다. 데이터 드리프트 및 품질: 입력 데이터의 통계적 분포와 무결성을 모니터링합니다. 여기에는 null 값 비율, 데이터 유형 오류, 피처 분포의 변화(Kolmogorov-Smirnov 테스트, PSI 등) 추적이 포함됩니다. 데이터 드리프트 (Data Drift) 피처 드리프트 (Feature Drift) 로도 불립니다. 모델 입력 피처의 통계적 분포가 변화하는 현상입니다. 예를 들어, 사용자의 평균 구매 금액이 시간이 지남에 따라 점차 증가하는 경우나, 새로운 연령대의 사용자가 서비스를 사용하기 시작하는 경우입니다.\n프로덕션 입력 데이터의 분포를 참조 분포(예: 학습 데이터)와 비교합니다. 콜모고로프-스미르노프(Kolmogorov-Smirnov) 검정과 같은 통계적 검정이나 분포 간의 거리를 측정하는 지표를 통해 탐지할 수 있습니다. (레이블 불필요)\n통계적 검정: 단변량 연속 데이터에 대한 콜모고로프-스미르노프(KS) 검정, 범주형 데이터에 대한 카이제곱 검정. Kolmogorov-Smirnov (K-S) Test: 두 데이터 샘플의 누적 분포 함수를 비교하는 비모수 통계 검정입니다. 거리 측정: 인구 안정성 지수(PSI), 젠슨-섀넌 발산, 바서슈타인 거리. Population Stability Index (PSI): 두 시점 간에 변수의 분포가 얼마나 변했는지를 정량적으로 측정하는 지표입니다. KL 다이버전스 \u0026amp; JS 다이버전스: 두 확률 분포 간의 차이를 측정하는 정보 이론 기반의 지표입니다. NOTE: 학습-서빙 편향 (Training-Serving Skew)\n모델 학습 시 사용된 데이터 전처리 파이프라인과 실제 서빙 환경의 전처리 파이프라인 간에 불일치가 존재하여 발생하는 특별한 형태의 데이터 드리프트입니다. 이는 모델 배포 직후 성능 저하의 주요 원인이 됩니다.\nTIP: 데이터 드리프트와 개념 드리프트의 차이점\n데이터 드리프트 (공변량 변화, Covariate Shift): 입력 데이터의 통계적 분포(P(X))는 변하지만, 입력과 출력 간의 근본적인 관계는 동일하게 유지됩니다. 예: 여름 사진으로 학습된 이미지 분류기가 겨울 사진을 더 많이 받기 시작합니다. 픽셀 분포는 변하지만, 고양이는 여전히 고양이입니다. 개념 드리프트 (Concept Drift): 입력 피처와 목표 변수 간의 관계(P(Y∣X))가 변합니다. 데이터 자체의 의미가 바뀐 것입니다. 예: 사기 탐지 모델이 새로운 규제나 경제 변화로 인해 고객 행동이 변하는 것을 봅니다. 동일한 거래 피처가 이제 사기 위험 측면에서 다른 의미를 갖게 됩니다. 예측 드리프트 (Prediction Drift) 모델이 출력하는 예측 값의 분포가 시간이 지남에 따라 변화하는 현상입니다. 이는 데이터 드리프트나 개념 드리프트의 조기 경고 신호가 될 수 있습니다.\n프록시 사용: 즉각적인 레이블이 없는 경우, 모델의 출력 분포를 모니터링합니다. 이전에 \u0026ldquo;스팸\u0026quot;을 5% 예측하던 모델이 갑자기 50%를 예측하기 시작하면, 무언가 변경되었을 가능성이 높습니다. 해결 전략 재학습 (Retraining): 가장 일반적인 해결책으로, 최신 데이터를 사용하여 모델을 다시 학습시키고 재배포합니다. 온라인 학습 (Online Learning): 새로운 데이터를 작은 미니 배치 단위로 지속적으로 모델에 주입하여 실시간으로 업데이트하는 방식입니다. 모델 재설계: 심각한 컨셉 드리프트가 발생한 경우, 새로운 피처를 추가하거나 모델 아키텍처 자체를 변경해야 할 수 있습니다. 관측 가능성 (Observability) 확보 전통적인 모니터링은 CPU/메모리 사용량, 지연 시간, 오류율과 같은 운영 메트릭에 중점을 둡니다. MLOps 모니터링은 이를 포함하면서도 더 나아가 모델을 위한 \u0026ldquo;인식\u0026rdquo; 시스템으로서 기능해야 합니다. 이를 위해선 단순한 지표(CPU, 메모리)를 보는 모니터링을 넘어, 시스템의 외부 출력(로그, 메트릭, 트레이스)을 통해 내부 상태를 추론하고 이해하는 관측 가능성을 확보하는 것이 중요합니다.\n모니터링 스택 모니터링을 위해 사용되는 기술 스택은 다른 소프트웨어와 유사합니다:\n계측(Instrumentation): 애플리케이션 코드(예: Flask/FastAPI 서버 또는 Triton Python 백엔드)는 이러한 지표를 노출하도록 계측되어야 합니다. 이는 prometheus-client와 같은 클라이언트 라이브러리를 사용하여 수행됩니다. 프로메테우스(Prometheus): 애플리케이션이 노출하는 /metrics 엔드포인트를 정기적으로 \u0026ldquo;스크랩\u0026quot;하여 수집된 데이터를 저장하는 오픈 소스 시계열 데이터베이스입니다. 그라파나(Grafana): 프로메테우스를 데이터 소스로 연결하는 시각화 도구입니다. 수집된 지표를 시각화하고 지표가 미리 정의된 임계값을 초과할 때 경고를 설정하기 위한 그래프, 게이지 및 테이블이 포함된 실시간 대시보드를 구축할 수 있습니다. 주요 지표 효과적인 ML 모니터링은 애플리케이션 상태 뿐만 아니라 데이터 상태, 모델 상태를 모두 모니터링하는 패러다임 전환을 요구합니다. 모니터링 스택(프로메테우스, 그라파나)은 동일하지만, 수집되는 지표 집합은 훨씬 더 광범위하고 통계 지향적입니다.\n운영 지표: 지연 시간, 처리량(초당 추론 수), 오류율, CPU/GPU/메모리 사용률. (모든 서비스의 표준 지표) 모델 성능 지표: 정확도, 정밀도, 재현율, F1-점수 또는 RMSE와 같은 비즈니스 관련 지표 추적 (실제 정답 레이블을 사용할 수 있는 경우) 데이터 및 예측 지표: 입력 피처와 모델 예측의 통계적 분포 추적 구분 지표명 정의 중요성 시스템 상태 지연 시간 (p99 Latency) 요청의 99%를 처리하는 데 걸리는 시간 대부분의 사용자가 경험하는 서비스 응답성 측정 처리량 (Throughput) 단위 시간당 처리하는 요청 수 (RPS) 시스템 부하 및 용량 계획의 기준 오류율 (Error Rate) 전체 요청 중 실패한 요청의 비율 (%) 서비스 안정성 및 즉각적인 장애 감지 자원 활용도 (Resource Utilization) CPU/GPU/메모리 사용률 (%) 시스템 포화도 및 잠재적 성능 저하 예측 모델 성능 정확도 (Accuracy) 전체 예측 중 올바르게 예측한 비율 모델의 전반적인 예측 정확성 평가 정밀도/재현율 (Precision/Recall) 예측의 질과 커버리지를 평가하는 지표 불균형 데이터셋에서 모델 성능을 다각도로 평가 MAE/MSE 실제 값과 예측 값의 평균 오차 회귀 모델의 예측 오차 크기 정량화 비즈니스 KPI 클릭률, 전환율, 매출 등 모델이 비즈니스 목표에 기여하는 정도를 직접 측정 데이터 무결성 데이터 드리프트 점수 학습 데이터와 추론 데이터의 분포 차이 모델 성능 저하의 선행 지표 예측 드리프트 점수 시간 경과에 따른 예측 값 분포의 변화 데이터 또는 개념 드리프트의 조기 경고 피처 Null 비율 입력 피처의 결측치 비율 데이터 파이프라인의 품질 및 안정성 확인 학습-서빙 편향 학습과 서빙 환경 간의 데이터 불일치 배포 직후 성능 저하의 원인 진단 전통적인 SRE/DevOps는 애플리케이션의 운영 상태, 즉 \u0026lsquo;작동 중인가?\u0026rsquo;, \u0026lsquo;빠른가?\u0026rsquo;, \u0026lsquo;오류가 발생하는가?\u0026lsquo;와 같은 \u0026ldquo;운영 지표\u0026quot;에 중점을 둡니다. 하지만 시스템이 건강하더라도, 쓸모없는 예측을 생성하여 비즈니스에 부정적인 가치를 초래할 수 있습니다.\n따라서 ML 모니터링은 애플리케이션 상태 모니터링에 더해 모델의 논리적 정확성과 통계적 안정성을 추적하기 위한 \u0026ldquo;모델 성능\u0026rdquo; 및 \u0026ldquo;데이터/예측\u0026rdquo; 지표를 반드시 포함해야 합니다. 그리고 이 지표들로 드리프트를 탐지해야 합니다. 이러한 MLOps의 접근 방식은 모델 관리를 수동적이고 사후 대응적인 활동에서 자동화된 사전 예방적 활동으로 전환시킵니다.\n과거에는 비즈니스 KPI(매출, 사용자 참여도 등)가 하락한 뒤에야 원인을 분석하여 모델 성능 저하를 발견하고 재학습을 수행했습니다. 이 과정은 느리고 비즈니스 손실을 유발합니다.\n반면, MLOps는 PSI, K-S 테스트와 같은 통계적 모니터링 도구를 자동화하여, 모델 성능 저하가 비즈니스에 심각한 영향을 미치기 전에 드리프트를 감지하고 경고를 보냅니다. 더 나아가, 설명가능 AI(XAI) 기법을 활용하면 단순히 드리프트 발생 여부뿐만 아니라, 어떤 피처에서 드리프트가 발생했는지 근본 원인을 진단하여 문제 해결 과정을 가속화합니다.\nMLOps의 목표는 드리프트 발생을 막는 것이 아니라, 드리프트 탐지까지의 시간(Time-to-Detection)과 해결까지의 시간(Time-to-Remediation)을 최소화하는 것입니다.\n모니터링 아키텍처 하나의 위협이 전체 모니터링 지표에 어떤 영향을 끼칠까요? 예를 들어 상위 데이터 소스의 스키마가 변경되면(데이터 무결성 문제), 모델은 예상치 못한 입력을 받게 되어 예측 오류가 급증할 수 있습니다(시스템 상태 문제). 시간이 지나면서 이러한 부정확한 예측들은 비즈니스 성과에 악영향을 미치고, 최종적으로 실제 값 데이터가 수집되었을 때 정확도 하락으로 나타납니다(모델 성능 문제).\n이처럼 ML 모니터링 요소들은 문제 발생 시 서로 다른 시간적 특성을 보이며 계층적 관계를 형성합니다. 효과적인 모니터링 전략은 인과 사슬의 가장 앞 단계, 즉 데이터 무결성 단계에서 문제를 포착하는 것을 목표로 해야 합니다.\n모니터링 전략은 실제 값을 획득하기 위한 비용과 지연 시간에 따라 결정됩니다. 만약 실제 값이 실시간으로 확인 가능하다면(예: 추천 시스템의 클릭 여부), 정밀도와 같은 모델 성능 지표를 직접 모니터링하고 경고 기준으로 삼을 수 있습니다.\n그러나 실제 값 확인에 수 주가 걸린다면(예: 대출 부도 예측), 실시간 장애 대응을 위해 모델 정확도를 모니터링하는 것은 무의미합니다. 이런 시나리오에서는 데이터 드리프트나 예측 드리프트와 같은 대리 지표(proxy metrics)를 주요 경고 메커니즘으로 활용할 수밖에 없습니다.\n이는 모니터링 아키텍처가 모든 경우에 적용되는 단일 해법이 아니라, 특정 비즈니스 문제와 데이터 수명주기에 맞춰 설계되어야 함을 시사합니다.\n평가 지표 모델의 성공은 비즈니스 핵심 성과 지표(KPI)에 미치는 영향으로 평가됩니다. 매출 증대, 사용자 참여도 향상, 비용 절감과 같은 비즈니스 지표를 측정하기 위해서는 모델 예측 결과를 다운스트림의 비즈니스 이벤트 데이터와 결합하여 분석하는 과정이 필요합니다.\n모델 서빙 시스템의 성공은 궁극적으로 비즈니스 가치에 얼마나 기여하는지로 측정됩니다. 따라서 모든 기술적 결정은 비즈니스 요구사항에서 시작해야 합니다. 머신러닝 시스템에 대한 일반적인 비즈니스 기준은 높은 품질의 결과, 낮은 지연 시간(Latency), 그리고 높은 처리량(Throughput)입니다.\n삼중고 (Trilemma) 성능, 정확도, 비용의 세 목표는 서로 긴밀하게 연결되어 있어 하나를 개선하면 다른 하나가 저하되는 경우가 많습니다. 이처럼 목표 간 균형을 잡아야 하는 경우를 \u0026lsquo;삼중고(Trilemma)\u0026lsquo;라 합니다.\n예를 들어, 더 크고 복잡한 모델은 일반적으로 더 높은 정확도를 보이지만, 추론에 더 많은 시간과 컴퓨팅 자원을 필요로 하므로 성능(지연 시간)이 저하되고 비용은 증가합니다. 반대로, 양자화와 같은 최적화 기법은 모델 크기를 줄여 성능을 높이고 비용을 절감하지만, 정확도가 낮아질 수 있습니다.\n이러한 상충 관계는 최적화 과정을 복잡하게 만들었습니다:\n처리량 극대화. 한 번에 많은 요청을 처리하도록 배치 크기를 늘립니다. KV 캐시 문제 발생. 배치 크기가 커지면 동시 요청이 증가합니다. 각 요청은 동적으로 커지는 KV 캐시를 가지고 있으며, 모든 KV 캐시에 필요한 총 메모리는 GPU의 VRAM을 빠르게 초과하므로 최대 배치 크기를 제한하여 처리량을 제한합니다. 자기회귀 문제 (지연 시간). 요청들은 서로 다른 길이를 가집니다. 단순한 \u0026ldquo;정적 배치(static batching)\u0026rdquo; 시스템에서는 전체 배치가 가장 긴 요청이 토큰 생성을 마칠 때까지 기다려야 합니다. 이는 막대한 유휴 시간(GPU 비활용)을 발생시키고 모든 짧은 요청의 평균 지연 시간을 증가시킵니다. 이는 선두 차단(Head-of-Line, HOL) 블로킹으로 알려져 있습니다. 위는 다음의 상충 관계가 발생했습니다:\n높은 처리량(큰 배치)을 얻으려면 더 많은 메모리가 필요하지만, HOL 블로킹으로 인해 높은 지연 시간과 낮은 활용률이 나타납니다. 낮은 지연 시간(작은 배치 또는 순차 처리)을 얻으려면 처리량과 GPU 활용률이 매우 낮아집니다. 높은 활용률을 얻으려면 GPU를 작업으로 가득 채워야 하지만, 이는 다시 메모리 및 지연 시간 문제로 이어집니다. 현대 LLM 서빙의 핵심 과제는 이 삼중고를 해결하기 위해 PagedAttention, 동적 배치, 반복 수준 스케줄링 등의 기술을 시도하고 있습니다. 이 기술들은 메모리를 더 효율적으로 관리하고 작업을 더 세밀하게 스케줄링함으로써 지연 시간을 희생하지 않으면서 높은 활용률과 처리량을 달성하는 것을 목표로 합니다.\nNOTE: PagedAttention\nPagedAttention은 KV 캐시를 고정된 크기의 \u0026ldquo;페이지\u0026rdquo; 또는 \u0026ldquo;블록\u0026quot;으로 분할합니다. 이 페이지들은 물리적 GPU 메모리에 비연속적으로 저장될 수 있습니다. \u0026ldquo;블록 테이블\u0026quot;은 토큰의 논리적 시퀀스를 이러한 비연속적인 물리적 블록에 매핑하는 역할을 합니다.\n이 방식은 메모리가 작은 블록 단위로 필요에 따라 할당되므로 내부 단편화를 제거합니다. 이를 통해 훨씬 더 큰 배치 크기를 사용할 수 있게 되어 처리량을 향상시킵니다. 또한 복잡한 샘플링 전략을 위한 효율적인 메모리 공유(쓰기 시 복사, copy-on-write)를 가능하게 합니다.\nNOTE: 동적 배치\n동적 배치는 서버 측에서 짧은 시간 동안 도착하는 개별 추론 요청들을 수집하여 하나의 더 큰 배치로 묶은 다음 GPU로 보내는 기술입니다.\nGPU는 병렬 처리에 매우 효율적이므로, 더 큰 데이터 배치에서 더 효율적으로 작동합니다. 이 기법은 많은 모델에서 처리량을 3배에서 10배까지 향상시킬 수 있습니다.\nNOTE: 반복 수준 스케줄링\n스케줄러는 전체 요청을 스케줄링하는 대신, 매 단일 토큰 생성 단계마다 결정을 내립니다. 실행 중인 배치에 새로운 요청을 추가하거나 완료된 요청을 제거할 수 있습니다.\n이 방식은 패딩의 필요성을 제거하고 GPU 유휴 시간을 최소화하여 처리량과 활용률을 크게 향상시킵니다. 이는 모든 현대 서빙 시스템의 기초적인 최적화 기술입니다.\n지연 시간 대 처리량 대표적인 상충 관계 중 하나는 개별 요청의 지연 시간을 최소화하는 것과 시스템 전체의 처리량을 최대화하는 것입니다.\n낮은 지연 시간(한 사용자에 대한 빠른 응답)을 위해 최적화하는 것은 종종 요청을 개별적으로 처리하는 것을 포함하며, 이는 전체 처리량을 제한할 수 있습니다. 반면 높은 처리량(초당 많은 사용자)을 위해 최적화하는 것은 종종 요청을 일괄 처리하는 것을 포함하며, 이는 각 개별 요청에 대한 지연 시간을 증가시킵니다.\n예를 들어, 배치(Batching) 기술은 여러 요청을 하나로 묶어 GPU에서 한 번에 처리함으로써 GPU의 활용률을 높여 전체 처리량을 향상시킵니다. 하지만 이 방식은 배치가 채워질 때까지 기다려야 하므로, 배치에 포함된 각 개별 요청의 지연 시간은 증가하게 됩니다. 반대로, 모든 요청을 도착하는 즉시 개별적으로 처리하면 지연 시간은 최소화되지만, GPU가 충분히 활용되지 않아 전체 처리량은 낮아질 수 있습니다.\n이처럼 지연 시간과 처리량은 서로 반비례 관계에 있는 경우가 많습니다. 서비스의 요구사항에 따라 이 둘 사이의 적절한 균형점을 찾는 것이 시스템 아키텍처 설계의 핵심 과제입니다.\nNOTE: 실시간 서빙\n사용자와 시스템이 즉각적인 응답을 기다리는 동기식, 저지연 예측을 위해 설계된 아키텍처입니다. 일반적으로 REST API 엔드포인트나 gRPC 서비스 형태로 구현되어, 요청이 들어오면 실시간으로 추론을 수행하고 결과를 반환합니다.\n높은 처리량보다는 낮은 지연 시간을 최우선으로 고려합니다. 따라서 고가용성의 반응성이 뛰어난 인프라가 필수적입니다.\nNOTE: 배치 서빙\n대량의 데이터를 비동기적으로 처리하는 아키텍처입니다. 모델은 정해진 스케줄(예: 매일 밤)에 따라 대규모 추론 작업을 수행하고, 그 결과를 데이터베이스나 데이터 웨어하우스에 저장하여 필요할 때 애플리케이션에서 가져다 사용합니다.\n지연 시간이 중요하지 않은 대신, 높은 처리량과 비용 효율성을 우선시합니다. 컴퓨팅 자원을 필요할 때만 집중적으로 사용하므로 비용을 최적화할 수 있습니다.\n비용 대 성능 더 강력한 하드웨어(예: GPU 대 CPU)는 더 나은 성능을 제공하지만 더 높은 비용이 필요합니다. 적절한 균형점은 모델의 특정 요구 사항과 속도의 비즈니스 가치에 따라 다릅니다.\n예측 성능 대 속도 더 복잡한 모델이 더 정확할 수 있지만, 실행 속도가 느리고 서빙 비용이 더 비싼 경우가 많습니다. \u0026ldquo;실용적 정확성(practical accuracy)\u0026ldquo;의 원칙은 순위표에서 가장 높은 점수를 받은 모델이 아니라, 비즈니스 문제에 \u0026ldquo;충분히 좋은\u0026rdquo; 모델을 선택할 것을 권장합니다.\n전략적 의사결정 프레임워크: 파레토 최적 전선 이러한 복잡한 상충 관계 속에서 \u0026lsquo;최고의\u0026rsquo; 단일 솔루션을 찾는 것은 거의 불가능합니다. 대신, \u0026lsquo;파레토 최적(Pareto Optimal)\u0026lsquo;이라는 개념을 활용하여 합리적인 의사결정 프레임워크를 구축할 수 있습니다.\nNOTE: 파레토 최적\n다른 목표를 악화시키지 않고서는 하나의 목표를 더 이상 개선할 수 없는 상태를 의미합니다. 예를 들어, 현재 모델보다 정확도를 높이려면 반드시 비용이나 지연 시간이 증가해야만 하는 경우, 현재 모델은 파레토 최적 상태에 있다고 할 수 있습니다.\n다양한 모델과 그 구성(예: 양자화 적용 여부, 하드웨어 종류)을 2차원 또는 3차원 그래프(예: X축-비용, Y축-지연 시간, 색상-정확도)에 표시하면, \u0026lsquo;파레토 최적 전선(Pareto Frontier)\u0026lsquo;을 시각적으로 확인할 수 있습니다. 이 전선 위에 있는 모든 점들은 기술적으로 \u0026lsquo;효율적인\u0026rsquo; 선택지들입니다. 전선 안쪽에 있는 점들은 비효율적인데, 왜냐하면 동일하거나 적은 비용 및 지연 시간으로 더 높은 정확도를 달성하는 다른 점이 전선 위에 존재하기 때문입니다.\n어떤 파레토 최적점을 선택할지는 비즈니스와 제품 요구사항에 달려 있습니다. 예를 들어, 생명이 달린 의료 영상 진단 모델은 비용이 아무리 많이 들더라도 정확도가 가장 높은 점을 선택해야 합니다. 반면, 무료 사용자에게 제공되는 비핵심적인 추천 기능은 정확도를 다소 희생하더라도 비용이 가장 저렴한 점을 선택하는 것이 합리적입니다.\n이 프레임워크는 \u0026ldquo;가장 정확한 모델이 무엇인가?\u0026ldquo;라는 질문을 \u0026ldquo;우리의 비즈니스 제약 조건 하에서 가장 효율적인 모델은 무엇인가?\u0026ldquo;라는 더 전략적인 질문으로 전환시킵니다.\n종종 최고의 정확도 점수를 내는 모델을 \u0026lsquo;최고\u0026rsquo;라고 여기는 경향이 있지만, 프로덕션 환경은 비용과 성능이라는 두 가지 치명적인 제약 조건을 추가합니다.\n99%의 정확도를 가졌지만 응답에 10초가 걸리고 시간당 10달러의 비용이 드는 모델은, 100ms의 지연 시간과 엄격한 예산이 요구되는 실시간 애플리케이션에서는 사실상 쓸모가 없습니다. 이 경우, 97%의 정확도를 가졌지만 50ms 내에 응답하고 시간당 1달러의 비용이 드는 모델이 훨씬 더 가치 있습니다. 2%의 정확도 하락은 200배의 속도 향상과 10배의 비용 절감을 위한 합리적인 트레이드오프입니다.\n프로덕션에서 \u0026lsquo;최고의\u0026rsquo; 모델은 학술적인 정확도 리더보드의 최상단에 있는 모델이 아니라, 비용-성능-정확도라는 파레토 최적 전선 위에서 비즈니스 목표와 가장 잘 부합하는 지점에 위치한 모델입니다.\n비즈니스 목표에서 시작: 개발을 시작하기 전에 허용 가능한 지연 시간, 최소 정확도, 그리고 예산 한도를 명확히 정의해야 합니다. 반복적인 최적화: MLOps 원칙에 따라 다양한 모델, 하드웨어(CPU vs. 다양한 GPU), 최적화 기법을 체계적으로 실험하고 그 결과를 파레토 전선에 플로팅하여 최적의 조합을 찾아야 합니다. 자원의 적정 규모화 (Right-Sizing): 오토스케일링과 MIG 같은 하드웨어 관리 기법을 적극 활용하여, 실제로 필요한 만큼의 리소스에 대해서만 비용을 지불하도록 시스템을 구성해야 합니다. 모델 캐스케이딩 (Model Cascading): 간단하고 저렴한 모델로 대부분의 쉬운 요청을 처리하고, 어려운 요청만 복잡하고 비싼 모델로 전달되도록 라우팅하는 고급 전략입니다. 이는 시스템 전체의 비용-성능 곡선을 최적화하는 데 매우 효과적일 수 있습니다 SRE 원칙 적용 비즈니스 요구사항을 측정 가능하고 실행 가능한 기술적 목표로 전환하는 과정은 서비스 수준 목표(Service Level Objectives, SLOs) 를 설정하는 것입니다. SLO는 시스템이 달성해야 할 구체적인 성능 목표를 정의하며, 서빙 아키텍처 설계의 기반이 됩니다.\n예를 들어, 실시간 상호작용이 중요한 챗봇 애플리케이션은 100ms 미만의 매우 낮은 지연 시간을 SLO로 설정해야 하는 반면, 대규모 문서 요약을 처리하는 배치 시스템은 높은 처리량을 SLO로 설정할 수 있습니다.\nSRE는 IT 운영을 자동화하고 높은 신뢰성을 달성하기 위해 소프트웨어 엔지니어링 관행을 사용하는 것에 관한 것입니다. ML의 경우 이는 다음을 의미합니다.\n서비스 수준 목표(SLO) 정의: 표준 지연 시간 및 가용성 SLO 뿐만 아니라, 데이터 신선도(모델이 학습하는 데이터가 얼마나 오래되었는가?), 예측 품질(예: 28일 동안 정확도가 95% 이상이어야 함), 학습 파이프라인 완료율과 같은 ML 관련 문제에 대한 SLO를 정의합니다. 오류 예산(Error Budgets): 오류 예산은 허용 가능한 실패 수준입니다. 이 예산은 팀이 전체 오류 예산을 \u0026ldquo;소비\u0026quot;하지 않는 한 혁신하고 위험을 감수할 수 있도록 권한을 부여합니다. 성숙한 MLOps는 성숙한 SRE와 구별할 수 없습니다. 초기 단계의 MLOps는 모델을 프로덕션 환경에 배포하는 데 중점을 둡니다(CI/CD, 배포). 시스템이 성숙해짐에 따라 초점은 배포에서 신뢰성과 유지보수로 이동하며, 이는 SRE의 핵심 영역입니다.\nSRE가 소프트웨어에 사용하는 원칙들(SLO, 오류 예산, 모니터링, 자동화된 사고 대응, 비난 없는 사후 검토)은 ML 시스템에 직접 적용될 수 있습니다. 유일한 차이점은 모니터링 및 관리 대상의 범위입니다.\nML을 위한 SRE는 이러한 원칙을 애플리케이션 코드뿐만 아니라 데이터 파이프라인과 모델의 통계적 행동까지 포괄하도록 확장합니다. 결론적으로, MLOps의 최종 목표는 별개의 학문 분야가 되는 것이 아니라, SRE의 전문화된 한 분야가 되는 것입니다. 목표는 모델, 데이터, 코드를 단일의 신뢰할 수 있고 자동화된 프로덕션 시스템의 구성 요소로 취급하고, SRE가 전통적인 소프트웨어에 적용하는 것과 동일한 엔지니어링 엄격함으로 관리하는 것입니다.\n문제 진단을 위한 구조화된 플레이북 모델 성능 경고가 발생했을 때, 엔지니어는 임시방편적인 디버깅이 아닌 구조화된 계획이 필요합니다.\n1단계: 분류 (실제 상황인가?):\n경고가 오탐이 아닌지 확인합니다. 모니터링 대시보드(Grafana)를 확인하여 문제의 범위와 기간을 파악합니다.\n2단계: 도메인 격리 (시스템 대 데이터 대 모델):\n시스템 문제 확인: 지연 시간이 높은지, 서버 충돌이 발생했는지 운영 지표를 확인합니다. 이는 기본적인 SRE 사고입니다. 데이터 문제 확인: null 값이 급증했는지, 입력 분포가 급격히 변했는지 데이터 드리프트 및 품질 대시보드를 확인합니다. 이는 상위 데이터 파이프라인 문제를 가리킵니다. 모델 문제 확인: 시스템 및 데이터 지표는 정상이지만 예측 품질(정확도 등)이 떨어지는 경우, 이는 개념 드리프트 또는 모델 코드 자체의 버그를 가리킵니다. 3단계: 즉각적인 완화 (피해 확산 방지):\n잘못된 배포인 경우, 이전 버전으로의 자동 롤백을 트리거합니다. 특정 소스의 데이터 품질 문제인 경우, 가능하다면 해당 소스를 일시적으로 차단하는 것을 고려합니다. 4단계: 근본 원인 분석 (사후 검토):\n완화 조치 후, 근본 원인을 이해하기 위해 비난 없는 사후 검토를 수행합니다. 상위에서 데이터 스키마가 변경되었는지, 새로운 사용자 행동이 나타났는지 확인합니다.\n5단계: 수정 및 자동화:\n근본 원인을 수정합니다. 그리고 같은 실패가 다시 발생하지 않도록 새로운 모니터링 검사나 자동화된 테스트를 추가합니다.\n마치며 이번 포스트에서는 모델 서빙의 목표를 나타내는 기술 지표들을 알아보고, 비즈니스 환경에서 평가를 위해 지표를 어떻게 정해야 하는지 알아보았습니다.\n기본적인 지표 위주로 알아보았기에, 실제 환경에서는 비즈니스 목표에 따라 위에서 언급하지 않은 지표도 고려해야 할 것입니다. 그러한 경우에도 지표를 정하는 이유와 최종 목적은 같을 것이라 생각합니다.\n다음 포스트에서는 모델 서빙의 구성 요소와 서빙 형태를 살펴보겠습니다.\n본 포스트는 Google Gemini의 응답을 기반으로 저의 의견을 반영하여 다시 작성했습니다.\n","date":"2025-09-18T19:03:00Z","permalink":"https://yeonhl.github.io/systems/model_serving/monitoring/","title":"모델 서빙의 모니터링"},{"content":"개요 인공지능(AI) 모델 서빙은 단순히 학습된 모델을 배포하는 단계를 넘어, AI의 가치를 실제 비즈니스 성과로 전환하는 핵심적인 공학 분야로 자리 잡았습니다. 모델 서빙의 본질적인 과제는 실제 운영 환경의 다양한 제약 조건 하에서 빠르고, 안정적이며, 비용 효율적인 예측 서비스를 제공하는 것입니다.\n이는 단순히 모델을 API 뒤에 배치하는 것을 넘어, 분산 시스템, MLOps, 하드웨어 최적화 원칙을 통합하는 총체적인 접근을 요구합니다. 성공적인 모델 서빙은 예측의 품질뿐만 아니라, 예측이 전달되는 속도와 신뢰성, 그리고 이를 유지하는 데 드는 비용까지 모두 고려하는 다차원적인 최적화 문제입니다.\n이번 포스트에서는 모델 서빙이 어떻게 등장했고, 어떤 특징과 목표를 갖는지 알아보겠습니다.\n모델 서빙의 발전 초기 배포: 스크립트와 사일로 초기 머신러닝 배포는 개발 과정의 마지막 단계에서 고려되는 부차적인 작업이었습니다. 데이터 과학자들은 훈련된 모델 결과물(예: pickle 파일)을 엔지니어링 팀에 전달하는 방식으로 작업을 마무리했습니다. 이 시기의 배포는 임시방편으로 작성된 스크립트, 수동 프로세스, 그리고 표준화되지 않은 환경의 조합으로 이루어져 여러 문제점을 내포하고 있습니다.\n재현성 부족: 일관되지 않은 환경과 수동적인 단계들은 모델이나 그 예측 결과를 신뢰성 있게 재현하는 것을 거의 불가능합니다. 확장성 문제: 모델의 수와 데이터의 복잡성이 증가함에 따라 수동 프로세스는 본질적으로 확장 불가능합니다. 오류 위험 증가 및 비효율성: 데이터 과학팀과 운영팀 간의 수동적인 인계 과정은 소통의 단절, 오류 발생 위험 증가, 그리고 느린 출시 주기로 이어졌습니다. 모델 개발과 소프트웨어 개발 사이의 간극은 주요 병목 현상의 원인이었습니다. 이러한 초기 단계의 어려움은 머신러닝이 학문적 탐구에서 실용적인 애플리케이션으로 전환하기 위한 배포에서 체계적이고 원칙에 기반한 접근법의 필요성을 절실하게 만들었다.\nMLOps: 철학적, 문화적 전환 MLOps는 개발(DevOps), 데이터(DataOps), 모델(ModelOps)을 통합하여 ML의 고유한 복잡성에 맞춰 CI/CD(지속적 통합/지속적 배포), 버전 관리, 자동화와 같은 DevOps 원칙을 적용합니다. 코드뿐만 아니라 데이터와 모델을 일급 시민(first-class citizens)으로 취급하고 관리합니다.\n목표는 데이터 수집, 모델 개발, 테스트, 배포, 모니터링, 거버넌스에 이르는 전체 ML 생명주기에 걸쳐 반복 가능하고, 신뢰할 수 있으며, 확장 가능한 워크플로우를 만드는 것입니다. 이를 통해 시장 출시 시간을 단축하고, 생산성을 향상시키며, 효율적인 배포를 달성할 수 있습니다.\n이는 결과물 중심(artifact-centric) 관점에서 시스템 중심(system-centric) 관점으로의 전환입니다. 초기에는 \u0026lsquo;훈련된 모델 파일\u0026rsquo; 자체가 최종 결과물로 간주되었고 \u0026ldquo;이 저장된 모델 파일을 어떻게 서버에서 실행할까?\u0026ldquo;라는 질문이 이 시대의 핵심 과제였습니다. 그러나 이러한 접근은 의존성 관리, 환경 불일치, 수동 오류와 같은 문제들을 야기했습니다.\n이에 대한 해답은 데이터 검증, 훈련, 모델 검증, 배포, 모니터링에 이르는 전체 프로세스를 자동화하는 것입니다. 이 자동화된 파이프라인, 즉 \u0026lsquo;시스템\u0026rsquo;이 진정한 의미의 지속 가능하고, 버전 관리되며, 확장 가능한 자산입니다.\n핵심 원칙: 자동화: CI/CD(지속적 통합/지속적 배포) 파이프라인을 통해 모델의 테스트, 검증, 배포 과정을 자동화하여 수동 개입과 인적 오류를 줄입니다. 실험 추적 및 모델 레지스트리: 모든 실험, 데이터셋, 모델 아티팩트를 버전 관리하여 재현성을 보장하고, 승인된 모델을 중앙에서 관리하여 거버넌스를 강화합니다. 모니터링: 프로덕션 환경에서 시스템 성능(지연 시간, 에러율)과 모델 품질(예측 정확도, 드리프트)을 지속적으로 추적합니다. 협업: 데이터 과학자, ML 엔지니어, 운영팀이 공통된 프레임워크와 도구를 사용하여 원활하게 협업할 수 있는 환경을 제공합니다. LLMOps: LLM의 고유 요구사항 전통적인 ML 서빙 전통적인 ML 서빙은 구조화되거나 반구조화된 데이터에 대해 분류나 회귀와 같은 특정 판별적(discriminative) 작업을 수행하는 모델을 위해 설계되었습니다. 주요 목표는 단일 예측에 대한 정확성, 낮은 지연 시간, 그리고 마이크로서비스로서의 관리 용이성이었습니다.\n이러한 모델들은 특정 문제를 해결하기 위해 처음부터 구축되는 경우가 많으며, 깊은 도메인 전문 지식과 피처 엔지니어링을 필요로 합니다. 추론은 일반적으로 단일의 병렬화 가능한 순방향 패스(forward pass)로 이루어집니다. 입력 크기가 고정되거나 제한적이어서 예측 가능한 계산 부하를 가집니다.\n배포는 종종 모델을 마이크로서비스로 패키징하는 간단한 방식으로 이루어지며, 재학습도 쉽게 자동화할 수 있습니다. 주요 초점은 사기 탐지나 주택 가격 예측과 같이 명확하게 정의된 문제에 맞춰져 있습니다.\nLLM의 차이점 LLM은 이전 ML 서빙과 다릅니다. 이들은 생성적(generative)이며, 방대한 양의 비정형 텍스트를 처리하고, 처음부터 구축되기보다는 거대한 파운데이션 모델(foundation model)을 기반으로 조정됩니다. 이러한 차이점들은 확장성, 지연 시간, 메모리 관리 측면에서 전통적인 프레임워크가 처리할 수 없는 전례 없는 과제들을 야기합니다.\nLLM은 방대한 파라미터 수로 특징지어지며, 이로 인해 메모리 집약적입니다. 이들은 특정 예측뿐만 아니라 텍스트 요약이나 코드 생성과 같은 광범위한 생성 작업을 위해 사용됩니다. 추론 과정은 단일 패스가 아니라 자기회귀 디코딩(autoregressive decoding)이라는 순차적이고 반복적인 과정입니다. 사용자 요청은 입력 및 출력 길이가 매우 다양하여 예측 불가능한 계산 부하와 메모리 사용량을 초래합니다.\n자기회귀 디코딩(autoregressive decoding)과 KV 캐싱(KV caching)은 LLM 서빙의 주요 기술적 과제의 근본 원인입니다. 이들은 성능 병목 현상을 연산 집약적인 문제에서 메모리 집약적 및 지연 시간 문제로 변화시켰습니다.\n자기회귀 디코딩: LLM은 한 번에 하나의 토큰을 생성하며, 각 새로운 토큰은 이전의 모든 토큰에 의존합니다. 이 순차적인 과정은 느리고 병렬화를 어렵게 만듭니다. 프롬프트를 처리하는 첫 번째 단계를 \u0026ldquo;프리필(prefill)\u0026ldquo;이라고 하며, 이후 토큰별 생성 단계를 \u0026ldquo;디코딩(decode)\u0026ldquo;이라 합니다. 이 두 단계는 서로 다른 연산 프로필(연산 집약적 vs. 메모리 대역폭 집약적)을 가집니다. KV 캐싱: 매 단계마다 전체 시퀀스에 대한 어텐션 메커니즘을 재계산하는 것을 피하기 위해, 시스템은 중간 어텐션 상태(키와 값)를 \u0026ldquo;KV 캐시\u0026quot;에 저장합니다. 이 캐시는 생성되는 모든 토큰과 함께 크기가 커집니다. KV 캐시는 추론 중 GPU 메모리의 주요 소비자이며, 종종 전체 메모리 사용량의 대부분을 차지합니다. 이러한 변화는 미세 조정(fine-tuning), 프롬프트 튜닝, 검색 증강 생성(RAG), 인간 피드백 기반 강화 학습(RLHF) 등 LLM 수명 주기에 맞춰진 MLOps의 진화된 형태인 LLMOps라는 전문 분야의 발전을 이끌었습니다.\n속성 전통적 머신러닝 거대 언어 모델 (LLM) 주요 목적 패턴 식별 및 예측 (판별적) 언어 이해 및 생성 (생성적) 데이터 유형 구조적 및 반구조적 (테이블, 레이블 데이터) 비정형 텍스트 (책, 웹사이트, 기사) 모델 아키텍처 다양한 알고리즘 (결정 트리, SVM, 간단한 신경망) 트랜스포머 아키텍처 추론 패턴 단일, 병렬화 가능한 순방향 패스 반복적, 순차적 자기회귀 디코딩 주요 병목 현상 CPU 연산, 피처 엔지니어링 GPU 메모리 대역폭, VRAM 용량 핵심 성과 지표 (KPIs) 정확도, 단일 요청 지연 시간 처리량 (토큰/초), 첫 토큰까지의 시간 (TTFT), 토큰당 비용 배포 복잡성 상대적으로 낮음 (마이크로서비스) 매우 높음 (특수 서빙 시스템 필요) 일반적인 사용 사례 사기 탐지, 고객 분류, 수요 예측 챗봇, 텍스트 요약, 코드 생성, 콘텐츠 제작 다음 목표: AI 에이전트 배포 다음 단계는 단순한 요청-응답 서빙을 넘어, 복잡한 목표를 이해하고, 계획을 세우며, 다른 모델을 포함한 도구를 사용하여 이를 실행할 수 있는 자율적인 AI 에이전트를 배포하는 것입니다.\n이는 복잡성을 크게 증가시킵니다. 에이전트를 서빙하는 것은 단일 모델을 서빙하는 것이 아니라, 모델, 도구, 상태 관리의 전체 시스템을 서빙하는 것입니다. 에이전트는 동적인 다단계 워크플로우, 도구 호출, 장기 기억(RAG는 이의 기초 구성 요소임)을 처리할 수 있는 더 정교한 서빙 인프라를 필요로 할 것입니다. 에이전트 AI로의 추세는 이러한 자율 시스템의 백본으로서 견고하고 확장 가능하며 안전한 모델 서빙의 필요성을 더욱 강화할 것입니다. 모델 서빙의 특징 훈련과 추론의 분리 머신러닝 프로세스를 훈련과 추론으로 나눌 수 있습니다.\n훈련 (귀납/귀추): 이는 \u0026lsquo;발견\u0026rsquo;의 과정입니다. 데이터에서 패턴을 관찰하여 일반적인 규칙을 추론(귀납)하거나 가설을 형성(귀추)하는 단계입니다. 이 단계는 계산 집약적이고, 실험적이며, 반복적인 특징을 가집니다. 추론 (연역): 이는 \u0026lsquo;적용\u0026rsquo;의 과정입니다. 이미 알려진 규칙(훈련된 모델)을 특정 사례(새로운 데이터)에 적용하여 결론(예측)에 도달하는 단계입니다. 이 단계는 빠르고, 효율적이며, 신뢰할 수 있어야 합니다. 이러한 분리는 엔지니어링 과정에서 전혀 다른 두 워크로드를 최적화합니다. 훈련 파이프라인은 강력하고 값비싼 하드웨어(GPU/TPU 등)에서 배치(batch) 지향적인 방식으로 실행될 수 있으며, 추론 서비스는 더 가볍고 비용 효율적이며 고가용성을 갖춘 인프라에서 낮은 지연 시간(latency)에 최적화되어 배포될 수 있습니다.\n재현성 예측은 그것을 생성한 과정이 재현 가능할 때에만 신뢰할 수 있습니다. ML 서빙에서의 재현성이란, 동일한 입력(코드, 데이터, 구성)이 주어졌을 때 시스템이 동일한 모델을 생성하고, 결과적으로 동일한 예측을 산출하는 것을 의미합니다. 재현성을 확보하기 위한 핵심 메커니즘은 다음과 같습니다.\n컨테이너화 (Docker): 모델, 의존성, 그리고 서빙 애플리케이션을 단일하고 불변하는 컨테이너 이미지로 패키징하여 실행 환경이 어디에서나 동일함을 보장합니다. 버전 관리 (코드, 데이터, 모델): 코드와 함께 데이터와 모델을 버전 관리되는 결과물로 취급하는 것(Git, DVC 등의 도구 사용)은 모든 예측에 대한 완전하고 감사 가능한 계보(lineage)를 만드는 데 필수적입니다. 코드형 인프라 (Infrastructure as Code, IaC): 배포 환경(서버, 네트워크 등)을 코드로 정의(Terraform, CloudFormation 등)함으로써 배포 환경 자체의 재현성을 보장합니다. FAIR 원칙: 찾을 수 있고(Findable), 접근 가능하며(Accessible), 상호 운용 가능하고(Interoperable), 재사용 가능한(Reusable) FAIR 원칙은 데이터와 모델을 본질적으로 재현성을 지원하는 방식으로 관리하기 위한 높은 수준의 철학적 프레임워크를 제공합니다. 서비스 모델 서빙 인프라는 모델이 애플리케이션과 최종 사용자의 필요를 충족시킬 수 있도록 지원합니다. 이를 구현하는 주요 메커니즘은 API(Application Programming Interface)입니다.\nAPI는 모델과 인프라의 복잡성을 추상화하는 잘 정의된 계약 역할을 합니다. 사용자는 모델이 심층 신경망인지 단순한 로지스틱 회귀인지 알 필요 없습니다. 요청을 어떻게 형식화하고 응답을 어떻게 해석하는지 알면 됩니다. 이러한 관심사의 분리(separation of concerns)는 현대 소프트웨어 아키텍처의 기본 원칙입니다.\n정확성 모델 서빙은 전통적인 소프트웨어와 차이가 있습니다. 전통적 소프트웨어의 \u0026lsquo;정확성\u0026rsquo;은 정적인 논리에 의해 결정되는 내재적이고 고정된 속성입니다. 반면, 머신러닝(ML) 모델의 \u0026lsquo;정확성\u0026rsquo;은 확률적이며, 끊임없이 변화하는 실제 세계의 데이터에 대한 성능으로 결정되는 동적인 품질입니다. 모델 서빙은 이러한 동적인 관계를 지속적으로 적응하고, 모니터링하며, 관리해야 합니다.\n전통적 소프트웨어 (결정론적): 명시적으로 인간이 작성한 논리에 따라 작동합니다. 동일한 입력과 상태가 주어지면 항상 동일한 출력을 생성한다. 그 행동은 코드로 완전히 결정됩니다. ML 모델 (확률론적): 데이터로부터 학습된 패턴에 따라 작동합니다. 결정론적 확실성이 아닌 확률적 평가를 제공한다. 그 출력은 가장 가능성 있는 결과에 대한 추정치인 예측이며, 불확실성을 내포합니다. 동일한 입력에 대해서도 모델의 예측은 확률 분포로부터의 샘플로 간주될 수 있습니다. 발생하는 문제점과 그 대응 방법에도 차이가 있습니다.\n\u0026ldquo;버그\u0026rdquo; 대 \u0026ldquo;드리프트\u0026rdquo;: 전통적 소프트웨어의 버그는 내부 로직의 결함, 즉 의도된 결정론적 행동으로부터의 이탈이며, 코드의 수정으로 해결합니다. 반면, ML 모델이 \u0026ldquo;부정확한\u0026rdquo; 예측을 하는 것은 반드시 버그가 아닐 수 있습니다. 모델은 훈련된 대로 정확하게 작동하고 있더라도, 실제 세계가 변하여 학습된 패턴이 더 이상 유효하지 않기 때문에 \u0026ldquo;실패\u0026quot;할 수 있습니다. 이는 개념 드리프트(Concept Drift) 라 합니다. 유지보수 철학: 전통적 소프트웨어의 유지보수는 버그를 수정하고 기능을 추가하는 것을 포함합니다. 반면 ML 모델의 유지보수는 지속적인 적응의 철학을 포함합니다. 성능 저하에 대한 주된 \u0026ldquo;수정\u0026rdquo; 방법은 모델의 코드를 변경하는 것이 아니라, 새로운 현실을 반영하는 새로운 데이터로 모델을 재훈련하는 것입니다. 특성 전통적 소프트웨어 서빙 ML 모델 서빙 핵심 로직 결정론적 (코딩된 규칙) 확률론적 (학습된 패턴) 데이터 의존성 로직이 우선, 데이터는 입력 데이터가 로직의 핵심; \u0026ldquo;데이터가 새로운 코드\u0026rdquo; 진실의 원천 코드베이스 코드 + 데이터 + 모델 결과물 실패 모드 버그 (코드의 논리적 오류) 성능 저하 (개념 드리프트, 데이터 드리프트 등) 유지보수 코드 패치 및 업데이트 지속적인 모니터링, 재훈련, 버전 관리 출력 결정론적 결과 내재적 불확실성을 가진 예측 핵심 과제 로직의 복잡성, 확장성 재현성, 데이터 품질, 드리프트 관리 모델 서빙의 비즈니스 목표 AI 모델 서빙의 최종 목표는 기술적 우수성을 넘어 비즈니스 가치를 창출하는 것입니다. 이를 위해서는 기술적 결정이 경제적 결과에 미치는 영향을 깊이 이해해야 합니다.\n비즈니스 가치 및 ROI 모델 서빙은 그 자체로 목적이 아니라, 실질적인 비즈니스 가치를 창출하기 위한 수단입니다. 모든 AI 이니셔티브의 성공은 조직의 목표에 얼마나 기여했는지로 측정됩니다. 이를 위해 모델의 예측이 비즈니스 결과로 이어지는 명확한 경로를 설정해야 합니다.\nAI/ML 애플리케이션은 사기 탐지, 고객 이탈 감소, 실시간 추천, 예측 유지보수와 같은 기능이 시장에서 결정적인 차별화 요소로 작용합니다. 성공적인 AI 프로젝트는 명확한 비즈니스 문제를 식별하는 것에서 시작하며, \u0026ldquo;잘못된 예측이 얼마나 큰 비용을 초래하는가?\u0026ldquo;라는 질문을 통해 프로젝트의 타당성을 검토합니다.\n비즈니스 가치는 AI 솔루션의 전체 수명 주기에 걸쳐 개념 단계부터 제품화까지 점진적으로 평가되어야 합니다. 가치를 창출하는 방법으로는 직원 생산성 향상, 매출 증대, 비용 효율성 개선, 고객 경험 향상 등이 있습니다. 예를 들어, 반복적인 작업을 자동화함으로써 수만 시간의 업무 시간을 절약하고 생산성을 25% 이상 향상시킬 수 있습니다.\nFinOps LLM과 같이 복잡하고 자원 집약적인 모델의 등장은 재무 관리에 초점을 맞춘 FinOps 간의 긴밀한 통합을 요구합니다. 단순히 모델을 배포하는 것만으로는 충분하지 않으며, 비용 효율적이고 아키텍처적으로 최적화된 방식으로 배포해야 합니다. AI 프로젝트, 특히 LLM은 막대한 기술적, 재무적 부채를 유발할 수 있습니다.\nMLOps는 배포 파이프라인을 간소화하여 속도와 안정성을 보장합니다. 반면, FinOps는 근본적인 비용 효율성에 의문을 제기합니다. 해당 모델이 정말로 비용을 절감하고 있는지, 아니면 \u0026ldquo;기존의 비효율성을 자동화\u0026quot;하고 있을 뿐인지, 선택된 인프라(예: 인스턴스 유형, 배포 전략)가 워크로드에 최적인지를 묻습니다.\n결론적으로, MLOps는 모델이 \u0026lsquo;실행될 수 있도록\u0026rsquo; 보장하고, FinOps는 현재 구성으로 \u0026lsquo;실행되어야 하는지\u0026rsquo;를 보장합니다. FinOps 없이는 MLOps 파이프라인이 과도하게 크고 비싼 GPU에 모델을 효율적으로 배포하여 ROI를 음수로 만들 수 있습니다. MLOps 없이는 FinOps가 승인한 예산이 느리고 신뢰할 수 없는 수동 배포로 인해 낭비될 수 있습니다. 따라서 모델 서빙의 진정한 ROI를 측정하기 위해서는 MLOps 지표(배포 빈도, 모델 성능)와 FinOps 지표(총소유비용(TCO), 추론당 비용, 자원 활용률)를 결합한 전체적인 관점이 필요합니다.\n마치며 이번 포스트에서는 모델 서빙이 무엇을 지향해야 하는지 알아봤습니다. MLOps를 넘어 LLMOps에서 모델 서빙의 차이점을 이해하고, 에이전트 시대에서도 모델 서빙이 이어질 것임을 알 수 있었습니다.\n가장 인상 깊었던 내용은 기존 소프트웨어와의 차이점입니다. 분명 다름을 인지하고 있었지만, 이를 설명하는 것이 어려웠는데, 모델의 정확성은 확률론적 특성을 갖는다는 내용이 좋은 표현이 생각했습니다. 또한 ML 모델에서는 버그 외에 드리프트라는 개념이 존재한다는 것과 이에 대한 대응 방향을 인지할 수 있었습니다.\n이후의 포스트에서는 먼저 모델 서빙을 평가할 수 있는 지표들을 알아보겠습니다. 그리고 이 지표들을 근거로 더 좋은 모델 서빙을 위한 내용들을 알아보겠습니다.\n본 포스트는 Google Gemini의 응답을 기반으로 저의 의견을 반영하여 다시 작성했습니다.\n","date":"2025-09-16T19:03:00Z","permalink":"https://yeonhl.github.io/systems/model_serving/concept/","title":"모델 서빙의 개념"},{"content":"개요 이번 포스트에서는, MCP를 이루는 구성 요소들을 더 자세히 살펴보겠습니다. 각 요소가 어떤 목적으로 설계되었는지 알아보고 이에 맞게 아키텍처를 구성하는 것을 목표로 합니다.\n핵심 요소 이전 포스트에서 언급했듯이, Model Context Protocol (MCP)는 호스트(Host), 클라이언트(Client), 서버(Server)의 상호작용으로 동작합니다. 이 클라이언트-서버 아키텍처는 언어 서버 프로토콜(LSP)에서 영감을 받았으며, 각 구성요소의 역할을 분리함으로써 모듈성, 보안성 및 확장성을 달성합니다. 이 구성 요소들이 구체적으로 어떻게 동작하는지 알아보겠습니다.\nNOTE: 언어 서버 프로토콜 (LSP)과의 비교\nMCP는 코드 편집기와 언어별 도구 간의 통신을 표준화한 언어 서버 프로토콜(Language Server Protocol, LSP)에서 명시적으로 영감을 받았습니다. 그러나 MCP는 이 모델을 훨씬 더 확장합니다. LSP는 대체로 반응적(reactive) 입니다. 즉, IDE에서 사용자가 코드를 입력하거나 마우스를 올리는 등의 행동에 반응하여 진단 정보나 자동 완성을 제공합니다.\n반면, MCP는 능동적인 에이전트 중심 실행 모델(agent-centric execution model) 을 지원하도록 설계되었습니다. 핵심적인 차이점은 제어의 주체입니다. LSP에서는 인간 사용자가 주된 에이전트입니다. MCP에서는 AI 모델이 에이전트입니다. MCP는 AI가 도구를 발견하고, 여러 도구를 연계하며(chaining), 다단계 계획을 실행하는 자율적 워크플로우를 지원해야 함을 의미합니다. 이러한 능동성으로 MCP는 인간 주도 작업을 돕는 도우미를 넘어, 자율 에이전트의 중추적인 역할을 할 수 있습니다.\n호스트 MCP 호스트는 사용자가 직접 상호작용하는 AI 애플리케이션으로, VS Code와 같은 통합 개발 환경(IDE), Claude Desktop과 같은 데스크톱 어시스턴트, 또는 맞춤형 에이전트 애플리케이션의 형태를 가집니다. 호스트는 LLM을 포함하는 컨테이너 뿐만 아니라, 모든 MCP 상호작용을 시작하고, 관리하며, 보호하는 복잡한 책임을 수행합니다.\n호스트의 핵심 책임은 다음과 같이 분류할 수 있습니다.\n생명주기 관리 (Lifecycle Management): 호스트는 연결된 각 MCP 서버에 대해 하나씩, 여러 MCP 클라이언트 인스턴스를 생성, 관리 및 종료할 책임이 있습니다. 이는 전체 MCP 세션의 시작과 끝을 통제하는 역할입니다.\n오케스트레이션 (Orchestration): 호스트는 사용자 프롬프트나 에이전트 워크플로우에 대응하여 어떤 서버의 기능이 필요한지를 결정하는 주요 오케스트레이션 로직을 포함합니다. 예를 들어, 사용자가 \u0026ldquo;오늘 가입한 고객 수는?\u0026ldquo;이라고 질문하면, 호스트는 이 요청을 분석 MCP 서버로 라우팅하는 결정을 내립니다.\n컨텍스트 집계 (Context Aggregation): 호스트는 연결된 모든 클라이언트로부터 도구(Tools), 리소스(Resources), 프롬프트(Prompts)와 같은 컨텍스트를 수집하고 병합하여 LLM에 제공합니다. 여러 서버에서 제공하는 다양한 컨텍스트를 하나의 일관된 프롬프트로 구성하는 이 복잡한 작업은 주로 호스트가 수행하며, 프로토콜 자체는 이에 대한 구체적인 방법을 정의하지 않습니다.\n보안 및 동의 집행 (Security and Consent Enforcement): 호스트는 최종적인 보안 게이트키퍼 역할을 합니다. 사용자의 명시적인 동의 없이 도구를 실행하거나 데이터에 접근하는 것을 방지하고, 각 서버 간의 엄격한 보안 경계를 유지하며, 전반적인 보안 정책을 강제합니다. 이는 MCP 시스템의 신뢰성과 안전성을 보장하는 가장 중요한 기능입니다.\n클라이언트 MCP 클라이언트는 독립적인 애플리케이션이 아닌, 호스트 프로세스 내에 존재하는 저수준 컴포넌트입니다. 클라이언트의 핵심 역할은 단일 MCP 서버와의 연결을 관리하는 중개자 역할입니다. 호스트가 여러 서버와 통신해야 할 경우, 각 서버마다 별도의 클라이언트 인스턴스를 생성하여 1:1 관계를 유지합니다.\n클라이언트의 주요 책임은 다음과 같습니다.\n1:1 연결 관리 (1:1 Connection Management): 각 클라이언트는 특정 MCP 서버와 단일의 상태 저장(stateful) 세션을 설정하고 유지합니다. 이 구조는 각 서버와의 통신을 격리하여 복잡성을 줄이고 보안을 강화합니다.\n프로토콜 변환 (Protocol Translation): 클라이언트는 MCP 프로토콜의 기술적인 세부 사항을 처리합니다. JSON-RPC 메시지를 양방향으로 라우팅하고, 초기 핸드셰이크 과정에서 기능 협상(capability negotiation)을 관리하며, 구독 및 알림과 같은 비동기 통신을 처리합니다.\n격리 유지 (Maintaining Isolation): 클라이언트는 호스트의 관점에서 보안 경계를 강제합니다. 한 클라이언트의 통신 채널이 다른 클라이언트의 채널을 \u0026ldquo;엿보거나\u0026rdquo; 간섭할 수 없도록 보장하여, 서버 간의 정보 유출을 원천적으로 차단합니다.\nMCP 서버 MCP 서버는 MCP 사양에 따라 특정하고 집중된 기능 집합을 노출하는 독립적인 프로그램입니다. 서버는 로컬 머신에서 실행될 수도 있고(예: 파일 시스템 접근), 원격으로 호스팅될 수도 있습니다(예: Stripe API 연동). 서버의 핵심은 복잡한 비즈니스 로직을 추상화하고, 이를 LLM이 이해하고 사용할 수 있는 표준화된 형태로 제공하는 것입니다.\n서버의 핵심 책임은 다음과 같습니다.\n프리미티브 노출 (Exposing Primitives): 서버는 표준화된 MCP 데이터 모델인 리소스, 도구, 프롬프트를 통해 클라이언트에 컨텍스트를 제공합니다. 예를 들어, Git 서버는 git_log, git_diff와 같은 도구를 노출할 수 있습니다.\n집중된 로직 (Focused Logic): 서버는 단일 도메인이나 서비스에 집중하도록 설계되었습니다. 복잡한 오케스트레이션은 호스트의 역할이므로, 서버는 독립적으로 작동하며 자신의 전문 분야에만 집중합니다.\n제약 조건 준수 (Respecting Constraints): 서버는 호스트가 강제하는 보안 제약 및 권한 내에서 작동해야 합니다. 전체 대화 기록이나 다른 서버의 컨텍스트에 접근할 수 없으며, 호스트로부터 전달받은 최소한의 정보만을 사용하여 작업을 수행합니다.\n설계 원칙 MCP의 아키텍처는 몇 가지 핵심적인 설계 원칙에 기반합니다. 이 원칙들은 프로토콜의 유연성과 견고성을 보장하는 기반이 됩니다.\n결합성 (Composability): 각 서버는 독립적이고 모듈화된 단위입니다. 호스트는 여러 개의 서버를 조합하여 복잡한 기능을 구성할 수 있습니다. 예를 들어, 코드 분석 에이전트는 파일 시스템 서버, Git 서버, 정적 분석 서버를 동시에 사용하여 사용자 요청을 처리할 수 있습니다.\n격리 (Isolation): 서버 간의 엄격한 분리는 MCP의 근본적인 보안 원칙입니다. 호스트가 유일한 컨텍스트 집계자 역할을 함으로써, 서버는 인가되지 않은 데이터에 접근할 수 없으며 서버 간 상호 간섭으로 인한 위험이 완화됩니다.\n단순성 및 확장성 (Simplicity and Extensibility): 이 설계는 복잡한 오케스트레이션 로직을 호스트에 배치하여 서버를 쉽게 구축할 수 있도록 만듭니다. 또한, 프로토콜은 기능 협상 메커니즘을 통해 점진적으로 확장 가능하도록 설계되었습니다.\n이러한 아키텍처는 의도적인 트레이드오프를 내포합니다. MCP 사양은 서버 개발 경험을 단순화하는 대신, 복잡성과 책임을 호스트 구현에 집중시킵니다. 서버는 간단하고, 격리되어 있으며, 특정 기능에 집중합니다. 반면, 호스트는 모든 클라이언트를 관리하고, 모든 컨텍스트를 집계하며, 모든 보안 및 동의 정책을 시행해야 합니다. 결과적으로, MCP 기반 시스템의 보안과 견고성은 프로토콜 자체보다는 호스트 구현의 품질에 의해 결정됩니다. 호스트는 신뢰, 통제, 그리고 잠재적 실패의 단일 지점(single point of trust and failure)이 됩니다. 핵심 과제는 단순히 서버에 연결하는 것이 아니라, 정교하고 안전한 호스트를 구축하는 데 있습니다.\n데이터 모델 다음으로 살펴볼 것은 \u0026ldquo;프리미티브(Primitives)\u0026ldquo;입니다. 이전 포스트에서 데이터를 교환할 때 맥락을 보다 세분화된 표현으로 전달하기 위해 사용한다고 언급했습니다. 이는 프리미티브가 AI 애플리케이션과 공유할 수 있는 컨텍스트 정보의 유형과 수행할 수 있는 작업의 범위를 명시적으로 정의하기 때문입니다. 그렇다면 이들이 어떻게 풍부하고 구조화된 컨텍스트 교환을 가능하게 하는지 알아보겠습니다.\n서버 프리미티브 서버가 클라이언트에 컨텍스트를 제공하는 주요 방법은 세 가지 프리미티브를 통해 이루어집니다. 이를 누가 그것의 사용을 통제하는가에 대한 \u0026lsquo;제어 모델\u0026rsquo;에서 살펴보면 아키텍처를 알 수 있습니다.\n도구 (Tools): 행동과 상호작용 활성화 (모델 제어) 개념: 도구는 LLM이 발견하고 호출을 결정할 수 있는 실행 가능한 함수입니다. 데이터베이스 쿼리, API 호출, 파일 쓰기와 같이 외부 시스템의 상태를 변경하거나 부작용(side effect)을 일으키는 작업을 수행하는 데 사용됩니다. 예를 들어, OpenAI의 ChatGPT와 통합하기 위해서는 search와 fetch라는 특정 도구를 구현해야 합니다.\n제어 모델: \u0026ldquo;모델 제어(Model-controlled)\u0026ldquo;는 LLM이 사용자의 프롬프트와 도구의 설명을 바탕으로 자율적으로 특정 도구의 호출 여부와 시점을 결정합니다.\n스키마: 도구 정의는 고유한 name, LLM의 의사결정에 결정적인 역할을 하는 description, 그리고 인수를 정의하는 inputSchema를 포함합니다. 결과 검증을 위한 outputSchema를 선택적으로 포함할 수도 있습니다. inputSchema는 일반적으로 JSON Schema 객체 형식입니다.\n리소스 (Resources): 수동적 지식 제공 (애플리케이션 제어) 개념: 리소스는 LLM에 컨텍스트를 제공하는 읽기 전용의 파일과 유사한 데이터 객체입니다. 데이터베이스 스키마, 문서 내용, API 응답 등이 이에 해당하며, 부작용을 일으키지 않도록 설계되었습니다.\n제어 모델: \u0026ldquo;애플리케이션 제어(Application-controlled)\u0026rdquo; 또는 \u0026ldquo;사용자 제어(User-controlled)\u0026ldquo;는 호스트 애플리케이션이나 사용자가 UI 요소(예: \u0026lsquo;@\u0026rsquo; 기호 입력, \u0026lsquo;컨텍스트 추가\u0026rsquo; 버튼 클릭)를 통해 명시적으로 리소스를 컨텍스트에 포함시킬 시점을 결정한다는 것을 의미합니다. LLM은 자율적으로 리소스를 가져오도록 결정하지 않습니다.\n스키마: 리소스는 URI로 식별되며 name, title, mimeType과 같은 메타데이터를 포함합니다.\n프롬프트 (Prompts): 사용자 주도 워크플로우 (사용자 제어) 개념: 프롬프트는 사용자 상호작용을 안내하거나 복잡한 작업을 구조화하는, 미리 정의되고 매개변수화 가능한 메시지 템플릿입니다.\n제어 모델: \u0026ldquo;사용자 제어(User-controlled)\u0026ldquo;는 일반적으로 사용자가 UI 명령(예: 슬래시 명령어)을 통해 명시적으로 프롬프트를 선택한다는 것을 의미합니다.\n스키마: 프롬프트 정의는 name, description, arguments 목록, 그리고 템플릿을 구성하는 messages 배열을 포함합니다.\n클라이언트 프리미티브 MCP는 서버가 클라이언트에게 특정 작업을 요청할 수 있는 프리미티브를 정의합니다. 이는 더욱 복잡하고 양방향적인 워크플로우를 가능하게 합니다.\n샘플링 (Sampling): 서버의 언어 모델 추론 요청 개념: 서버가 클라이언트의 LLM에게 추론 작업(예: 텍스트 생성, 콘텐츠 요약)을 수행하고 그 결과를 반환하도록 요청할 수 있도록 허용합니다. 서버는 모델, 시스템 프롬프트, temperature와 같은 매개변수를 지정할 수 있습니다. 이는 AI 기능이 필요하지만 특정 모델에 종속되지 않고 LLM SDK를 내장하고 싶지 않은 서버에게 매우 중요합니다.\n사용 사례: Python SDK는 이를 위해 ctx.session.create_message 메서드를 제공합니다. 서버는 자체 API 키나 모델 종속성 없이, 샘플링을 사용하여 자체 데이터를 처리하거나 요약한 후 결과를 반환할 수 있습니다.\n정보 요청 (Elicitation): Human-in-the-loop 개념: 서버가 워크플로우를 일시 중지하고 호스트의 UI를 통해 사용자에게 추가 정보나 확인을 요청할 수 있게 합니다. 서버는 필요한 입력에 대한 스키마를 정의합니다. 호스트가 이 상호작용을 중재하여 사용자 제어와 개인 정보 보호를 보장합니다. 이 기능은 에이전트가 중요한 결정 지점에서 사용자에게 명확한 설명이나 승인을 요청할 수 있는 \u0026ldquo;인간 참여형(human-in-the-loop)\u0026rdquo; 워크플로우를 구현하는 공식적인 메커니즘입니다.\n사용 사례: Python SDK 예제에서는 book_table 도구가 특정 날짜에 예약이 불가능할 경우 ctx.elicit을 사용하여 사용자에게 대체 날짜를 묻는 것을 보여줍니다. 다단계 인증이 필요한 도구는 정보 요청을 사용하여 서버가 직접 원시 입력을 처리하지 않고도 사용자에게 안전하게 코드를 입력하도록 요청할 수 있습니다.\n루트 (Roots): 호스트 환경 상호작용 개념: 서버가 허가를 받아 접근할 수 있는 호스트의 로컬 환경에 대한 진입점입니다.\n사용 사례: 서버는 파일 시스템 디렉토리의 정보를 요청할 수 있습니다.\n데이터 스키마와 검증: JSON Schema 스키마: MCP 사양은 TypeScript 스키마 파일(schema.ts)을 통해 정의됩니다. 이는 프로토콜의 모든 데이터 구조에 대한 단일 진실 공급원(single source of truth) 역할을 합니다.\n구현: 실제 개발에서는 TypeScript의 Zod나 Python의 Pydantic과 같은 라이브러리를 사용하여 도구 인수 및 기타 프리미티브의 스키마를 정의하고 검증합니다. 이는 타입 안정성을 보장하고 견고한 오류 처리를 가능하게 합니다. 도구의 inputSchema는 일반적으로 JSON Schema 객체로 정의되어 언어에 구애받지 않는 상호 운용성을 제공합니다.\n프리미티브의 제어 모델 구분은 MCP에 내장된 근본적인 보안 및 사용자 경험(UX) 설계 패턴입니다. 이는 개발자가 AI 에이전트가 특정 기능에 대해 가져야 할 자율성의 수준을 추론할 수 있는 프레임워크를 제공하여, 시스템이 얼마나 안전하고 예측 가능하게 작동할지에 직접적인 영향을 미칩니다. 예를 들어, 민감한 작업은 LLM이 예기치 않게 호출할 수 있는 \u0026lsquo;도구\u0026rsquo;보다는 명시적인 사용자 조치가 필요한 \u0026lsquo;리소스\u0026rsquo;로 노출해야 합니다. 이처럼 제어 모델을 올바르게 선택하는 것은 안전하고 신뢰할 수 있는 AI 에이전트를 설계하는 데 있어 가장 중요한 아키텍처 결정 중 하나입니다.\n통신 앞에서 데이터 모델(\u0026ldquo;무엇을\u0026rdquo;)을 알아봤다면, 이제는 통신 메커니즘(\u0026ldquo;어떻게\u0026rdquo;)으로 초점을 전환합니다. MCP의 기본 프로토콜 및 전송 계층, 상태 저장 세션 생명주기까지 전체 통신 시퀀스를 살펴보겠습니다.\n상태: 에이전트 워크플로우 특화 RESTful API는 상태 비저장(stateless) 방식으로 작동합니다. 각 요청은 이전 요청과 독립적으로 처리되며, 클라이언트는 여러 단계에 걸친 작업의 상태와 컨텍스트를 스스로 관리해야 할 책임이 있습니다.\n이는 간단한 데이터 조회에는 효율적일 수 있으나, 여러 단계의 상호작용이 필요한 복잡한 작업을 수행하는 AI 에이전트에게는 상당한 부담입니다. 에이전트 또는 클라이언트 측 오케스트레이터가 모든 상태를 기억하고 매 요청마다 전체 컨텍스트를 다시 전송하는 유일한 상태 관리자가 되면서 비효율적이고 시스템을 취약하게 만드는 원인이 됩니다.\n반면, MCP는 상태를 유지하는(stateful) 영속적인 연결(persistent connections) 을 기반으로 설계되었습니다. 이는 일련의 개별적인 요청-응답이 아닌, 마치 웹소켓(WebSocket) 세션과 유사한 지속적인 양방향 통신 채널을 구축하는 것을 의미합니다.\n상태 유지 연결은 다단계 작업 수행에 있어 결정적인 장점을 가집니다. 서버는 진행 중인 워크플로우의 컨텍스트를 세션 내에서 유지할 수 있으므로, 클라이언트는 매번 전체 컨텍스트를 다시 보낼 필요가 없습니다. 인지적 부담의 일부를 도구 서버로 오프로드합니다. 이를 통해 도구 자체가 메모리를 가지고 에이전트를 특정 프로세스로 안내하는 등 훨씬 더 정교한 상호작용이 가능해집니다.\nMCP 로드맵에서도 연결 끊김 및 재연결에 대한 탄력적인 처리와 장기 실행 작업을 지원하는 것이 핵심 우선순위로 명시되어 있으며, 이는 상태 유지가 핵심 설계임을 의미합니다.\n이러한 아키텍처적 선택은 기술적 편의를 넘어, 에이전트와 도구 간의 상호작용 모델을 단순한 \u0026lsquo;요청-응답\u0026rsquo;에서 \u0026lsquo;대화형 상호작용\u0026rsquo; 모델로 전환시킵니다. 그리고 AI 에이전트와 도구 간의 관계를 \u0026lsquo;파트너십\u0026rsquo;으로 발전시키는 \u0026ldquo;협력적 인지(collaborative cognition)\u0026ldquo;라는 새로운 패러다임을 가능하게 합니다. 도구는 더 이상 수동적으로 호출되는 함수가 아니라, 문제 해결 과정에 능동적으로 참여하는 주체가 됩니다. 이는 고급 에이전트 행동을 위한 필수 전제 조건입니다.\n구체적으로 살펴보겠습니다. 상태 비저장 API는 거래적(transactional)입니다. 에이전트가 도구에게 명령하면, 과거 상호작용에 대한 기억이 없는 도구는 그저 명령을 수행합니다. 그러나 MCP의 상태 유지 연결은 서버가 세션의 컨텍스트를 \u0026ldquo;기억\u0026quot;할 수 있음을 의미합니다. 예를 들어, \u0026ldquo;순차적 사고(Sequential Thinking)\u0026rdquo; 서버는 동적인 문제 해결 과정을 추적할 수 있습니다.\n여기에 Elicitation 및 Sampling과 같은 서버 주도 상호작용 기능은 서버가 통신을 시작하여 에이전트에게 추가 정보를 요청하거나 추론 작업을 수행하도록 요청할 수 있게 합니다. 이는 강력한 피드백 루프를 형성합니다. 에이전트가 도구에 도움을 요청하면, 도구는 자체 상태와 전문화된 로직을 사용하여 추가 정보가 필요하다고 판단하고 에이전트에게 다시 질문(Elicitation)하거나, 창의적인 단계가 필요하다고 판단하여 에이전트에게 텍스트 생성을 요청(Sampling)할 수 있습니다. 결과적으로 복잡한 문제를 해결하는 데 필요한 인지적 부하가 분산됩니다.\n정리하면, LLM은 일반적인 추론과 언어를 처리하고, 전문화된 MCP 서버는 도메인 특화 로직, 상태 및 상호작용 흐름을 처리합니다. 이 협력 모델은 LLM이 모든 것을 관리해야 하는 모델보다 훨씬 더 강력하고 효율적입니다.\n세션 생명주기 생명주기의 핵심은 \u0026lsquo;기능 협상\u0026rsquo; 과정으로, 클라이언트와 서버가 서로가 제공하고 지원하는 기능을 동적으로 확인하고 합의하는 절차입니다.\n초기화: 핸드셰이크 및 기능 협상 초기화 (initialize): 연결이 시작되면, 클라이언트는 먼저 initialize 요청을 서버에 보냅니다. 이 요청에는 클라이언트가 지원하는 프로토콜 버전 정보와 함께, 클라이언트가 제공할 수 있는 기능(예: 서버로부터 LLM 추론 요청을 받을 수 있는 sampling 기능 지원 여부)에 대한 정보가 포함됩니다.\n기능 협상 (Response to initialize): initialize 요청을 받은 서버는 자신의 정보(이름, 버전 등)와 함께 자신이 제공할 수 있는 기능의 목록을 담아 응답합니다. 예를 들어, 서버는 자신이 tools와 resources 프리미티브를 제공하는지, 그리고 도구 목록이 변경될 때 listChanged 알림을 보낼 수 있는지 여부를 capabilities 객체에 담아 클라이언트에 알립니다. 이 교환 과정을 통해 양측은 세션에서 어떤 종류의 상호작용이 가능한지에 대해 명시적으로 합의하게 됩니다.\n초기화 완료 (initialized): 서버로부터 기능 목록을 성공적으로 수신한 클라이언트는 initialized 알림을 서버에 보내 핸드셰이크가 완료되었음을 알립니다. 이 시점부터 본격적인 상호작용이 시작됩니다.\n운영: 동적 컨텍스트 교환 이 단계에서는 클라이언트와 서버가 초기화 단계에서 협상된 기능에 따라 요청, 응답, 알림을 자유롭게 교환합니다. 예를 들어, 클라이언트는 tools/list로 사용 가능한 도구를 확인하고 tools/call로 특정 도구를 실행할 수 있으며, 서버는 notifications/prompts/list_changed 알림을 보내 프롬프트 목록의 변경 사항을 알릴 수 있습니다.\n종료: 정상적인 연결 해제 연결은 기본 전송 메커니즘을 사용하여 정상적으로 종료됩니다. stdio의 경우 stdin 스트림을 닫고, HTTP 세션의 경우 DELETE 요청을 보내는 방식입니다.\nNOTE: 상태 저장 세션과 기능 협상 채택의 목적\n초기 핸드셰이크는 단순한 인증 절차가 아니라, 세션의 \u0026ldquo;계약 조건\u0026quot;을 설정하는 공식적인 협상 과정입니다. 이 상태 저장 및 협상된 컨텍스트는 listChanged와 같은 동적 업데이트 알림, sampling과 같은 서버 주도 요청, 세션 내 효율적인 컨텍스트 캐싱 등 상태 비저장 모델에서는 불가능한 고급 기능을 가능하게 합니다.\n이는 클라이언트와 서버가 독립적으로 개발되고 시간이 지남에 따라 새로운 기능을 추가할 수 있게 하면서도, 핸드셰이크를 통해 하위 호환성을 보장하고 지원되지 않는 작업으로 인한 런타임 오류를 방지합니다. 이것이 프로토콜의 확장성과 장기적인 생존 가능성의 핵심입니다.\nJSON-RPC 2.0 MCP는 메시징 형식을 위해 널리 확립된 JSON-RPC 2.0 프로토콜을 기반으로 구축되었습니다.3 이는 요청, 응답 및 오류 처리를 위한 표준화된 구조를 제공하여 프로토콜의 안정성과 예측 가능성을 높입니다.\n요청(Request): 요청 메시지는 클라이언트가 서버의 특정 기능을 호출하기 위해 전송하는 통신의 시작점입니다. 이 메시지는 jsonrpc, method, params, 그리고 고유한 id 필드를 포함합니다. method 필드는 tools/call이나 resources/list와 같이 호출할 함수의 이름을 명시합니다. 가장 중요한 것은 id 필드로, 모든 요청을 고유하게 식별하여 비동기적인 환경에서도 요청과 응답을 정확하게 연결하는 역할을 합니다. id의 존재는 MCP가 단순한 단방향 호출이 아닌, 상태를 추적하고 관리하는 상태 기반(stateful) 상호작용을 전제로 설계되었음을 보여줍니다.\n응답(Response): 서버는 요청을 처리한 후, 반드시 해당 요청의 id를 포함하는 응답 메시지를 반환합니다. 작업이 성공했을 경우 result 필드에 결과 데이터를 보내고, 실패했을 경우에는 error 필드에 에러 코드와 메시지를 보냅니다.\n알림(Notification): 알림은 요청과 달리 id 필드가 없어 응답을 요구하지 않는 단방향 메시지입니다. 이는 서버가 자신의 상태 변화를 클라이언트에게 능동적으로 전파해야 할 때 사용합니다. 예를 들어, 서버에 새로운 도구가 추가되거나 기존 도구가 제거되었을 때, 서버는 notifications/tools/list_changed 알림을 보내 클라이언트가 최신 도구 목록을 유지할 수 있도록 합니다. 이 메커니즘은 MCP 환경을 정적인 상태가 아닌, 동적으로 변화하고 반응하는 생태계로 만드는 핵심 요소로 작용합니다.\nTIP: 도구 변동 알림이 필요한 이유\n성능 향상을 위해 호스트는 서버로부터 받은 도구 목록을 캐시합니다. 이 캐시가 오래된 정보가 되는 것을 방지하기 위해, 서버는 초기화 시 listChanged: true 기능을 선언할 수 있습니다.\n만약 서버에서 사용 가능한 도구가 변경되면, 서버는 notifications/tools/list_changed 알림을 보냅니다. 이 알림을 받은 호스트는 자신의 캐시가 오래되었음을 인지하고 tools/list를 다시 요청하여 도구 목록을 갱신합니다.\n이 이벤트 기반 모델은 지속적으로 서버 상태를 확인하는 폴링(polling) 방식보다 훨씬 효율적입니다.\n전송 방식 MCP는 배포 시나리오와 요구사항에 맞게 전송 방식을 정할 수 있습니다. MCP 프로토콜 자체는 특정 전송 방식에 종속되지 않지만, 두 가지 표준 메커니즘을 정의하여 대부분의 사용 사례를 지원합니다.\n이러한 이중화된 전송 메커니즘은 MCP가 \u0026lsquo;보안\u0026rsquo;을 중시하는 로컬 환경과 \u0026lsquo;확장성\u0026rsquo;을 중시하는 원격 클라우드 환경이라는 두 가지 핵심 가치를 동시에 추구하기 위한 의도적인 설계적 타협입니다. 개발자는 자신의 사용 사례에 맞춰 최적의 전송 방식을 선택할 수 있지만, 이는 동시에 MCP 클라이언트(호스트)가 두 가지 상이한 통신 모델을 모두 안정적으로 처리해야 하는 기술적 과제를 가집니다.\nstdio (표준 입출력) 메커니즘: 이 방식은 MCP 서버가 호스트 애플리케이션(예: IDE, 데스크톱 앱)의 로컬 서브프로세스로 실행될 때 사용됩니다. 통신은 운영체제의 표준 입력(stdin)과 표준 출력(stdout) 스트림을 통해 이루어집니다. 가장 큰 장점은 네트워크 스택을 거치지 않아 지연 시간이 마이크로초 단위로 극도로 낮고, 운영체제 수준의 프로세스 격리를 통해 외부 네트워크로부터의 접근이 원천적으로 차단되어 보안성이 매우 높다는 점입니다.\n사용 사례: VS Code나 Claude Desktop과 같은 개발 도구에서 사용자의 로컬 파일 시스템에 접근하거나 로컬 스크립트를 실행하는 등 민감한 작업을 안전하고 신속하게 처리하는 데 최적화되어 있습니다. 단일 사용자, 로컬 전용 시나리오에 대해 간단하고 안전한 통신을 제공합니다.\nStreamable HTTP (레거시 HTTP+SSE 대체) 메커니즘: 원격 또는 네트워크 서버에 사용됩니다. 단일 표준 HTTP 연결을 통해 더 확장 가능하고 효율적인 양방향 통신을 가능하게 합니다. 주로 청크 분할 전송 인코딩(chunked transfer encoding)을 사용합니다.\n사용 사례: 클라우드 배포(예: AWS Lambda) 및 엄격한 방화벽 규칙이 있는 엔터프라이즈 네트워크 환경에 적합합니다. 웹 서비스, 서드파티 API 또는 다중 사용자/원격 서비스 연결에 필수적입니다. Mcp-Session-Id 헤더를 통해 상태 저장 세션을 지원하여, 여러 요청에 걸쳐 컨텍스트를 유지할 수 있습니다.\n전송 계층이 STDIO/SSE에서 Streamable HTTP로 발전한 것은 MCP가 개발자 중심의 로컬 우선 도구에서 엔터프라이즈급 클라우드 네이티브 프레임워크로 전략적으로 전환하고 있음을 보여줍니다. Streamable HTTP는 단방향 통신이라는 SSE의 한계와 웹소켓의 운영 복잡성을 극복하고, 원격 보안 통신을 위한 강력하고 널리 호환되는 솔루션을 제공합니다.\nNOTE: HTTP+SSE 방식이 대체된 이유\nStreamable HTTP 이전 원격 서버와의 통신을 위해 설계되었습니다. 클라이언트에서 서버로의 요청은 일반적인 HTTP POST 요청을 사용하고, 서버에서 클라이언트로의 지속적인 데이터 스트리밍은 Server-Sent Events(SSE)를 통해 이루어집니다.\nSSE는 단방향(서버→클라이언트) 통신 채널을 오랫동안 유지하며 업데이트를 푸시하는 데 특화되어 있습니다. 이 방식은 기존 웹 인프라(프록시, 방화벽, 로드밸런서)와 완벽하게 호환되므로, Stripe, GitHub, Sentry 등 클라우드 기반의 원격 SaaS API를 MCP 서버로 연동하는 데 이상적입니다.\n다만, 양방향 통신을 위해 요청(HTTP POST)과 응답 스트림(SSE)을 별도의 채널로 관리해야 하는 복잡성이 존재했고, 최신 MCP 사양에서는 이러한 복잡성을 줄이기 위해 단일 HTTP 연결 내에서 양방향 스트리밍을 지원하는 Streamable HTTP로의 통합을 추진하고 있습니다.\nNOTE: WebSockets\n최대의 상호작용성이 요구될 경우 사용 가능한 전송 방식입니다. 완전한 양방향(full-duplex) 통신을 제공하여 스트리밍 데이터, 진행 상황 업데이트, 협업 상호작용 등이 포함된 복잡한 AI 워크플로우에 특히 유용할 수 있습니다.\n특성 stdio (Standard Input/Output) HTTP+SSE Streamable HTTP WebSockets (대안) 통신 모델 양방향 단방향 (서버→클라이언트) 양방향 (단일 연결) 완전 양방향 (Full-duplex) 주요 장점 - 매우 낮은 지연 시간 - 네트워크 설정 불필요 - OS 수준의 높은 보안 (샌드박싱) - 기존 웹 인프라와 호환 - 실시간 결과 스트리밍 가능 - 기존 웹 인프라와 호환 - 서버리스 환경 지원 - 실시간 결과 스트리밍 가능 - 진정한 실시간 양방향 통신 - 단일 연결 관리로 인한 단순성 주요 단점/고려사항 - 원격 서버 연결 불가 - 호스트가 서버 프로세스 생명주기 관리 필요 - 양방향 통신을 위한 채널 관리 복잡성 (HTTP+SSE) - 장기 연결을 위한 프록시/로드밸런서 설정 필요 - 장기 연결을 위한 프록시/로드밸런서 설정 필요 - 일부 기업 네트워크에서 제한될 수 있음 - 초기 핸드셰이크 오버헤드 존재 대표 사용 사례 - IDE 플러그인 (VS Code, Cursor) - 로컬 파일 시스템 접근 - 데스크톱 AI 어시스턴트 (Claude Desktop) 원격 API 연동 (Stripe, Sentry) - 클라우드 기반 데이터 소스 연결 - 웹 브라우저 기반 클라이언트 - 원격 API 연동 (Stripe, Sentry) - 클라우드 기반 데이터 소스 연결 - 웹 브라우저 기반 클라이언트 - 고빈도 상호작용이 필요한 채팅 애플리케이션 - 실시간 협업 도구 지연 시간 매우 낮음 낮음-중간 낮음-중간 매우 낮음 확장성 제한적 (단일 머신) 중간 높음 높음 네트워크 호환성 해당 없음 (로컬) 표준 HTTP/S 포트 표준 HTTP/S 포트 별도 프로토콜/포트 필요 가능 현재 상태 활성 사용되지 않음 (Deprecated) 현재 표준 고려 대상 마치며 이번 포스트에서는 MCP 생태계를 이루는 각 요소와 그 역할, 데이터 교환을 위한 각 프리미티브의 목적, MCP 통신의 특징과 그 목적을 확인했습니다. MCP 설계 과정에 담긴 철학을 확인하고 이에 맞게 아키텍처를 설계했을 때 향후 업데이트 및 생태계의 지원을 기대할 수 있다고 생각합니다.\n포스트를 위한 자료를 찾아보면서, 내부 동작이 어떻게 이루어지는지 자세히 이해할 수 있었습니다. 또한 MCP가 상태를 저장하는 목적을 명확하게 알 수 있었습니다. 여러 에이전트가 공통으로 사용할 수 있는 기능들을 어떻게 재사용할 것인지 고민이 있었는데, MCP에 담긴 도메인 로직 구현의 철학에서 그에 대한 답변을 얻은 것 같습니다.\n다음 포스트에서는 MCP를 비즈니스 환경에서 설계하고 구현할 때, 보안 등의 측면에서 고려해야 할 점에 대해 살펴보겠습니다. 이후에는 MCP의 아키텍처와 철학, 고려할 점을 반영하여 개발하는 과정을 작성하려 합니다.\n본 포스트는 Google Gemini의 응답을 기반으로 저의 의견을 반영하여 다시 작성했습니다.\n","date":"2025-08-14T19:03:00Z","permalink":"https://yeonhl.github.io/systems/mcp/architecture/","title":"MCP 아키텍처"},{"content":"개요 MCP 이전에도 LLM은 도구를 사용하고 있었고, 외부 데이터에 접근하고 있었습니다. 그렇다면 MCP는 어떤 문제를 해결하기 위해 등장했을까요?\nMCP가 등장하기 전에는, M개의 AI 모델을 N개의 외부 도구 또는 데이터 소스에 연결하기 위해 각각의 통합, 즉 M×N개의 커넥터를 개발해야 했습니다. 그리고 각각의 맞춤형 커넥터는 개발, 테스트, 유지보수, 보안 검토를 위해 재사용이 불가능한 상당한 엔지니어링 자원을 요구했습니다. 이는 재사용이 불가능하고 감사가 어려웠으며, 기반이 되는 API나 모델이 업데이트될 때마다 쉽게 손상되어 단편적이고 예측 불가능한 시스템을 만들었고, \u0026ldquo;높은 통합 비용과 개발자 오버헤드\u0026rdquo;, \u0026ldquo;중복된 개발 노력\u0026rdquo;, 그리고 \u0026ldquo;과도한 유지보수 부담\u0026quot;으로 이어졌습니다. (이 문제를 \u0026lsquo;M×N 문제\u0026rsquo;라고 합니다.) 이 문제는 소프트웨어 개발의 비효율성을 넘어 AI 시스템의 확장을 근본적으로 저해하는 장벽이었습니다.\n이 포스트에서는 MCP가 어떤 특징을 갖고, 어떤 변화를 가져왔는지 살펴보겠습니다.\nMCP의 목표 M x N 통합 위기 해결 MCP는 \u0026ldquo;M×N 문제\u0026quot;를 더 관리하기 쉬운 \u0026ldquo;M+N 문제\u0026quot;로 전환합니다. 도구 제작자는 N개의 표준화된 MCP 서버를 구축하고, 애플리케이션 개발자는 M개의 MCP 클라이언트를 구축합니다. 이는 재사용 가능하고 상호 운용 가능한 계층을 생성하여, 개발자들이 맞춤형 통합을 만들고 유지하는 데 드는 시간을 절약합니다.\n이는 단순히 함수 호출을 대체하는 것이 아니라, 더 일관되고 단순한 개발 패러다임을 구축하는 것입니다. 이 프로토콜은 AI 애플리케이션이 외부 도구, 데이터 소스, 시스템과 연결되는 방식을 표준화하여 분편화된 통합 워크플로우 문제를 해결하기 위해 설계되었습니다.\n에이전트 지원 주요 목표 중 하나는 더 정교하고 자율적인 AI 에이전트의 개발을 촉진하는 것입니다. MCP는 다양한 도구와 데이터셋에 걸쳐 컨텍스트를 유지함으로써 다단계 \u0026ldquo;사고의 연쇄(chain-of-thought)\u0026rdquo; 추론을 지원하고, 에이전트가 사용자를 대신하여 복잡한 작업을 수행합니다.\n이 프로토콜은 AI 기능을 \u0026ldquo;레고 블록\u0026quot;처럼 조합하고 맞출 수 있는 모듈식 아키텍처를 장려합니다. 문서 조회와 메시징 API를 결합하는 등 여러 도구를 조율하여 더 복잡한 목표를 달성합니다.\n이 비전은 다중 에이전트 시스템까지 확장됩니다. 목표는 모델이 최신 컨텍스트에 접근하여 AI 성능과 관련성을 향상하는 것입니다. 모델을 고립 상태에서 벗어나게 하여, 더 신뢰할 수 있는 결과물을 제공합니다.\nMCP의 특징 원칙 상호 운용성 (Interoperability): 벤더 종속성(vendor lock-in)을 깨고 MCP를 준수하는 모든 모델이 MCP를 준수하는 모든 도구와 작동할 수 있도록 합니다. 구성 가능성 (Composability): 개발자들이 모듈식 \u0026ldquo;플러그 앤 플레이\u0026rdquo; 방식으로 도구와 데이터 소스를 결합하여 복잡하고 다단계적인 에이전트 워크플로우를 쉽게 구축할 수 있도록 합니다. 발견 가능성 (Discoverability): AI 에이전트가 사전에 프로그래밍된 지식 없이도 런타임에 서버에 동적으로 질의하여(\u0026ldquo;어떤 도구를 제공하나요?\u0026rdquo;) 그 능력을 파악하고, 더 큰 적응성을 가능하게 합니다. 이는 특히 정적인 API와의 핵심적인 차이점입니다. 구성 요소 MCP는 세 가지 핵심 구성 요소로 이루어집니다:\n호스트(Host): Claude Desktop이나 IDE와 같이 여러 클라이언트를 관리하고 사용자 권한을 집행하는 조정자 역할의 LLM 애플리케이션입니다. 클라이언트(Client): 단일 서버와 1:1 상태 저장(stateful) 연결을 유지하는 전용 커넥터입니다. 메시지 라우팅, 프로토콜 버전 협상, 서버의 기능 관리 등을 책임집니다. 서버(Server): 외부 세계의 기능(tools, resources, prompts)을 AI 모델에게 노출하는 구성 요소입니다. 이 아키텍처의 핵심은 결합성(Composability) 입니다. 하나의 애플리케이션이 클라이언트와 서버 역할을 동시에 수행할 수 있어, 계층적인 에이전트 시스템 구축이 가능합니다. 예를 들어, 주 에이전트(클라이언트)가 특정 작업을 전문 하위 에이전트(서버)에게 위임하고, 이 하위 에이전트는 다시 다른 MCP 서버(파일 시스템 서버 등)의 클라이언트가 되어 필요한 도구를 호출하는 복잡한 워크플로우를 구성할 수 있습니다.\n통신 MCP는 구조화된 통신을 위해 명확한 역할을 가진 클라이언트-서버 모델을 채택합니다.\n클라이언트는 서버와 일대일 연결을 유지하며, 안전하고 격리된 통신 채널 역할을 합니다. 호스트가 요청을 조율하지만, 모든 통신은 클라이언트를 통해 중개됩니다. 이는 서버와 호스트가 직접 통신하지 않도록 보장하는 핵심적인 보안 설계 원칙입니다.\nMCP는 클라이언트와 서버 간의 연결에 대해 예측 가능한 동작을 보장하기 위해 엄격한 3단계 생명주기를 강제합니다.\n초기화(Initialization): 클라이언트와 서버가 서로 지원하는 프로토콜 버전과 기능을 교환하고 협상합니다. 이는 런타임에 사용 가능한 기능을 동적으로 발견하고 호환성을 보장하는 매우 중요한 단계입니다. 작동(Operation): 협상된 기능의 범위 내에서 정상적인 통신이 이루어집니다. 종료(Shutdown): 연결을 정상적으로 종료하는 절차를 따릅니다. 전송 계층 프로토콜 자체는 가볍고 널리 이해되는 원격 프로시저 호출 프로토콜인 JSON-RPC 2.0을 기반으로 구축되었습니다. MCP는 다양한 배포 시나리오를 지원하기 위해 전송 계층을 유연하게 설계했으며, 그 발전 과정은 프로토콜의 적용 범위 확대를 명확히 보여줍니다.\nSTDIO (Standard Input/Output): 가장 초기의 단순한 전송 방식으로, 클라이언트와 서버가 동일한 환경에서 실행되는 로컬 프로세스 통합에 이상적입니다. 예를 들어, IDE(호스트)가 로컬 파일 시스템에 접근하는 서버와 통신하는 경우에 사용됩니다. STDIO의 가장 큰 강점은 기존의 원격 전용 API(예: REST)가 쉽게 복제할 수 없는 로컬 우선(local-first) 에이전트 워크플로우를 가능하게 한다는 점입니다. HTTP와 서버-전송 이벤트 (SSE): 원격 연결을 위해 처음 도입된 메커니즘으로, 서버가 클라이언트에게 비동기적으로 알림을 푸시할 수 있게 했습니다. 하지만 SSE는 장시간 연결을 유지해야 하는 특성 때문에 기업 방화벽에 의해 차단되거나, AWS Lambda와 같은 상태 비저장(stateless) 클라우드 함수 환경에서는 사용하기 어렵고, 역압력(back-pressure) 처리가 까다로운 문제점을 드러냈습니다. 스트리밍 가능한 HTTP (Streamable HTTP): 2025년 3월 업데이트에서 도입된 현재의 표준 원격 통신 방식입니다. 이 방식은 청크 분할 전송 인코딩(chunked transfer encoding)을 지원하는 단일 HTTP 요청을 통해 양방향 바이트 스트림을 터널링합니다. 이 방식을 통해 MCP 서버를 상태 비저장 클라우드 함수로 배포할 수 있게 되었고, 일반적인 기업 네트워크 프록시 문제를 우회할 수 있어, MCP가 프로덕션 등급의 클라우드 네이티브 애플리케이션에 적용될 수 있는 길을 열었습니다. WebSocket: 최대의 상호작용성이 요구되는 시나리오를 위한 전송 방식으로 언급되며, 완전한 양방향(full-duplex) 통신을 제공합니다. 이는 스트리밍 데이터, 진행 상황 업데이트, 협업 상호작용 등이 포함된 복잡한 AI 워크플로우에 특히 유용할 수 있습니다. 전송 방식 주요 사용 사례 통신 모델 지연 시간 확장성 네트워크 호환성 현재 상태 STDIO 로컬 프로세스 간 통신 (예: 데스크톱 앱) 양방향 매우 낮음 제한적 (단일 머신) 해당 없음 (로컬) 활성 HTTP + SSE 초기 원격 연결 단방향 (서버→클라이언트) 낮음-중간 중간 표준 HTTP/S 포트 사용되지 않음 (Deprecated) Streamable HTTP 원격/클라우드 배포 양방향 (단일 연결) 낮음-중간 높음 표준 HTTP/S 포트 현재 표준 WebSocket 고도의 실시간 상호작용 완전 양방향 (Full-duplex) 매우 낮음 높음 별도 프로토콜/포트 필요 가능 고려 대상 데이터 교환 단순한 함수 호출 API는 모든 외부 상호작용을 \u0026ldquo;이것을 하라\u0026quot;는 명령으로 취급합니다. 이는 맥락에 대한 일차원적인 시각입니다.\nMCP는 맥락을 전달하기 위해 기능보다 더 세분화된 표현 단위인 프리미티브를 사용합니다. 이는 기능 목록이 아니라, AI와 시스템 간의 통신을 위한 구조화된 문법입니다. 이는 맥락 뿐만 아니라 맥락의 의도를 전달하는 것이 중요하다는 철학을 아키텍처적으로 구현한 것입니다.\n서버의 프리미티브 핵심은 세 가지 프리미티브를 통한 표준화된 상호작용입니다: Tools(실행 가능한 함수), Resources(구조화된 읽기 전용 데이터), 그리고 Prompt Templates(사전 정의된 지침). 일반적으로 도구는 모델이 제어하고, 리소스와 프롬프트는 사용자가 제어합니다.\n도구 (Tools): API를 호출하거나, 데이터베이스에 쿼리를 보내거나, 계산을 수행하는 등 행동을 수행하고 부수 효과(side effect)를 가질 수 있는 실행 가능한 함수입니다. 이는 행동하려는 의도(intent to act) 를 나타냅니다. 리소스 (Resources): 파일의 내용, API 응답 결과, 데이터베이스 레코드 등 같이 모델의 컨텍스트를 풍부하게 하기 위해 제공되는 구조화된 읽기 전용 데이터입니다. 이는 정보를 제공하려는 의도(intent to inform) 를 나타냅니다. 프롬프트 (Prompts): 특정 작업을 위해 모델의 추론을 안내할 수 있는 재사용 가능한 사전 정의된 지침 템플릿입니다. 이는 안내하려는 의도(intent to guide) 를 나타냅니다. 이러한 의도 분리로 더 정교한 시스템 설계가 가능합니다. 예를 들어, 보안에 민감한 애플리케이션은 에이전트에게 민감한 데이터베이스의 Resources(읽기 전용 데이터)에 대한 접근은 허용하되, 데이터를 수정할 수 있는 Tools(행동)에 대한 접근은 거부할 수 있습니다. 단순한 함수 호출 인터페이스는 이를 명확하게 표현하기 어렵습니다. 이 문법은 시스템이 AI에게 \u0026ldquo;여기 읽을 데이터가 있다\u0026rdquo;, \u0026ldquo;여기 네가 취할 수 있는 행동이 있다\u0026rdquo;, \u0026ldquo;이 문제는 이렇게 접근해야 한다\u0026quot;와 같이 미묘한 지시를 전달하여, LLM과 보다 정교하게 통신합니다.\n프리미티브 철학적 의도 기술적 기능 사용 사례 예시 도구 (Tool) 행동하기 (To Act) 부수 효과가 있는 실행 가능한 함수 post_message_to_slack, create_calendar_event 리소스 (Resource) 정보 제공하기 (To Inform) 구조화된 읽기 전용 데이터 재무 보고서 PDF, 데이터베이스 스키마, 사용자 프로필 프롬프트 (Prompt) 안내하기 (To Guide) 재사용 가능한 지침 템플릿 법률 문서 요약 템플릿, 코드 리팩토링 지침 클라이언트의 프리미티브 서버보단 사용 빈도가 낮지만, 클라이언트도 프리미티브를 가집니다.\n루트 (Roots): 서버가 허가를 받아 접근할 수 있는 호스트의 로컬 환경(예: 파일 시스템 디렉토리)에 대한 진입점입니다. 샘플링 (Sampling): 서버가 클라이언트의 LLM에게 추론 작업(예: 텍스트 생성, 콘텐츠 요약)을 수행하고 그 결과를 반환하도록 요청할 수 있습니다. 서버는 모델, 시스템 프롬프트, temperature와 같은 매개변수를 지정할 수 있습니다. Python SDK는 이를 위해 ctx.session.create_message 메서드를 제공합니다. 이 기능은 전문화된 도구가 범용 추론이나 창의적 능력을 가진 호스트 LLM을 활용할 수 있게 하여, 도구가 다른 도구를 사용하는 것과 같은 구성 가능한 시스템을 만듭니다. 여기서 LLM은 일종의 \u0026ldquo;추론 도구\u0026rdquo; 역할을 하게 됩니다. 샘플링을 사용하면 MCP 서버는 자체적으로 무거운 LLM을 내장하거나 특정 모델에 종속될 필요 없이, 모델 독립적인 도구를 만들 수 있습니다. 채록 (Elicitation): 서버가 워크플로우를 일시 중지하고, 클라이언트를 통해 최종 사용자에게 추가적인 구조화된 정보를 요청할 수 있습니다. 서버는 필요한 입력에 대한 스키마를 정의합니다. Python SDK 예제에서는 book_table 도구가 특정 날짜에 예약이 불가능할 경우 ctx.elicit을 사용하여 사용자에게 대체 날짜를 묻는 것을 보여줍니다. 이 기능은 에이전트가 중요한 결정 지점에서 사용자에게 명확한 설명이나 승인을 요청할 수 있는 \u0026ldquo;인간 참여형(human-in-the-loop)\u0026rdquo; 워크플로우를 구현하는 공식적인 메커니즘입니다. 활용 MCP의 핵심 원칙 중 하나는 구조화된 컨텍스트의 사용입니다. 기존 REST API의 응답을 그대로 LLM의 컨텍스트 창에 주입하면, 불필요한 정보로 인해 토큰이 낭비되고 모델의 추론 능력이 저하될 수 있습니다. MCP는 구조화된 도구 출력(Tool Output Schema) 과 같은 기능을 통해 이 문제를 해결합니다. 서버는 반환할 데이터의 형태를 미리 선언하고, LLM에게는 작업과 관련된 간결하고 구조화된 객체만 전달하여 토큰 효율성을 높이고 모델이 정보를 더 효과적으로 파싱하도록 돕습니다.\n또한 정적인 데이터 교환을 넘어 동적 컨텍스트 처리를 지원합니다. 구독 가능한 리소스(Subscribable Resources) 는 기반 데이터가 변경될 때마다 클라이언트에게 알림을 보내 재처리를 유발할 수 있으며, 샘플링된 리소스(Sampled Resources) 는 대규모 데이터 소스에서 요약 정보를 추출하는 데 사용될 수 있습니다. 특히 샘플링(Sampling) 기능은 서버가 클라이언트 측의 LLM에게 특정 프롬프트에 대한 응답 생성을 요청할 수 있게 하여, 단순한 요청-응답 패턴을 넘어서는 진정한 의미의 양방향 대화를 가능하게 합니다. 이는 MCP를 다른 프로토콜과 차별화하는 핵심적인 특징입니다.\n다른 기술과의 관계 MCP는 기존 기술 스택을 완전히 대체하기보다는 보완하고 확장하는 역할에 가깝습니다. 다른 기술과 비교하여, 그 독자적인 가치와 기술 생태계 내에서의 전략적 위치를 명확히 규명합니다.\nREST/GraphQL: 상태, 발견, 컨텍스트의 패러다임 전환 MCP와 REST/GraphQL 같은 전통적 API는 근본적으로 다른 목적을 위해 설계되었습니다. 이 둘의 차이는 상태 관리, 서비스 발견, 그리고 컨텍스트 처리 방식에서 패러다임의 전환을 보여줍니다.\n목적의 차이: 전통적인 API는 주로 애플리케이션 간의 통신을 위해 설계되었으며, 애플리케이션이 처리할 구조화된 데이터(data) 를 반환합니다. 반면, MCP는 AI, 특히 LLM을 위해 제작되었습니다. MCP 서버는 LLM이 추론하는 데 필요한 컨텍스트(context) 를 제공하는 것을 목표로 합니다. 예를 들어, REST API가 10KB 크기의 방대한 JSON 객체를 반환한다면, 잘 설계된 MCP 서버는 LLM의 컨텍스트 창에 최적화된 간결하고 작업 관련성이 높은 요약 정보를 제공할 것입니다. 발견(Discovery) 방식: REST나 GraphQL API는 정적입니다. 개발자는 OpenAPI 명세서와 같은 문서를 사전에 읽고 학습해야만 해당 API를 사용할 수 있습니다. 이와 대조적으로, MCP는 동적인 런타임 발견을 지원합니다. MCP 클라이언트는 서버에 tools/list와 같은 요청을 보내 사전 지식 없이도 해당 서버가 제공하는 기능 목록과 사용법을 실시간으로 파악할 수 있습니다. 이는 AI 에이전트가 보다 자율적이고 적응적으로 동작할 수 있게 하는 핵심 기능입니다. 통신 패턴: REST는 각 요청이 독립적으로 처리되는 상태 비저장(stateless) 방식입니다. MCP는 세션 기반의 양방향(bidirectional) 통신 모델을 채택하여 여러 상호작용에 걸쳐 컨텍스트를 유지합니다. 이는 여러 단계로 구성된 복잡한 에이전트 워크플로우를 수행하는 데 필수적입니다. 상호 관계: MCP는 REST를 대체하는 기술이 아니라는 것입니다. 오히려 MCP는 기존 API 위에 구축되는 보완적인 추상화 계층입니다. 실제로 많은 MCP 서버는 기존 REST API를 AI 친화적으로 감싸는 래퍼(wrapper) 역할을 합니다. 예를 들어, GitHub MCP 서버는 내부적으로 GitHub의 REST API를 호출하여 MCP의 표준화된 형식으로 변환한 후 클라이언트에게 제공합니다. 특징/원칙 모델 컨텍스트 프로토콜 (MCP) 전통적인 REST API 표준화 개방형, 범용 표준 표준 부재 (각 API가 고유) 발견 메커니즘 동적, 런타임에 질의 가능 정적, 문서를 통해 파악 주요 목적 AI 모델과 외부 시스템 간의 맥락 교환 범용 소프트웨어 간 통신 생태계 모델 개방형, 커뮤니티 주도, 상호 운용 가능 단편화된, 개별적 통합 맥락의 풍부함 도구, 리소스, 프롬프트를 통한 의도 전달 주로 데이터 검색/조작에 초점 gRPC: 성능, 스키마, 그리고 LLM 친화성에 대한 고찰 gRPC는 고성능 마이크로서비스 통신을 위해 설계된 반면, MCP는 LLM과의 상호작용에 최적화되어 있습니다. 이 둘의 비교는 성능과 LLM 해석 가능성 사이의 중요한 트레이드오프를 보여줍니다.\n성능 vs. 해석 가능성: gRPC는 고효율의 바이너리 직렬화 포맷인 프로토콜 버퍼(Protocol Buffers)와 HTTP/2를 사용하여 기계 간 통신 성능을 극대화합니다. 반면 MCP는 LLM의 해석 가능성을 우선시합니다. 이를 위해 사람이 쉽게 읽고 이해할 수 있는 JSON 형식을 사용하며, 스키마 내에 자연어 설명과 사용 지침을 포함시킵니다. 이 덕분에 LLM은 별도의 변환 계층 없이도 도구의 목적과 매개변수를 훨씬 쉽게 이해할 수 있습니다. 스키마 및 계약: 두 기술 모두 강력한 타입의 스키마를 사용합니다. gRPC는 프로토콜 버퍼를 통해 엄격한 서비스 계약을 정의합니다. MCP 역시 JSON-RPC 2.0 기반의 정의된 스키마를 사용하지만, 여기에 자연어 프롬프트와 설명이라는 추가적인 의미 계층을 더합니다. OpenAI 함수 호출: \u0026lsquo;인프라\u0026rsquo;와 \u0026lsquo;의도\u0026rsquo;의 상호보완적 관계 MCP와 OpenAI의 함수 호출(Function Calling)은 경쟁 관계가 아니라, 서로 다른 역할을 수행하는 상호보완적인 관계입니다.\n핵심적인 차이: 함수 호출은 \u0026lsquo;의도(intent)\u0026rsquo; 에 해당합니다. LLM이 특정 작업을 수행해야 할 필요성을 인식하고, 필요한 매개변수를 구조화된 형식으로 출력하는 메커니즘입니다. 반면, MCP는 \u0026lsquo;인프라(infrastructure)\u0026rsquo; 입니다. MCP는 LLM이 표현한 의도를 받아, 실제 작업을 발견하고, 실행하며, 관리하는 표준화된 프로토콜입니다. 표준화 문제: 함수 호출의 형식은 각 LLM 제공사(OpenAI, Anthropic, Google 등)마다 다릅니다. MCP를 사용하면 각 도구는 단 하나의 MCP 서버만 구현하면 되고, MCP와 호환되는 모든 모델이 이를 사용할 수 있습니다. 아키텍처: 함수 호출에 사용되는 도구는 일반적으로 단일 애플리케이션 내에 하드코딩되어 정의됩니다. MCP는 도구가 그것을 소비하는 애플리케이션과 분리된, 재사용 가능한 별도의 서버에 존재하는 분산 아키텍처를 장려합니다. 시너지: 이 둘은 함께 작동합니다. LLM은 자신의 고유한 함수 호출 기능을 사용하여 요청을 생성합니다. 그러면 MCP 클라이언트가 이 요청을 받아 표준화된 프로토콜을 통해 적절한 MCP 서버로 전송하여 실행합니다. 이러한 비교 분석을 통해 MCP의 핵심 가치 제안이 특정 기술적 축에서의 우월성이 아니라, 표준화를 통한 통합 마찰의 감소에 있음을 알 수 있습니다. gRPC보다 빠르거나 단일 REST 호출보다 단순하지는 않지만, M×N 문제를 해결함으로써 전체 생태계의 개발 효율성을 향상시킵니다. MCP는 기존 기술 스택을 대체하는 것이 아니라, 그 위에 새로운 가치를 창출하는 추상화 계층으로 위치합니다. 이는 MCP와 기존 기술 사이에서 양자택일할 필요 없이, MCP를 새로운 계층으로 활용하여 시스템을 강화하는 전략을 고려해야 함을 의미합니다.\nMCP의 방향 변경 사항 초기 안정화 (2024-11-05): 최초 공개된 버전은 MCP의 핵심 개념을 정립하는 데 중점을 두었습니다. JSON-RPC 2.0 기반의 메시지 형식과 함께, 프로토콜의 핵심 프리미티브(primitive)인 tool(도구), resource(리소스), prompt(프롬프트)가 정의되었습니다. 이 시점에서는 스트리밍 통신을 위한 전송 방식으로 HTTP와 서버-전송 이벤트(Server-Sent Events, SSE)를 채택했는데, 이는 실시간 대화형 사용 사례에 초기 초점을 맞추었음을 시사합니다. 엔터프라이즈 및 보안 강화 (2025-03-26): 이 버전은 MCP가 본격적으로 엔터프라이즈 시장을 겨냥하기 시작했음을 알리는 중요한 전환점입니다. 가장 주목할 만한 변화는 OAuth 2.1 기반의 포괄적인 인증 프레임워크 도입입니다.1 이는 기업 환경에서 필수적인 강력한 보안 및 신원 관리 요구사항을 충족시키기 위한 결정이었습니다. 또한, 기존 SSE 방식이 가진 기업 방화벽 및 프록시 환경에서의 호환성 문제를 해결하기 위해, 보다 유연하고 견고한 스트리밍 가능한 HTTP(Streamable HTTP) 전송 방식으로 대체되었습니다. 더불어, 도구의 속성을 명시하는 tool annotations(예: 읽기 전용, 파괴적 행위) 기능과 오디오 콘텐츠 지원이 추가되어, 더욱 풍부하고 제어된 상호작용이 가능해졌습니다. 성능 최적화를 위해 JSON-RPC 일괄 처리(batching) 기능이 잠시 도입되었다가 이후 버전에서 철회되었는데, 이는 프로토콜의 효율성과 단순성 사이에서 균형점을 찾으려는 시도가 있었음을 보여줍니다. 정제 및 보안 강화 (2025-06-18): 가장 최신 릴리스는 프로토콜의 완성도를 높이고 보안을 한층 더 강화하는 데 집중했습니다. 일괄 처리 기능을 제거하여 프로토콜의 복잡성을 낮추고 구현을 단순화했습니다. 핵심적인 추가 사항은 예측 가능한 통합을 위한 구조화된 도구 출력(structured tool output)과, MCP 서버를 OAuth 리소스 서버로 분류하고 토큰 오용을 방지하기 위해 리소스 표시자(Resource Indicators, RFC 8707) 사용을 의무화한 것입니다. 이는 \u0026lsquo;혼란된 대리인(confused deputy)\u0026rsquo; 문제와 같은 실제적인 보안 위협에 대한 깊은 이해를 바탕으로 한 조치입니다. 또한, 서버가 사용자에게 요청을 시작할 수 있는 유도(elicitation) 기능이 추가되어 프로토콜의 양방향 상호작용성이 더욱 강화되었습니다. 이러한 프로토콜의 발전 과정은 우연이 아닙니다. 초기에는 개발자 커뮤니티의 지지를 확보하는 데 주력하고, 이후에는 기업 고객의 엄격한 보안 및 운영 요구사항을 충족시키는 방향으로 명확하게 전환하는 전략적 움직임을 보여줍니다. 이는 MCP가 단기적인 유행이 아닌, 장기적으로 지속 가능한 표준으로 자리매김하려는 의도를 명백히 드러냅니다.\n버전 (릴리스 날짜) 주요 기능 및 향상점 주요 변경 사항 (Breaking Changes) 전략적 중요성 2024-11-05 • 핵심 아키텍처 및 메시지 형식 정의 (JSON-RPC 2.0) • tool, resource, prompt 프리미티브 도입 • HTTP + SSE 기반 스트리밍 전송 해당 없음 (초기 버전) 개발자 중심의 초기 생태계 구축 및 핵심 개념 정립. 대화형 AI 에이전트의 기본 기능 지원에 초점. 2025-03-26 • OAuth 2.1 기반 인증 프레임워크 추가 • tool annotations (읽기 전용, 파괴적 등) 도입 • 오디오 콘텐츠 지원 추가 • SSE를 Streamable HTTP로 대체 엔터프라이즈 도입의 핵심 장벽인 보안 및 네트워크 호환성 문제 해결. 프로토콜의 적용 범위를 기업 환경으로 확장. 2025-06-18 • structured tool output 지원 • MCP 서버를 OAuth 리소스 서버로 분류 • Resource Indicators (RFC 8707) 요구 • 서버 주도 요청을 위한 elicitation 기능 추가 • JSON-RPC 일괄 처리 기능 제거 \u0026lsquo;혼란된 대리인\u0026rsquo; 등 정교한 보안 위협에 대응하여 프로토콜을 강화. 프로토콜의 단순성과 보안성을 동시에 향상. 현재 보고된 문제점 하지만 앞의 발전 외에도, 사용자들은 MCP에 대해 다음의 문제들을 보고하고 있습니다.\n멀티테넌시(Multi-Tenancy): 많은 사용자가 공유 서버에 안전하게 접근해야 하는 멀티테넌트 SaaS 아키텍처를 위해 설계되지 않았습니다. 이는 많은 엔터프라이즈 사용 사례에 주요한 장애물입니다. 디버깅 및 관찰 가능성: 개발자들은 MCP 통합을 디버깅하는 것이 매우 어렵다고 보고합니다. 클라이언트 측 추적이 종종 누락되거나 접근하기 어렵고, 각 클라이언트마다 고유한 특성이 있기 때문입니다. 발견 및 신뢰: 서버를 발견할 수는 있지만, 신뢰성을 검증할 중앙화된 신뢰할 수 있는 레지스트리가 없습니다. 이는 에이전트가 신뢰할 수 없거나 악의적인 서버에 연결될 위험을 초래합니다. 워크플로우 오케스트레이션: MCP에는 복잡한 다단계 워크플로우를 관리하기 위한 기능이 부족하여, 클라이언트의 재시도나 재개 가능성과 같은 로직을 직접 구현해야 합니다. 영역 구체적인 과제 잠재적 미래 방향 / 연구 분야 거버넌스 내장된 세분화된 권한 모델 없음 사양에 권한 모델 도입, 제3자 PAM 솔루션과의 표준 통합 운영 확장성 멀티테넌시 지원 부족 멀티테넌트 서버 아키텍처를 위한 패턴 정의 생태계 건전성 신뢰할 수 있는 서버 레지스트리 부재 커뮤니티가 관리하는 검증된 레지스트리 구축 개발자 경험 디버깅 및 워크플로우 관리의 복잡성 표준화된 디버깅 도구 및 워크플로우 오케스트레이션 프리미티브 도입 MCP에서는 이러한 문제점을 어떻게 개선하고, 향후 어떻게 나아갈 것인지 살펴보겠습니다.\n2025년 7월 공식 로드맵 분석: 에이전트, 보안, 멀티모달리티 에이전트 기능 강화: 로드맵의 최우선 과제 중 하나는 더 복잡한 에이전트 워크플로우를 지원하는 것입니다. 특히 수 분에서 수 시간에 이르는 작업을 처리할 수 있는 비동기 작업(asynchronous operations) 지원이 핵심입니다. 이는 단순한 질의응답을 넘어 장기적인 목표를 수행하는 자율 에이전트 구현에 필수적인 기능입니다. 인증 및 보안 고도화: 엔터프라이즈 도입을 위해 보안은 여전히 가장 중요한 영역입니다. 로드맵에는 세분화된 권한 부여(fine-grained authorization), 사용자 경험을 해치지 않으면서 보안을 강화하기 위한 동적 클라이언트 등록(DCR)의 대안 탐색, 그리고 SSO(Single Sign-On)를 통한 엔터프라이즈 관리형 인증(enterprise-managed authorization) 기능 추가 계획이 포함되어 있습니다. 검증 및 레지스트리: 생태계의 신뢰성과 확장성을 위해, 일관된 구현을 보장하는 준수 테스트 스위트(compliance test suites) 와 중앙에서 서버를 발견할 수 있는 MCP 레지스트리(MCP Registry) 개발이 계획되어 있습니다. 특히 레지스트리는 앤스로픽이 직접 운영하는 앱스토어 형태가 아니라, 서드파티 마켓플레이스가 그 위에 구축될 수 있는 API 계층으로 구상되고 있어 개방형 생태계를 지향함을 보여줍니다. 멀티모달리티 확장: 현재의 텍스트와 이미지를 넘어, 비디오와 같은 추가적인 데이터 양식(modality)을 지원하고, 대화형 경험을 위한 스트리밍 기능을 개선하여 AI의 전체 스펙트럼을 지원하는 것을 목표로 하고 있습니다. 커뮤니티 논의와 기술적 과제: A2A, gRPC 통합, 게이트웨이의 필요성 공식 로드맵 외에도, 커뮤니티에서는 MCP의 미래를 형성할 중요한 기술적 논의가 활발하게 이루어지고 있습니다.\nMCP와 A2A (Agent-to-Agent Protocol): 커뮤니티의 핵심 논의 중 하나는 MCP와 구글의 A2A 프로토콜 간의 관계입니다. 현재 지배적인 견해는 이 둘이 경쟁 관계가 아닌 상호 보완적이라는 것입니다. MCP가 에이전트와 도구(agent-to-tool) 간의 통신을 표준화하는 반면, A2A는 에이전트와 에이전트(agent-to-agent) 간의 협업을 표준화하는 것을 목표로 합니다. 미래에는 A2A 프로토콜 기반의 에이전트가 자신의 도구를 사용하기 위해 내부적으로 MCP를 활용하는 구조가 될 수 있습니다. gRPC 통합 가능성: 커뮤니티에서는 기존의 JSON/HTTP 전송 방식 대신, gRPC의 높은 성능과 성숙한 생태계를 활용하자는 주장이 꾸준히 제기되고 있습니다. 이는 MCP가 향후 다양한 전송 계층을 지원하는 방향으로 발전할 수 있음을 시사합니다. 게이트웨이의 필연성: 프로덕션 환경, 특히 다중 테넌트나 기업 환경에서 보안, 관찰 가능성, 트래픽 관리를 위한 게이트웨이 계층의 필요성은 전문가들 사이에서 거의 공통된 의견입니다. 게이트웨이는 MCP 핵심 명세에 포함되어 있지는 않지만, 사실상 안전한 배포를 위한 필수 구성 요소로 인식되고 있습니다. MCP 생태계의 확장: 주요 플레이어, 시장, 그리고 인프라 주요 플레이어의 참여: MCP의 가장 큰 성공 요인 중 하나는 주요 기업들의 지지입니다. Anthropic이 시작했지만, 경쟁사인 OpenAI, Google DeepMind, Microsoft가 이를 채택했습니다. Microsoft는 Copilot Studio에 MCP를 통합하고 C# SDK를 공동으로 유지 관리하고 있으며 , GitHub는 VS Code 내 MCP 지원을 정식 버전으로 출시했습니다. 이러한 \u0026lsquo;경쟁적 협력(coopetition)\u0026rsquo; 구도는 MCP가 단일 벤더에 종속되지 않는 사실상의 업계 표준으로 자리 잡을 가능성을 높여줍니다. 이는 특정 기업의 향방과 관계 없이 프로토콜의 장기적 안정성을 보장하므로, 다른 기업들의 채택 리스크를 크게 낮추는 효과가 있습니다. SDK 및 서버 생태계: TypeScript, Python, Java, C#, Go, Rust, Ruby 등 주요 언어에 대한 공식 SDK가 제공되고 있으며, 이는 종종 Microsoft(C#), Google(Go)과 같은 파트너사와의 협력을 통해 개발됩니다. GitHub, Slack, 데이터베이스, Puppeteer 등 널리 사용되는 도구들을 위한 오픈소스 서버 저장소도 방대하게 구축되어 있습니다. 신흥 마켓플레이스와 인프라: MCP를 중심으로 한 상업 생태계도 형성되고 있습니다. mcpmarket.com과 같이 사전 구축된 서버를 발견하고 사용할 수 있는 마켓플레이스와, MCP 서버의 구축 및 호스팅을 단순화하는 인프라 도구들이 등장하고 있습니다. 장기적 비전: 자율 에이전트와 AI 네이티브 웹의 기반으로서의 MCP MCP에 대한 장기적인 비전은 단순한 도구 통합을 훨씬 뛰어넘습니다. 이는 차세대 컴퓨팅을 위한 기반 프로토콜로 자리매김하는 것입니다.\n자율 시스템의 기반: MCP는 인간의 직접적인 개입 없이 기업의 리소스나 웹 서비스를 동적으로 발견하고, 학습하며, 상호작용할 수 있는 자율 에이전트 시스템에 필요한 핵심 패턴을 제공합니다. 에이전트 상거래와 AI 네이티브 웹: 전문가들은 MCP가 \u0026lsquo;AI 네이티브 웹(AI-Native Web)\u0026rsquo; 을 가능하게 할 것이라고 전망합니다. 과거 HTTP와 웹 브라우저가 인간이 웹 페이지와 상호작용하는 방식을 정의했다면, 미래에는 MCP가 AI 에이전트가 우리를 대신하여 웹에서 행동하는 방식을 정의할 수 있습니다. 이는 AI가 여러 서비스를 오가며 복잡한 구매 절차를 완료하는 \u0026lsquo;에이전트 상거래(agentic commerce)\u0026lsquo;와 같은 새로운 개념으로 이어질 수 있습니다. MCP의 궁극적인 영향은 단순히 기존 시스템을 AI에 연결하는 것을 넘어, 처음부터 \u0026lsquo;AI가 이해할 수 있는(AI-comprehensible)\u0026rsquo; 시스템의 설계를 촉진하는 것입니다. 현재 대부분의 MCP 서버는 AI를 위해 설계되지 않은 기존 API의 래퍼에 불과합니다. 하지만 미래에는 애플리케이션과 서비스가 설계 단계부터 MCP 인터페이스를 기본으로 고려하게 될 수 있으며, 이는 자율 AI 에이전트와의 상호작용을 전제로 하는 애플리케이션 설계 철학의 근본적인 변화입니다.\n최종 목표: 인지 운영 체제 MCP의 비전은 단순한 커넥터 이상으로 위치하는 것입니다. 이는 에이전트의 의도를 실행과 연결하는 \u0026ldquo;신경 계약 계층(neural contract layer)\u0026rdquo; 또는 \u0026ldquo;인지 운영 체제(cognitive OS)\u0026ldquo;로 표현합니다.\n여기서 MCP는 다중 에이전트 시스템 전반에서 컨텍스트, 목표, 제약 조건에 대한 공유된 이해를 강제합니다. 이는 \u0026ldquo;의미론적 기억 저장소(semantic memory keeper)\u0026rdquo; 역할을 하여, 에이전트가 원래 임무에 맞게 행동하도록 보장합니다. 이 패러다임은 동적 다중 에이전트 거버넌스, 버전화된 컨텍스트 계보를 가진 \u0026ldquo;에이전트 DNA\u0026rdquo;, 그리고 에이전트 추론 디버깅을 위한 인지적 컨텍스트 비교(cognitive context diffing)와 같은 목표를 통해, 무결성을 잃지 않으면서 지능을 확장합니다.\n마치며 MCP는 AI 개발의 초점을 \u0026ldquo;모델 중심\u0026rdquo;(더 나은 모델 구축)에서 \u0026ldquo;맥락 중심\u0026rdquo;(모델 주변 데이터 및 도구 생태계 엔지니어링)으로 이동시켰습니다. 모델이 상호작용하는 환경을 설계하여 AI 시스템의 능력을 향상시킬 수 있다는 새로운 관점을 제시합니다.\n목적을 달성하려면 모델 자체의 성능도 큰 영향을 주지만, 필요한 데이터를 적절하게 전달하거나 원하는 동작을 수행할 수 있는 도구 등 생태계가 갖춰져야 합니다. 이러한 환경을 구현할 표준이 있다면 생산성, 재사용성 등 개발 효율성에서 큰 이점을 얻을 수 있습니다. MCP를 Anthropic, OpenAI, Microsoft, Google DeepMind 등 주요 기업들이 수용했다는 점에서 MCP 기술 자체는 신뢰하고 사용할 수 있다고 생각합니다.\n이번 글을 위해 자료를 찾으면서, \u0026ldquo;대부분의 MCP 서버는 AI를 위해 설계되지 않은 기존 API의 래퍼\u0026quot;라 언급된 것처럼 저 또한 이전까지는 MCP를 단순 요청 응답 형태로 사용하고 있었습니다. 에이전트와 MCP 서버를 \u0026ldquo;대화형 상호작용 모델\u0026quot;로 사용하는 것이 MCP의 철학인 만큼, 저도 올바르게 사용할 수 있도록 더 자세히 알아보려 합니다. 앞으로의 포스트에서는 MCP의 보다 구체적인 내용과, 고려사항에 대해 다루겠습니다.\n본 포스트는 Google Gemini의 응답을 기반으로 저의 의견을 반영하여 다시 작성했습니다.\n","date":"2025-08-11T19:03:00Z","permalink":"https://yeonhl.github.io/systems/mcp/concept/","title":"MCP의 개념"},{"content":"개요 에이전트를 단순한 프로그램이나 챗봇과 구분할 수 있는 기준은 무엇일까요? 에이전트를 개발하면서 이를 고민하고, 본질을 잃지 않고 올바른 시스템을 만들기 위해 고민했습니다. 에이전트 (Agents) 페이지에서는 에이전트의 개념과 특징, 발전 과정을 다룹니다.\n이 포스트에서는 에이전트가 다른 시스템과 구분되는 특징과 속성을 알아보겠습니다.\n에이전트의 속성 에이전트는 자율성 (Autonomy), 반응성 (Reactivity), 주도성 (Pro-activeness), 사회성, (Sociality), 학습 능력 (Learning) 의 속성을 갖습니다.\n자율성 (Autonomy) 자율성은 사전에 프로그래밍된 작업을 수행하는 것을 넘어, 지속적인 인간의 감독이나 직접적인 개입 없이 독립적으로 상황을 판단하고, 의사결정을 내리며, 행동을 취할 수 있는 능력을 의미합니다. 에이전트의 가장 근본적이고 결정적인 특징으로 에이전트를 수동적인 도구에서 능동적인 행위자로 만듭니다.\n예를 들어, 자율 주행 자동차는 복잡한 도심 환경에서 수많은 센서로부터 쏟아지는 데이터를 실시간으로 처리하여 차선 변경, 속도 조절, 돌발 상황 회피 등 수많은 결정을 독립적으로 수행합니다. 마찬가지로, 현대적인 물류 창고의 로봇은 재고의 위치를 파악하고, 최적의 이동 경로를 스스로 계산하며, 다른 로봇과의 충돌을 피해 작업을 수행합니다. 이들은 단순히 명령을 따르는 기계가 아니라, 주어진 목표(예: 안전한 주행, 효율적인 물류 처리)를 달성하기 위해 스스로 판단하고 행동합니다.\n반응성 (Reactivity) 반응성은 에이전트가 센서를 통해 주변 환경의 변화를 인식하고, 그 변화에 실시간으로 적절하게 대응하는 능력입니다.\n예를 들어, 자율 주행 자동차는 갑자기 끼어드는 차량이나 도로에 나타난 장애물을 감지하고 즉시 감속하거나 회피합니다. 또한, 스마트 온도 조절기는 실내 온도의 변화나 사람의 유무를 감지하여 난방이나 냉방을 조절하며, 고객 서비스 챗봇은 사용자의 새로운 질문에 즉각적으로 응답하여 문제를 해결합니다. 이러한 실시간 적응 능력으로 에이전트는 정적인 프로그램을 넘어 동적인 환경의 일부로 동작합니다.\n주도성 (Pro-activeness) 주도성은 현재 상황을 분석하고 미래를 예측하여, 문제가 발생하거나 필요가 명시적으로 요구되기 전에 선제적으로 행동을 개시하는 능력입니다. 에이전트는 단순히 외부 자극에 반응하는 것을 넘어, 목표 지향적인 행동을 주도적으로 수행합니다. 앞의 자율성과 주도성을 통해 에이전트가 능동적 주체로 행동할 수 있습니다.\n예를 들어, AI 개인 비서는 사용자가 묻기 전에 다가올 회의 일정을 미리 알려주거나, 평소 출퇴근 패턴과 실시간 교통 정보를 분석하여 최적의 경로를 먼저 제안할 수 있습니다. AI 기반 금융 자문가는 현재 포트폴리오를 관리하는 데 그치지 않고, 시장 동향을 예측하여 위험을 줄이고 수익을 극대화하기 위한 포트폴리오 조정을 선제적으로 제안합니다. 이처럼 주도성은 에이전트가 사용자의 목표를 이해하고 목표 달성을 위해 적극적으로 행동하게 만드는 능력입니다.\n사회성 및 학습 (Sociality \u0026amp; Learning) 에이전트는 다른 에이전트나 사람과 소통하고 협력하는 사회적 능력을 가질 수 있습니다. 또한 경험을 통해 스스로의 성능을 개선하는 학습 능력도 가질 수 있습니다.\n사회성은 다중 에이전트 시스템(Multi-Agent Systems)에서 특히 중요합니다. 예를 들어, 물류 창고의 여러 로봇들은 서로 정보를 교환하며 작업 순서를 최적화하고, 교통 관제 시스템의 에이전트들은 서로 통신하며 전체적인 교통 흐름을 개선합니다. 이러한 협업 능력은 개별 에이전트의 능력을 합한 것 이상의 시너지를 창출합니다. 이 사회성의 기반에는 자연어 처리(NLP) 기술이 있으며, 특히 대규모 언어 모델(LLM)의 등장으로 에이전트가 인간과 훨씬 더 자연스럽고 정교하게 상호작용하고 있습니다.\n학습 능력은 에이전트가 시간이 지날수록 더 많은 역할을 수행할 수 있게 합니다. 자율 주행 자동차는 수백만 마일의 주행 데이터를 학습하며 운전 실력을 향상시키고, 콘텐츠 추천 시스템은 사용자의 피드백을 학습하여 더 정확한 추천을 제공합니다. 이처럼 학습 능력은 에이전트를 정적인 시스템에서 나아가, 환경과 상호작용하며 진화하는 동적인 존재로 만듭니다.\nAI 시스템의 발전 AI 에이전트는 어떤 발전 과정을 거쳤을까요? Google 등 업계에서는 다음과 같은 기준으로 분류합니다.\n봇 (Bots) 봇은 주로 자동화된 대화나 단순 반복 작업을 위해 설계됩니다. 봇의 핵심 특징은 반응성과 규칙 기반 작동입니다. 이들은 사전에 정의된 규칙이나 특정 트리거에 따라 반응하며, 학습 능력은 거의 없거나 매우 제한적입니다. 초기의 챗봇인 엘리자(ELIZA)나 간단한 키워드 기반의 스팸 필터, 웹사이트의 자동 응답 시스템 등이 대표적인 예시입니다. 봇은 독립적인 의사결정 능력을 갖추지 못하고 사용자의 명시적인 명령이나 특정 조건이 충족될 때만 작동하는 수동적인 도구에 가깝습니다.\nAI 어시스턴트 (AI Assistants) AI 어시스턴트는 봇보다 한 단계 진화한 형태로, 사용자의 작업을 보조하는 역할을 수행합니다. 애플의 시리(Siri)나 구글 어시스턴트, 아마존 알렉사(Alexa) 등이 여기에 해당합니다. 봇보다 향상된 자연어 처리 능력과 약간의 학습 능력을 갖추고 있어, 사용자의 요청이나 질문에 더 유연하게 반응하고 정보를 제공하거나 간단한 작업을 수행할 수 있습니다.\nAI 어시스턴트의 핵심적인 특징은 사용자와의 상호작용을 통한 보조입니다. 이들은 작업의 여러 단계에 걸쳐 사용자와 소통하며, 특정 행동을 추천할 수는 있지만 최종적인 의사결정 권한은 항상 사용자에게 있습니다. 즉, AI 어시스턴트는 여전히 사용자의 지시를 기다리는 반응적인 존재이며, 자율성의 정도가 제한적입니다.\nAI 에이전트 (AI Agents) AI 에이전트는 어시스턴트에서 더 나아가, 단순히 사용자를 보조하는 것을 넘어, 사용자를 대신하여 자율적이고 주도적으로 목표를 추구하고 과업을 완수합니다. 복잡하고 여러 단계로 이루어진 작업을 독립적으로 계획하고 실행할 수 있으며, 지속적으로 자신의 성능을 개선합니다. 자율 주행 자동차, 정교한 금융 거래 에이전트, 공급망을 최적화하는 시스템, 그리고 최근 등장한 OpenAI의 연구 에이전트나 구글의 프로젝트 아스트라와 같은 다중 모드 에이전트가 예시입니다. 단순한 도구나 보조자가 아닌, 특정 목표를 부여받고 그 목표를 달성하기 위해 스스로 세계와 상호작용합니다. 이러한 자율성과 주도성의 차이는 AI 에이전트를 이전 세대의 AI와 구별 짓는 가장 중요한 차이점입니다.\n특성 봇 (Bot) AI 어시스턴트 (AI Assistant) AI 에이전트 (AI Agent) 목적 단순 작업 또는 대화 자동화 사용자의 과업 보조 자율적, 주도적 과업 수행 핵심 능력 사전 정의된 규칙 준수, 기본적 상호작용 요청/프롬프트에 응답, 정보 제공, 간단한 과업 완료, 행동 추천 복잡한 다단계 행동 수행, 독립적 의사결정, 학습 및 적응 상호작용 방식 반응적 (트리거 또는 명령어에 응답) 반응적 (사용자 요청에 응답) 주도적 (목표 지향적) 자율성 수준 가장 낮음 (프로그래밍된 규칙 엄수) 중간 (사용자 지시 및 확인 필요) 가장 높음 (목표 달성을 위한 독립적 운영 및 의사결정) 학습 능력 제한적이거나 없음 일부 학습 능력 보유 지속적인 학습 및 성능 개선 AI 에이전트 vs. 챗봇 \u0026amp; AI 어시스턴트 챗봇과 AI 어시스턴트는 AI 에이전트와 흔하게 혼동되는 개념이지만, 자율성과 복잡성 측면에서 차이가 있습니다.\n자율성: 챗봇과 AI 어시스턴트는 \u0026lsquo;수동적(reactive)\u0026lsquo;이며, 사용자의 질문이나 명령(prompt)에 응답하는 방식으로 동작합니다. 행동을 위해 지속적인 사용자 입력이 필요합니다. 반면, AI 에이전트는 \u0026lsquo;능동적(proactive)\u0026lsquo;이고 목표 지향적입니다. 목표가 주어지면 사람의 개입 없이도 스스로 계획을 세우고 작업을 수행합니다. 복잡성: 챗봇과 AI 어시스턴트는 FAQ 답변, 정보 제공, 단일 작업 수행 등 비교적 단순하고 정형화된 상호작용을 처리하는 데 적합합니다. 반면, AI 에이전트는 여러 시스템과 연동하고, 여러 단계를 거쳐야 하는 복잡한 워크플로우도 처리할 수 있습니다. AI 에이전트 vs. 로보틱 프로세스 자동화(RPA) RPA와 AI 에이전트는 모두 비즈니스 프로세스를 자동화하지만, 그 방식과 지능 수준에 차이가 있습니다.\n핵심 기능: RPA는 인간의 행동을 모방하여 정해진 \u0026lsquo;절차(procedure)\u0026lsquo;를 자동화합니다. 예를 들어, 특정 폴더에서 엑셀 파일을 열어 데이터를 복사한 후, 다른 시스템의 특정 필드에 붙여넣는 것과 같은 반복적이고 규칙 기반의 작업을 수행합니다. 반면, AI 에이전트는 특정 \u0026lsquo;결과(outcome)\u0026lsquo;를 달성하기 위해 자율적으로 행동합니다. \u0026ldquo;이 고객의 환불 요청을 처리하라\u0026quot;는 목표가 주어지면, 에이전트는 스스로 필요한 시스템에 접근하고, 정책을 확인하며, 고객과 소통하여 문제를 해결합니다. 데이터 처리: RPA는 스프레드시트나 양식과 같은 \u0026lsquo;정형 데이터(structured data)\u0026lsquo;를 처리하는 데 최적화되어 있으며, 애플리케이션의 사용자 인터페이스(UI)가 변경되면 쉽게 오류가 발생합니다. 반면, AI 에이전트는 자연어 처리(NLP) 기술을 통해 이메일, 채팅 기록, 문서 등 \u0026lsquo;비정형 데이터(unstructured data)\u0026lsquo;를 이해하고 처리할 수 있으며, 환경 변화에 더 잘 적응합니다. 지능과 학습: 전통적인 RPA 봇은 학습 능력이 없으며, 프로그래밍된 규칙을 기계적으로 수행합니다. 반면, AI 에이전트는 경험으로부터 학습하고, 추론하며, 복잡한 의사결정을 내릴 수 있습니다. 발전 방향 기술이 발전할수록 시스템은 점차 인간의 개입을 덜 필요로 하고, 수동적인 반응에서 능동적인 형태로 나아갑니다. 이는 어시스턴트에서 에이전트로 넘어가는 지점에서 큰 차이가 나타납니다.\n최근 이러한 발전이 가능한 것은 대규모 언어 모델(LLM)의 등장 덕분입니다. LLM은 추론, 계획, 자연어 상호작용 능력에서 탁월한 성능을 보이며, 현대 AI 에이전트의 \u0026lsquo;두뇌\u0026rsquo; 또는 \u0026lsquo;제어기\u0026rsquo; 역할을 수행하기에 이상적인 기반을 제공합니다. LLM의 생성 능력은 자율성을, 다중 모드 통합은 반응성을, 예측 및 계획 능력은 주도성을, 그리고 자연어 처리 능력은 사회성을 구현하는 데 결정적인 역할을 합니다. 오늘날 AI 에이전트는 LLM이 주도하는 행위성(agency)으로 표현됩니다.\nLLM 이전 시대에 에이전트를 만드는 가장 큰 어려움은 핵심적인 추론 능력과 세계 모델링 기능을 처음부터 구축하는 것이었습니다. 그러나 LLM의 등장으로 강력한 범용 추론 엔진은 누구나 쉽게 활용할 수 있는 일종의 \u0026lsquo;상품(commodity)\u0026lsquo;이 되었고, 이제 현대 에이전트 개발의 다음 핵심 과제는 \u0026lsquo;오케스트레이션(Orchestration)\u0026lsquo;입니다. 이 강력한 인지 엔진을 어떻게 기억(memory), 도구(tools), 그리고 안전하고 신뢰할 수 있는 거버넌스 프레임워크와 효과적으로 결합하여 특정 목표를 안정적으로 달성하게 할지 고민해야 합니다. 경쟁의 핵심은 누가 더 뛰어난 범용 지능을 만드느냐가 아니라, 주어진 지능을 누가 더 정교하게 조율하여 실질적인 가치를 창출하느냐로 옮겨가고 있습니다.\n에이전트의 기원 그렇다면 AI 에이전트가 처음부터 지금과 같은 역할을 수행했을까요? 사실, 에이전트의 개념은 오래 전부터 존재했고, 수십 년간의 연구가 축적된 결과물입니다. 그 변천사를 살펴보겠습니다. 초기 연구자들은 기계가 인간처럼 합리적으로 사고하고 문제를 해결할 수 있는 방법을 모색했습니다.\n기호주의 시대 (1950년대-1980년대): 합리성을 향한 탐구 AI 연구의 초기 단계는 기호주의(Symbolic AI)로 표현할 수 있습니다. 이 시기의 에이전트는 논리적 규칙과 탐색 알고리즘을 기반으로 작동하는 시스템입니다.\n이 시대의 대표적인 예로는 우선 \u0026lsquo;일반 문제 해결사(General Problem Solver, GPS)\u0026rsquo; 프로그램이 있습니다. GPS는 주어진 목표를 달성하기 위해 일련의 연산을 순차적으로 적용하는 방법으로 해답을 찾았습니다.\n또한, 마빈 민스키와 시모어 페퍼트가 제안한 \u0026lsquo;마이크로월드(micro-worlds)\u0026rsquo; 접근법은 복잡한 현실 세계 대신 \u0026lsquo;블록 월드\u0026rsquo;와 같이 단순화된 환경에서 에이전트의 지능을 구현하려는 시도였습니다. 이 환경에서 작동하는 SHRDLU 시스템은 자연어 명령을 이해하고 가상 블록을 조작하며 계획을 수립하는 등 인상적인 능력을 보였습니다.\n이러한 초기 에이전트들은 정해진 절차와 규칙에 따라 자율적으로 작업을 수행하는 \u0026lsquo;절차적 자율성(procedural autonomy)\u0026lsquo;을 가집니다. 목표 달성을 위한 논리적 스크립트를 처음부터 끝까지 수행할 수는 있었지만, 그 스크립트를 벗어나는 창의적인 행동은 불가능했습니다.\n현실 세계의 문제는 탐색해야 할 경로의 수가 기하급수적으로 증가하는 \u0026lsquo;조합적 폭발(combinatorial explosion)\u0026rsquo; 문제를 야기했으며, 미리 정의된 규칙은 현실 세계의 모호함과 예측 불가능성에 대처하기에는 너무 경직되어 있어 AI에 대한 과도한 기대를 충족시키지 못했습니다.\n머신러닝과 다중 에이전트 시스템(MAS) 시대 (1980년대-2000년대): 특화된 학습의 부상 1980년대에 들어서면서 AI 연구는 새로운 국면을 맞이했습니다. 규칙 기반 전문가 시스템(Expert Systems)이 상업적으로 성공을 거두고, 프로그래밍된 논리에서 데이터 기반 학습으로 패러다임이 전환되기 시작했습니다. 이 시기 에이전트의 핵심은 \u0026lsquo;학습\u0026lsquo;입니다.\nDENDRAL이나 MYCIN과 같은 전문가 시스템은 특정 분야(화학 분석, 의료 진단 등)의 지식을 활용하여 전문가 수준의 결정을 내렸습니다. 이는 에이전트가 특정 도메인에서 높은 성능을 발휘할 수 있음을 보였습니다. 동시에, 머신러닝 기술의 발전은 에이전트가 데이터로부터 스스로 패턴을 학습하고 성능을 개선할 수 있는 계기가 됐습니다. 특히 강화학습(Reinforcement Learning)은 에이전트가 환경과의 상호작용을 통해 보상을 최대화하는 행동을 학습하여, 특정 작업에 대한 \u0026lsquo;통계적 자율성(statistical autonomy)\u0026lsquo;을 부여했습니다.\n또한, 여러 에이전트 간의 상호작용을 연구하는 다중 에이전트 시스템(Multi-Agent Systems, MAS) 분야가 발전했습니다. MAS 연구는 분산 AI, 게임 이론, 자율 로봇 등 다양한 분야에 적용되었으며, 여러 전문화된 에이전트들이 어떻게 협력하거나 경쟁하며 공동의 목표를 달성하는지를 탐구했습니다. 이는 오늘날 복잡한 문제를 해결하기 위해 여러 에이전트를 조율하는 현대적 에이전트 워크플로우의 이론적 기반이 되었습니다.\n하지만 이 시대의 에이전트들은 고도로 \u0026lsquo;특화\u0026rsquo;되어 있다는 점이 문제였습니다. 특정 게임에서 인간 챔피언을 이기거나(예: TD-Gammon) 1, 특정 공정을 제어하는 데에는 뛰어났지만, 한 분야에서 학습한 지식을 다른 분야로 일반화하거나 전이하는 능력은 부족했습니다. 각 에이전트는 자신이 훈련된 특정 작업을 위한 \u0026lsquo;장인\u0026rsquo;이 될 수 있지만, 관련된 다양한 문제를 해결하기엔 여전히 부족했습니다.\n트랜스포머 혁명 (2017년-현재) 2017년 구글이 발표한 트랜스포머 아키텍처는 자연어 처리(NLP) 분야에 혁명을 일으켰습니다. 어텐션 메커니즘(Attention Mechanism)을 통해 문장 내 단어 간의 장거리 의존성을 효과적으로 포착함으로써, 기존 모델들의 한계를 뛰어넘는 성능을 보였습니다. 이를 기반으로 OpenAI의 GPT-1(2018)과 구글의 BERT(2018) 같은 모델들이 등장했으며, 이들은 방대한 양의 레이블 없는 텍스트 데이터로 사전 훈련(pre-training)함으로써 전례 없는 일반화 능력을 갖췄습니다.\n이는 큰 변화를 불러왔습니다. 더 이상 특정 작업을 위해 모델을 처음부터 훈련시킬 필요 없이, 하나의 거대한 \u0026lsquo;기반 모델(Foundation Model)\u0026lsquo;을 만들어두고, 소량의 데이터로 미세 조정(fine-tuning)하거나 아예 조정 없이도 다양한 작업을 수행할 수 있게 됐습니다. 또한 이를 에이전트의 \u0026lsquo;두뇌\u0026rsquo; 또는 \u0026lsquo;인지 엔진(cognitive engine)\u0026rsquo; 역할을 하면서 자율적인 에이전트를 구현하려는 시도로 이어졌습니다.\n2023년 등장한 Auto-GPT나 BabyAGI와 같은 프레임워크는 LLM을 핵심 엔진으로 사용하여, 사용자가 제시한 복잡한 목표를 스스로 작은 하위 작업으로 분해하고(task decomposition), 계획을 수립하며(planning), 웹 검색이나 API 호출과 같은 외부 도구(tools)를 사용하여 정보를 수집하고, 최종적으로 목표를 달성하는 과정을 수행했습니다. 이는 이전의 자율성과 다른 \u0026lsquo;인지적 자율성(cognitive autonomy)\u0026lsquo;을 특징으로 합니다. 정해진 규칙이나 학습된 전략을 따르는 것을 넘어, 새로운 목표에 대해 스스로 \u0026lsquo;추론\u0026rsquo;하고 \u0026lsquo;계획\u0026rsquo;하며 \u0026lsquo;적응\u0026rsquo;하는 능력입니다. 이 인지적 자율성이 현대 AI 에이전트와 이전의 에이전트의 핵심적인 차이점이다.\n분석 및 시사점 AI 에이전트의 발전사의 핵심적인 변화는 \u0026lsquo;자율성\u0026rsquo;의 개념의 진화와 그에 따른 엔지니어링의 중심 과제 변화입니다.\n초기 기호주의 시대의 에이전트는 절차적 자율성을 가졌습니다. 실행은 자율적으로 했지만, 생각 과정은 정해진 스크립트를 벗어나지 못했습니다. 이후 머신러닝 시대의 에이전트는 특정 도메인 안에서 \u0026lsquo;전략을 학습하는\u0026rsquo; 통계적 자율성을 획득했습니다. 강화학습으로 훈련된 알파고(AlphaGo)는 바둑이라는 고정된 영역 안에서 최적의 전략을 자율적으로 학습했지만, 바둑 이외의 다른 문제에 능력을 적용할 수 없었습니다.\n그러나 LLM 기반의 현대 에이전트는 새로운 종류의 자율성, 즉 \u0026lsquo;전략 자체를 스스로 고안하는\u0026rsquo; 인지적 자율성을 가집니다. Auto-GPT와 같은 에이전트는 \u0026ldquo;지속 가능한 에너지 스타트업을 위한 사업 계획서 작성\u0026quot;과 같은 한 번도 접해보지 못한 새로운 목표가 주어졌을 때, 이를 달성하기 위한 계획을 스스로 수립하고 필요한 도구를 찾아 사용합니다.\n이전에 에이전트를 만드는 가장 큰 어려움은 핵심적인 추론 능력과 세계 모델링 기능을 처음부터 구축하는 것이었습니다. 그러나 LLM의 등장으로 강력한 범용 추론 엔진은 누구나 쉽게 활용할 수 있는 일종의 \u0026lsquo;상품(commodity)\u0026lsquo;이 되었습니다. 현대 에이전트 개발의 새로운 핵심 과제는 \u0026lsquo;오케스트레이션(Orchestration)\u0026lsquo;으로, LLM을 어떻게 기억(memory), 도구(tools), 거버넌스 프레임워크와 결합하여 특정 목표를 안정적으로 달성하게 할 것인가의 문제입니다. 최근 LangChain, LlamaIndex와 같은 에이전트 프레임워크가 그 예시입니다. 경쟁의 핵심은 누가 더 뛰어난 범용 지능을 만드느냐가 아니라, 누가 주어진 지능을 더 정교하게 조율하여 실질적인 가치를 창출할지 입니다.\n에이전트의 분류 AI 에이전트는 그 지능 수준과 의사결정 방식에 따라 여러 유형으로 분류됩니다. 이 분류는 기능이 점차 정교해지는 능력 계층 구조로 이해할 수 있습니다. 각 유형은 이전 유형의 한계를 극복하기 위해 새로운 구성 요소를 추가하는 방식으로 발전했습니다.\n단순 반사 에이전트 가장 기본적인 형태의 에이전트는 단순 반사 에이전트(Simple Reflex Agent) 입니다. 이 에이전트는 오직 \u0026lsquo;현재의 인식(current percept)\u0026lsquo;에만 기반하여 행동합니다. \u0026ldquo;만약 A 조건이라면, B 행동을 하라\u0026quot;는 식의 사전 정의된 \u0026lsquo;조건-행동 규칙(condition-action rule)\u0026lsquo;에 따라 반응할 뿐, 과거의 경험이나 행동의 결과를 고려하지 않습니다. 예를 들어, 실내 온도가 설정값 이하로 떨어지면 히터를 켜는 온도 조절기나, 연기를 감지하면 경보를 울리는 화재 경보기가 있습니다.\n이러한 에이전트는 규칙이 명확하고 환경의 모든 정보를 즉시 파악할 수 있는 \u0026lsquo;완전 관찰 가능(fully observable)\u0026rsquo; 환경에서는 효과적이지만, 환경이 복잡하고 동적으로 변하며, 현재의 인식만으로는 모든 상황을 파악할 수 없는 \u0026lsquo;부분 관찰 가능(partially observable)\u0026rsquo; 환경에서는 제대로 작동하기 어렵습니다.\n모델 기반 반사 에이전트 이러한 한계를 극복하기 위해 등장한 것이 모델 기반 반사 에이전트(Model-based Reflex Agent) 입니다. 핵심적인 차별점은 환경에 대한 \u0026lsquo;내부 모델(internal model)\u0026rsquo; 또는 \u0026lsquo;내부 상태(internal state)\u0026lsquo;를 유지하는 것입니다. 내부 모델은 과거의 인식 기록을 바탕으로 현재 보이지 않는 환경의 측면을 추론하고, 행동이 어떤 영향을 미칠지 예측합니다. 즉, 이 에이전트는 \u0026lsquo;기억\u0026rsquo;을 가집니다.\n예를 들어, 로봇 청소기는 단순 반사 에이전트처럼 눈앞의 장애물에만 반응하는 것이 아니라, 이미 청소한 구역을 지도로 기억(내부 모델)하여 같은 곳을 반복해서 청소하는 것을 방지합니다. 또한 자율주행차가 차선을 변경할 때, 현재 카메라에 보이는 옆 차선의 차뿐만 아니라, 잠시 보이지 않았던 사각지대의 차가 있을 가능성을 내부 모델을 통해 추론합니다.\n목표 기반 에이전트 모델 기반 에이전트가 \u0026lsquo;현재\u0026rsquo;와 \u0026lsquo;과거\u0026rsquo;를 이해한다면, 다음 단계는 \u0026lsquo;미래\u0026rsquo;를 계획하는 능력입니다. 목표 기반 에이전트(Goal-based Agent) 는 여기에 \u0026lsquo;목표(goal)\u0026lsquo;라는 정보를 추가하여 의사결정 능력을 한 차원 더 확장합니다. 이 에이전트는 상황에 반응하는 것을 넘어, 미래 상태(목표)를 달성하기 위해 어떤 행동 순서가 필요한지를 탐색하고 계획합니다.\n내비게이션 시스템이 그 예시입니다. 사용자가 목적지(목표)를 입력하면, 에이전트는 현재 위치에서 목적지까지 도달할 수 있는 여러 경로(행동 순서)를 탐색하고, 그중 하나를 선택하여 안내합니다. 이 과정에서 에이전트는 \u0026ldquo;이 길로 가면 목표에 더 가까워지는가?\u0026ldquo;를 끊임없이 평가하며, 더 나은 경로가 발견되면 계획을 수정합니다. 이처럼 목표 기반 에이전트는 검색(search)과 계획(planning) 능력을 통해 반사적 에이전트보다 유연하고 지능적으로 행동합니다.\n하지만 목표 기반 에이전트에도 한계는 있습니다. 목표 달성 여부가 이진법적(binary)이라는 점입니다. 목표에 도달하는 여러 경로가 있다면, 그 경로들 사이의 \u0026lsquo;질적인 차이\u0026rsquo;를 구분하지 못합니다. 예를 들어, 목적지에 도착하는 경로가 여러 개 있을 때, 어떤 길이 더 빠르고, 더 안전하며, 통행료가 더 저렴한지를 종합적으로 고려하여 \u0026lsquo;최적의\u0026rsquo; 경로를 선택하기는 어렵습니다.\n효용 기반 에이전트 효용 기반 에이전트(Utility-based Agent) 는 이 문제를 해결할 수 있습니다. 이 에이전트는 각 상태가 얼마나 \u0026lsquo;바람직한지\u0026rsquo;를 나타내는 수치적 척도인 \u0026lsquo;효용 함수(utility function)\u0026lsquo;를 사용합니다. 효용은 행복, 이익, 효율성 등 다양한 가치를 정량화한 것으로, 에이전트는 자신의 행동 결과로 도달하게 될 상태의 \u0026lsquo;기대 효용(expected utility)\u0026lsquo;을 최대화하는 방향으로 결정합니다.\n다시 내비게이션의 예를 들면, 효용 기반 에이전트는 단순히 목적지에 도착하는 것(목표)뿐만 아니라, 이동 시간, 연료 소모량, 통행료, 도로 안전성 등 여러 상충하는 요인들을 종합적으로 고려하여 가장 높은 효용을 주는 경로를 추천합니다. 이처럼 효용 기반 에이전트는 여러 목표가 충돌하거나, 목표 달성 방식에 여러 등급이 있을 때 훨씬 더 정교하고 합리적인 결정을 내릴 수 있습니다.\n학습 에이전트 앞의 모든 에이전트 유형은 설계자가 부여한 모델, 목표, 효용 함수에 따라 작동합니다. 그러나 환경이 예측 불가능하게 변하거나, 초기 지식이 불완전할 경우 이들의 성능은 저하됩니다. 학습 에이전트(Learning Agent) 는 이러한 한계를 \u0026lsquo;학습\u0026rsquo; 능력을 통해 극복합니다.\n학습 에이전트는 다른 에이전트들의 구성 요소를 모두 포함하면서, 스스로 성능을 개선할 수 있는 독특한 아키텍처를 가집니다. 이 아키텍처는 주로 4가지 요소로 구성됩니다.\n성능 요소(Performance Element): 현재 지식을 바탕으로 외부 환경에 대한 행동을 선택하고 실행하는 부분으로, 사실상 앞서 설명한 에이전트 전체에 해당합니다. 비평가(Critic): 성능 요소의 행동이 얼마나 좋았는지를 외부의 성능 표준(performance standard)과 비교하여 평가하고 피드백을 생성한다. 이 피드백은 보상이나 벌점의 형태일 수 있습니다. 학습 요소(Learning Element): 비평가로부터 받은 피드백을 바탕으로 성능 요소의 내부 모델이나 규칙을 수정하여 미래에 더 나은 결정을 내릴 수 있습니다. 문제 생성기(Problem Generator): 현재까지의 경험을 바탕으로, 새로운 지식을 얻기 위해 시도해볼 만한 새로운 행동(탐험적 행동)을 제안합니다. 이러한 피드백 루프를 통해 학습 에이전트는 경험으로부터 배우고, 낯선 환경에 적응하며, 시간이 지남에 따라 점점 더 나은 성능을 보입니다. 전자상거래 사이트의 추천 시스템이 사용자의 클릭과 구매 이력을 학습하여 점점 더 개인화된 상품을 추천하는 것이나, 자율주행차가 다양한 주행 경험을 통해 안전하고 효율적인 운전 방식을 스스로 터득하는 것이 학습 에이전트의 대표적인 예입니다. 이처럼 지속적인 자기 개선 능력 덕분에 학습 에이전트는 현재 가장 강력하고 적응성이 뛰어난 에이전트 유형으로 주목 받고 있습니다.\n에이전트 유형 핵심 추가 요소 의사결정 기반 환경 적합성 핵심 한계 대표 사례 단순 반사 없음 현재 인식, 조건-행동 규칙 완전 관찰 가능, 정적 기억 부재, 부분 관찰 환경 처리 불가 온도 조절기, 기본 스팸 필터 모델 기반 반사 내부 상태/모델 현재 인식 + 내부 상태 부분 관찰 가능, 동적 목표 부재, 장기 계획 불가 로봇 청소기, 자율주행차 목표 기반 목표 정보 미래 상태 예측, 계획 목표가 명확한 복잡한 환경 경로/방법의 질적 차이 구분 불가 내비게이션, 게임 AI 효용 기반 효용 함수 기대 효용 최대화 상충하는 목표가 있는 환경 효용 함수 설계의 복잡성 개인화 추천, 자원 배분 학습 학습/비평 요소 피드백 기반 자기 개선 미지의 동적 환경 초기 성능 낮음, 많은 데이터 필요 추천 시스템, 자율 로봇 주요 산업별 에이전트 적용 AI 에이전트는 단순한 업무 자동화를 넘어, 각 산업의 고유한 문제를 해결하고 새로운 전략적 가치를 창출하는 핵심 동력으로 나아가고 있습니다.\n산업 분야 주요 사용 사례 핵심 전략적 이점 주요 변혁 방향 핵심 도입 과제 금융 서비스 실시간 사기 탐지, 알고리즘 트레이딩, 개인화된 로보어드바이저, 자동화된 규제 준수 리스크 완화 및 운영 효율성 극대화 수동적 사후 분석에서 능동적 실시간 대응으로 전환 데이터 보안, 규제 준수, 모델의 설명가능성 확보 헬스케어 개인 맞춤형 치료 계획 수립, 의료 영상 분석을 통한 진단 보조, 신약 개발 가속화, 행정 업무 자동화 진단 정확도 향상 및 운영 비용 절감 단발적 진료에서 지속적인 환자 관리로 전환 의료 데이터 프라이버시(HIPAA), 임상적 유효성 검증 제조 및 공급망 예측 유지보수, 실시간 품질 관리, 수요 예측 및 재고 최적화, 적응형 공급망 조정 생산성 향상 및 공급망 탄력성 강화 사후 대응적 생산에서 예측 기반의 자율 운영으로 전환 기존 시스템과의 통합(OT/IT), 센서 데이터의 품질 고객 서비스 복잡한 다단계 문의 해결, 선제적 고객 지원, 옴니채널 경험 개인화, 인간 상담원 증강(Agent-assist) 고객 만족도 증대 및 운영 비용 절감 비용 센터에서 가치 창출 및 고객 관계 관리 허브로 전환 감성적 상호작용의 한계, 대화 맥락 유지의 어려움 금융 서비스 금융 산업은 데이터 집약적이고, 속도와 정확성이 경쟁력을 좌우하는 분야인 만큼 AI 에이전트의 도입이 가장 활발하게 이루어지고 있습니다.\n리스크 관리 및 트레이딩 사용 사례: 알고리즘 트레이딩 에이전트는 시장 데이터를 24시간 분석하여 인간의 개입 없이 최적의 타이밍에 거래 실행 사기 탐지 에이전트는 수백만 건의 거래 패턴을 실시간으로 모니터링하여 이상 징후나 사기 가능성이 있는 거래를 즉시 식별하고 차단 리스크 관리 에이전트는 자금세탁방지(AML), 사베인즈-옥슬리법(SOX), 일반정보보호규정(GDPR)과 같은 복잡한 규제 요건을 지속적으로 모니터링하고 보고서를 자동 생성하여 규제 준수 지원 전략적 영향: 경쟁의 축을 인간 트레이더의 직관과 속도에서 알고리즘의 정교함과 데이터 처리 규모로 이동 사기 및 규제 위반으로 인한 운영 리스크와 막대한 비용을 획기적으로 절감 금융 자문 사용 사례: 로보어드바이저(Robo-advisor)는 고객의 투자 성향, 재무 목표, 리스크 수용도를 분석하여 개인화된 투자 포트폴리오를 자동으로 생성하고 시장 상황에 따라 리밸런싱까지 수행 대출 심사 에이전트는 신청자의 신용 정보와 다양한 데이터를 종합하여 대출 가능 여부와 한도를 신속하게 결정하며, 지능형 챗봇은 복잡한 금융 상품에 대한 문의나 계좌 관련 문제 해결 지원 전략적 영향: 금융 기관은 더 넓은 고객층에게 저렴한 비용으로 자산 관리 서비스를 제공할 수 있게 되어 새로운 시장 창출 헬스케어 임상 의사결정 증강 사용 사례: 환자의 유전체 정보, 과거 진료 기록, 최신 연구 논문, 생활 습관 데이터 등을 종합적으로 분석하여 개인에게 최적화된 맞춤형 치료 계획을 생성하고 의사에게 제안 IBM Watson Health와 같은 시스템은 암 진단 및 치료법 추천에 활용 엑스레이, CT, MRI와 같은 의료 영상을 분석하여 인간 의사가 놓칠 수 있는 미세한 병변을 찾아내 진단을 보조 방대한 임상시험 데이터를 분석하여 신약 개발 후보 물질을 발굴하고 개발 기간 단축에 기여 전략적 영향: 표준화된 치료 방식에서 벗어나 정밀 의료(Precision Medicine)와 예측 의료(Predictive Medicine)로의 전환 가속화 헬스케어 운영 사용 사례: 의사의 진료 기록을 바탕으로 전자의무기록(EHR)을 자동으로 업데이트하고, 진료 내용에 맞는 의료 코드를 생성하여 보험 청구 및 정산 프로세스를 자동화 병원 내 자율주행 로봇이 약품이나 검체를 운송하고, 방역 작업을 수행하며 물류를 최적화 스마트워치나 혈당 측정기와 같은 웨어러블 기기와 연동하여 환자의 상태를 24시간 실시간으로 모니터링하고, 이상 징후 발생 시 의료진에게 즉시 경고 전략적 영향: 병원의 행정 업무 부담과 운영 비용을 획기적으로 절감 일회성 병원 방문에 의존하던 환자 관리 패러다임을 \u0026lsquo;지속적이고 예방적인 관리\u0026rsquo;로 전환 제조 및 공급망 스마트 팩토리 사용 사례: 생산 설비에 부착된 수많은 센서 데이터를 실시간으로 분석하여 장비의 미세한 이상 징후를 감지하고, 고장이 발생하기 전에 유지보수 일정을 알리는 예측 유지보수(Predictive Maintenance)를 수행 생산 라인의 비전 센서 데이터를 분석하여 제품의 결함을 실시간으로 검출하고 불량률 감소 공장 전체의 에너지 소비 패턴을 분석하여 비효율적인 부분을 찾아내고 에너지 사용을 최적화 전략적 영향: 설비의 가동 중단 시간(downtime)을 최소화하고 생산성을 극대화 불량품 발생으로 인한 폐기물과 재작업 비용을 줄이고, 에너지 비용을 절감하여 제조 원가 감소 적응형 공급망 사용 사례: 과거 판매 데이터, 시장 트렌드, 거시 경제 지표 등을 종합 분석하여 미래 수요 예측, 이를 바탕으로 최적의 재고 수준을 유지하도록 자동 발주를 실행 특정 지역의 자연재해나 지정학적 리스크로 인해 부품 공급에 차질이 생길 경우, 대체 공급업체를 탐색하거나, 다른 경로로 운송 계획을 재수립하는 등 실시간으로 공급망 조정 전략적 영향: \u0026lsquo;사후 대응적\u0026rsquo; 공급망 관리를 \u0026lsquo;예측 기반의 능동적\u0026rsquo; 관리로 전환 과잉 재고로 인한 비용과 재고 부족으로 인한 판매 기회 손실 감소 예측 불가능한 외부 충격에 대한 회복탄력성을 높여 비즈니스의 연속성 보장 고객 서비스 FAQ 봇 사용 사례: 고객이 복잡한 요금 청구 이의를 제기했을 때, 에이전트는 고객의 과거 이용 내역, 결제 시스템, 프로모션 데이터베이스 등 여러 백엔드 시스템에 자율적으로 접근하여 문제의 원인을 파악하고 해결책 제시 고객의 웹사이트 행동 패턴이나 과거 문의 이력을 분석하여 문제가 발생하기 전에 먼저 연락을 취하는 선제적 지원(Proactive Support) 제공 고객이 채팅으로 문의를 시작했다가 이메일이나 전화로 채널을 변경하더라도 이전 대화의 맥락을 그대로 유지하는 옴니채널(Omnichannel) 경험 제공 전략적 영향: 고객 만족도와 충성도를 높여 매출 기여 기대 24시간 365일 고품질의 지원을 제공함으로써 고객 경험을 획기적으로 개선 인간-에이전트 파트너십 사용 사례: 상담원의 통화 중, 에이전트 어시스트(Agent-assist) 도구는 실시간으로 대화를 분석하여 관련된 지식 베이스 문서를 화면에 표시 고객 문의에 대한 최적의 답변 초안을 작성하여 제안 통화 종료 후에는 전체 대화 내용을 자동으로 요약하여 CRM 시스템에 기록 전략적 영향: 인간 상담원의 역량 향상 신입 상담원의 교육 시간을 단축하고, 모든 상담원이 일관된 수준의 서비스를 제공하도록 보장 분석 및 시사점 위 사례에서 두 가지 핵심적인 변화의 흐름을 발견할 수 있습니다.\n첫 번째는 \u0026lsquo;능동적 패러다임으로의 전환(Proactive Paradigm Shift)\u0026lsquo;입니다. 모든 산업 분야에서 AI 에이전트가 창출하는 핵심 가치는 기존의 \u0026lsquo;수동적, 사후 대응적\u0026rsquo; 운영 모델을 \u0026lsquo;능동적, 예측적\u0026rsquo; 모델로 전환합니다. 단순히 주어진 질문에 답하거나 명령을 실행하는 것을 넘어, 제조업에서는 설비가 고장 나기 \u0026lsquo;전에\u0026rsquo; 유지보수를 예측하고, 금융에서는 사기가 발생한 \u0026lsquo;후\u0026rsquo;가 아니라 의심 현상을 먼저 탐지하며, 고객 서비스에서는 고객이 불만을 제기하기 \u0026lsquo;전에\u0026rsquo; 문제를 해결합니다. 이처럼 지속적으로 데이터를 모니터링하고 분석하여 미래를 예측하고 선제적으로 행동하는 능력이야말로 AI 에이전트가 제공하는 가장 큰 전략적 가치입니다.\n두 번째는 새로운 \u0026lsquo;데이터 플라이휠(Data Flywheel)\u0026lsquo;의 생성입니다. AI 에이전트의 성공적인 도입은 스스로를 강화하는 강력한 선순환 구조를 만듭니다. 에이전트가 더 많은 작업을 수행할수록, 더 많은 상호작용 및 결과 데이터가 생성됩니다. 이 데이터는 다시 에이전트의 기반 모델과 의사결정 로직을 개선합니다. 이 \u0026lsquo;데이터 플라이휠\u0026rsquo; 효과는 한번 앞서나가기 시작한 기업이 경쟁사와의 격차를 기하급수적으로 벌릴 수 있게 만드는 강력한 경쟁 해자(competitive moat)로 작용할 수 있습니다.\n에이전트의 과제 기술적 난제와 신뢰성 추론과 맥락 이해의 결함: 현재 에이전트들은 깊이 있는 다단계 추론이나 복잡한 지시의 맥락을 완전히 이해하는 능력이 부족합니다. 때로는 논리적 오류에 빠져 무한 루프를 돌거나, 중요한 맥락을 놓쳐 엉뚱한 행동을 수행합니다. 예를 들어, \u0026ldquo;부서 보고서를 작성해줘\u0026quot;라는 간단한 요청에도, 해당 사용자의 부서, 역할, 보고서의 성격, 이전 대화와의 연관성 등 수많은 맥락을 정확히 파악해야 하는데, 현재 모델들은 실수할 가능성이 높습니다. \u0026lsquo;취약성(Brittleness)\u0026rsquo; 문제: 통제된 환경에서 훈련된 에이전트는 한 번도 경험해보지 못한 새로운(out-of-distribution) 시나리오에 직면했을 때 예측 불가능하게 실패하는 경향이 있습니다. 이들의 성능은 아직 견고하지(robust) 않으며, 특정 상황에서는 정확도가 14.9%까지 떨어지는 등 심각한 오류를 보입니다. 이러한 \u0026lsquo;취약성\u0026rsquo;은 금융 거래나 의료 진단과 같이 실패의 대가가 큰 고위험 환경에 에이전트 단독 투입이 어려운 요인입니다. 장기 기억과 지속적 학습의 한계: 현재 에이전트들의 기억 시스템은 완전하지 않습니다. 모델 전체를 재훈련하지 않고 실시간으로 새로운 지식을 반영하여 파라미터를 업데이트하는 \u0026lsquo;지속적 학습(continuous learning)\u0026lsquo;은 아직 주요 연구 과제입니다. 확장성과 비용: AI 에이전트를 훈련하고 운영하는 데 필요한 컴퓨팅 자원은 막대합니다. 이는 많은 기업에게 상당한 비용 장벽으로 작용하며, 특히 온프레미스 환경에서 인프라를 구축하고 유지하는 것은 확장성 측면에서도 큰 부담입니다. 또한, 여러 종류의 AI를 통합하여 운영하고, 빠르게 변화하는 기술과 API 정책에 대응하는 것 역시 복잡하고 어려운 유지보수 과제입니다. 윤리 및 거버넌스 책임과 배상: 책임의 공백: 자율적인 에이전트가 잘못된 금융 자문을 제공하거나, 의료 과실을 일으키거나, 개인정보를 유출하는 등 피해를 발생시켰을 때, 그 책임은 누구에게 있는지는 아직 명확한 법적, 윤리적 기준이 정립되지 않았습니다. 2024년, 에어캐나다의 챗봇이 고객에게 잘못된 유족 할인 정책 정보를 제공했고, 법원이 이에 대해 회사에 배상 책임을 인정한 사건은 중요한 선례를 남았습니다. 이는 기업이 AI를 단순한 \u0026lsquo;도구\u0026rsquo;로 취급하며 책임을 회피할 수 없음을 의미합니다. 편향, 공정성, 그리고 조작: 편향된 데이터로 학습된 AI 에이전트는 채용, 대출 심사, 범죄 예측 등 민감한 영역에서 기존의 사회적 차별을 그대로 답습하거나 증폭시킬 위험이 있습니다. 예를 들어, 과거 남성 위주의 채용 데이터로 학습한 에이전트는 여성 지원자를 부당하게 차별할 수 있습니다. 더 나아가, 에이전트의 설득력 있는 상호작용 능력은 사용자의 인지적, 감정적 취약점을 이용하여 사용자의 최선의 이익에 부합하지 않는 방향으로 행동을 미묘하게 유도하는 \u0026lsquo;조작(manipulation)\u0026lsquo;의 위험이 있습니다. 투명성과 설명가능성(XAI): \u0026lsquo;블랙박스\u0026rsquo; 문제: 복잡한 딥러닝 모델에 기반한 에이전트의 의사결정 과정은 종종 개발자조차 완벽히 이해하기 어려운 \u0026lsquo;블랙박스\u0026rsquo;와 같습니다. 이러한 투명성의 부재는 에이전트의 행동을 감사하고, 그 결과를 신뢰하며, 오류의 원인을 진단하는 것을 어렵게 만듭니다. 데이터 프라이버시와 보안: AI 에이전트는 효과적으로 작동하기 위해 방대한 양의 개인 및 기업 데이터에 접근해야 합니다. 이는 심각한 프라이버시 침해 위험을 야기하며, 에이전트가 고부가가치 공격 대상이 됩니다. 특히, 공격자가 악성 이메일이나 교묘한 명령을 통해 에이전트를 속여 민감한 데이터를 외부로 유출하도록 만드는 \u0026lsquo;하이재킹(hijacking)\u0026rsquo; 공격은 새로운 보안 위협으로 떠오르고 있습니다. 특정 업무 시나리오 실험 결과에서 하이재킹 공격 성공률은 평균 92%에 달할 정도로 심각한 수준입니다. 분석 및 시사점 현재의 AI 에이전트는 현실 세계의 복잡성을 신뢰성 있게 처리할 만큼 충분히 \u0026lsquo;유능\u0026rsquo;하거나 \u0026lsquo;견고\u0026rsquo;하지 않습니다. 이는 에이전트의 기술적 한계이며, 이로 인해 발생하는 윤리적 문제를 경계해야 합니다. 예를 들어, 에이전트가 사용자의 지시를 잘못 해석하여 엉뚱한 도구를 사용하는 기술적 문제인 \u0026lsquo;기능 호출 환각(function-calling hallucination)\u0026lsquo;은, 에어캐나다 사례처럼 고객에게 잘못된 정책 정보를 제공하는 윤리적, 법적 실패로 직접 이어집니다. 기술적 문제인 \u0026lsquo;신뢰성 부족\u0026rsquo;이 \u0026lsquo;자율성\u0026rsquo;이라는 매개를 통해 \u0026lsquo;책임 소재의 문제\u0026rsquo;라는 윤리적 문제를 야기합니다.\n하지만 단순히 더 나은 기술을 개발하는 것만으로는 해결되지 않습니다. 기술 개발과 동시에, 에이전트가 안전하게 작동할 수 있는 강력한 \u0026lsquo;거버넌스 및 감독 프레임워크\u0026rsquo;를 구축하는 것이 새로운 핵심 역량으로 부상하고 있습니다. 이 프레임워크는 다음과 같은 다층적 접근을 요구합니다.\n기술적 안전장치(Technological Guardrails): 환각 탐지기, 편향 감사 도구, 보안 코드 실행 환경 등 에이전트의 오류와 악용을 기술적으로 방지하는 시스템 내장 조직적 책임(Organizational Accountability): 에이전트의 행동에 대한 명확한 인간 책임자를 지정하고, 윤리 위원회를 설치하며, 중요한 결정에는 반드시 인간이 개입(human-in-the-loop)하거나 감독(human-on-the-loop)하는 프로세스 수립 규제 준수(Regulatory Alignment): EU의 AI 법(AI Act)과 같이 새롭게 등장하는 규제에 선제적으로 대응하고, 투명성과 문서화를 시스템 설계 초기부터 고려 마치며 이번 포스트를 작성하면서 강화 학습이나 멀티 에이전트가 이전부터 있었던 개념임을 알았습니다. 새로운 개념 뿐만 아니라, 기존의 개념을 기술의 발전에 맞게 구현하는 것도 큰 가치를 가져올 수 있다고 느꼈습니다.\n그리고 위의 내용 중, 뛰어난 범용 지능보다 \u0026ldquo;주어진 지능을 더 정교하게 조율하여 실질적인 가치를 창출\u0026quot;하는 것이 경쟁의 핵심이라는 내용이 특히 인상 깊었습니다. 저 역시 에이전트를 개발하는 프로젝트에서, 상태 머신을 구현하여 추론을 보조하거나 계획 단계를 추가했을 때 더 높은 성능을 관찰할 수 있었기에 더욱 공감했습니다.\n언급된 에이전트의 사례들을 보면, 공통적으로 데이터를 기반으로 판단, 분석 등을 수행하여 사용자에게 알리거나 초안을 작성하고 있습니다. 각 산업의 데이터와 결합할 때 가치가 더 극대화되는 것으로 보이며, 그럼에도 에어캐나다의 사례와 같이 100% 신뢰할 수는 없기에 보조의 역할까지만 수행하고 사람이 개입하는 것이 적절할 것으로 보입니다.\n지금까지 에이전트를 구현할 때는 속성 중 학습 능력에 대한 고민이 부족했습니다. 앞으로의 목표로 사용 데이터를 수집하고, 이를 에이전트에 반영하여 서비스 품질을 개선할 방법을 고민하려 합니다.\n본 포스트는 Google Gemini의 응답을 기반으로 저의 의견을 반영하여 다시 작성했습니다.\n","date":"2025-08-07T19:03:00Z","permalink":"https://yeonhl.github.io/systems/agents/concept/","title":"에이전트의 개념"},{"content":"개요 인공지능은 갑작스럽게 등장한 것이 아닙니다. 그것은 기계에 생명과 의지를 불어넣고자 했던 인류의 수천 년에 걸친 철학적, 공학적 결과물입니다. 이 포스트에서는 역사 속에서 인공지능을 어떻게 접근했는지와, 앞으로 해결해야 할 문제를 살펴보겠습니다.\n인공지능에 대한 역사 속 접근 신화 속 자동기계에서 기계 인간까지 가장 오래된 기록은 신화와 전설에서 발견됩니다. 고대 그리스인들은 신의 장인 헤파이스토스가 만든 청동 거인 탈로스(Talos)의 이야기를 전했습니다. 탈로스는 크레타 섬의 해안을 하루에 세 번씩 순찰하며 침략자들에게 바위를 던지는, 의지를 가진 수호자였습니다. 중세 유대 전설에 등장하는 골렘(Golem)은 진흙으로 빚어져 신의 이름을 적은 종이를 통해 생명을 얻는 인조 인간으로, 말은 못 하지만 명령을 수행하는 존재로 묘사되었습니다. 이러한 신화적 존재들은 단순한 상상력의 산물이 아니라, 인간의 형상을 하고 스스로 움직이는 존재, 즉 \u0026lsquo;자동기계(automaton)\u0026lsquo;에 대한 열망을 반영합니다. \u0026lsquo;Automaton\u0026rsquo;이라는 단어도 고대 그리스어로 \u0026ldquo;스스로의 의지에 따라 행동하는\u0026quot;이라는 의미를 담고 있습니다.\n이러한 열망은 점차 공학적 현실로 구체화되었습니다. 기원전 3세기 알렉산드리아의 발명가 크테시비오스(Ctesibius)와 기원후 1세기 헤론(Hero of Alexandria)은 물과 압축 공기의 원리를 이용하여 스스로 움직이는 새, 자동으로 연주되는 오르간, 심지어 동전을 넣으면 성수가 나오는 자동판매기와 같은 정교한 기계 장치들을 만들었습니다. 8세기 이슬람 세계의 알자자리(Al-Jazari)는 왕실 연회에서 손님들을 즐겁게 하기 위해 호수 위를 떠다니며 자동으로 음악을 연주하는 인간형 로봇 밴드를 설계했습니다. 그의 설계에는 페그의 위치를 바꿔 다른 리듬을 연주하도록 프로그래밍할 수 있는 드럼 머신까지 포함되어 있었습니다. 고대 중국의 문헌 『열자(列子)』에는 주나라 목왕에게 살아있는 듯 정교하게 움직이는 기계 인형을 선보인 장인 연사(偃師)의 이야기가 기록되어 있습니다.\n이 고대의 자동기계들은 비록 현대적 의미의 지능을 갖추지는 못했지만, 중요한 철학적 전제를 공유합니다. 그것은 복잡한 기계 장치를 통해 생명체의 \u0026lsquo;행동\u0026rsquo;을 모방할 수 있다는 믿음입니다. 이들은 인간의 개입 없이 스스로 움직이는 존재를 만들고자 하는 오랜 탐구의 증거이며, 이후 근대 철학자들이 제기할 \u0026lsquo;기계와 인간\u0026rsquo;의 문제에 대한 서막을 열었습니다.\n데카르트의 분기점 - 기계와 영혼 17세기 프랑스 철학자 르네 데카르트(René Descartes)는 인공적 행위자에 대한 논의를 신화와 공학의 영역에서 본격적인 철학의 문제로 끌어올린 결정적 인물입니다. 인간과 기계를 구분하는 명확한 기준을 제시함으로써, 이후 인공지능 철학의 핵심적인 질문들을 300년이나 앞서 제기했습니다.\n데카르트의 사유의 중심에는 \u0026lsquo;동물-기계론(bête machine)\u0026lsquo;이 있습니다. 그는 동물들이 비록 복잡하고 정교하게 움직이지만, 이성과 영혼이 없는 자동기계에 불과하다고 주장했습니다. 동물의 행동은 시계의 톱니바퀴가 맞물려 돌아가듯, 오직 기관들의 배치와 물리적 법칙에 의해 결정되는 기계적 반응이라는 것입니다.\n이러한 기계론적 세계관을 바탕으로, 데카르트는 그의 저서 『방법서설(Discourse on the Method)』에서 아무리 인간과 흡사하게 만들어진 기계라 할지라도 \u0026lsquo;진정한 인간\u0026rsquo;과 구별할 수 있는 \u0026ldquo;두 가지 매우 확실한 시험 방법\u0026quot;이 있다고 주장했습니다.\n언어 사용의 유연성과 창의성: 데카르트는 기계가 특정 상황에 반응하여 단어를 내뱉을 수는 있겠지만, \u0026ldquo;자신의 앞에서 말해지는 모든 것의 의미에 응답하기 위해\u0026rdquo; 단어나 기호를 다양하게 배열하여 사용할 수는 없을 것이라 단언했습니다. 인간의 언어 사용은 단순히 소리를 내는 것이 아니라, 생각과 의미를 전달하는 창의적이고 맥락적인 행위이기 때문입니다.\n행동의 보편성과 일반성: 기계는 특정 과업에서는 인간만큼, 혹은 인간보다 더 잘할 수도 있습니다. 그러나 이들은 \u0026ldquo;지식으로부터 행동하는 것이 아니라, 단지 그 기관들의 배치에 따라서만 행동하기 때문에\u0026rdquo; 필연적으로 다른 많은 일들에서는 실패할 것이라고 보았습니다. 즉, 진정한 이성은 특정 작업에 국한되지 않고, \u0026ldquo;삶의 모든 상황에서\u0026rdquo; 보편적으로 작용하는 능력이라는 것입니다.\n이 두 가지 테스트는 놀라울 정도로 현대 AI 연구의 핵심 과제를 예견했습니다. 데카르트의 첫 번째 테스트는 문맥에 맞는 유연한 대화 능력을 평가하는 것으로, 이는 훗날 앨런 튜링이 제안한 \u0026lsquo;튜링 테스트\u0026rsquo;의 직접적인 원형이 됐습니다. 두 번째 테스트는 특정 작업에만 능한 \u0026lsquo;좁은 AI(Narrow AI)\u0026lsquo;와 다양한 영역에서 지능적으로 행동할 수 있는 \u0026lsquo;범용 인공지능(Artificial General Intelligence, AGI)\u0026lsquo;을 구분하는 문제와 정확히 일치합니다. 결국 데카르트는 영혼과 육체의 이원론을 통해, 기계가 결코 넘을 수 없는 인간 고유의 영역으로 \u0026lsquo;이성\u0026rsquo;을 설정했으며, 그 이성의 증거를 언어와 행동의 일반성에서 찾았습니다. 이로써 그는 인공지능이 무엇을 성취해야 \u0026lsquo;생각한다\u0026rsquo;고 말할 수 있는지에 대한 철학적 기준을 최초로 정립했습니다.\n라이프니츠의 계산하는 우주 데카르트가 기계와 영혼의 넘을 수 없는 간극을 설정했다면, 그와 동시대의 독일 철학자이자 수학자인 고트프리트 빌헬름 라이프니츠(Gottfried Wilhelm Leibniz)는 그 간극을 \u0026lsquo;계산\u0026rsquo;이라는 개념으로 메우고자 했습니다. 그는 계산주의적 인공지능의 직접적인 철학적 조상으로, 인간의 이성 자체를 기계적인 계산 과정으로 환원할 수 있다는 혁명적인 아이디어를 제시했습니다.\n라이프니츠의 야망은 \u0026lsquo;보편 기호법(characteristica universalis)\u0026lsquo;이라는 개념에 집약되어 있습니다. 그는 모든 인간의 생각을 기본적인 개념들의 조합으로 분해하고, 각각의 기본 개념에 고유한 기호를 부여하는 보편적인 상징 언어를 만들 수 있다고 믿었습니다. 이 체계가 완성되면, 모든 복잡한 아이디어나 논증은 이 기호들의 조합으로 표현될 수 있습니다. 그리고 \u0026lsquo;추론 계산기(calculus ratiocinator)\u0026lsquo;라는 논리적 연산 규칙을 통해 이 기호들을 조작함으로써, 모든 질문에 대한 답을 기계적으로 도출할 수 있다는 것입니다. 라이프니츠는 이러한 체계가 갖춰지면 철학자들 사이의 논쟁은 더 이상 필요 없으며, 회계사들이 계산하듯 \u0026ldquo;자, 계산해 봅시다(Calculemus)!\u0026ldquo;라고 말하며 답을 찾을 수 있을 것이라고 선언했습니다.\n이는 인간의 사유 과정을 형식화하고 자동화하려는 최초의 체계적인 시도였습니다. 데카르트가 기계와 구별되는 인간의 영혼을 강조했다면, 라이프니츠는 이성적 사유의 \u0026lsquo;과정\u0026rsquo; 자체에 주목하여 그것이 본질적으로 계산과 같다고 보았습니다. 그의 철학에서 우주는 \u0026lsquo;모나드(monad)\u0026lsquo;라는 영적 실체로 구성되지만, 자연 세계의 작동 원리는 기계적입니다. 그는 유기체를 \u0026ldquo;무한히 복잡한 기계\u0026quot;로 보았는데, 이는 인간이 만든 유한한 기계와는 구별되지만 근본적으로는 기계적 원리에 따른다는 점에서 데카르트와 궤를 같이 하면서도, 그 복잡성의 차원을 무한으로 끌어올립니다.\n라이프니츠의 이러한 비전은 현대 컴퓨터 과학과 인공지능의 철학적 토대를 마련했습니다. 그의 아이디어는 \u0026lsquo;모든 사유는 계산\u0026rsquo;이라는 계산주의(computationalism)의 핵심 명제를 예고했으며, 기호와 규칙을 통해 지능적 행동을 구현하려는 기호주의 AI의 직접적인 뿌리가 되었습니다. 데카르트가 AI가 \u0026lsquo;무엇을 해야 하는가\u0026rsquo;라는 목표를 설정했다면, 라이프니츠는 \u0026lsquo;어떻게 할 것인가\u0026rsquo;라는 방법에 대한 청사진을 제시했습니다. 이 두 거인의 사유를 통해, 인공적 행위자에 대한 논의는 단순한 모방을 넘어 지능과 이성의 본질을 탐구하는 심오한 철학적 문제로 자리 잡았습니다.\n모방 게임 - 튜링의 실용적 시험 20세기에 들어서면서 인공적 행위자에 대한 오랜 꿈은 앨런 튜링의 이론적 토대와 디지털 컴퓨터의 발명으로 현실적인 공학적 목표가 되었습니다. 이와 함께 \u0026ldquo;기계가 생각할 수 있는가?\u0026ldquo;라는 질문은 더 이상 형이상학적 사변이 아닌, 구체적인 검증과 논박의 대상이 되었습니다.\n1950년, 영국의 수학자이자 컴퓨터 과학의 선구자인 앨런 튜링(Alan Turing)은 \u0026ldquo;기계가 생각할 수 있는가?\u0026ldquo;라는 질문이 정의하기 어려운 용어들로 인해 \u0026ldquo;너무 무의미하다\u0026quot;고 선언하며, 이를 대체할 구체적이고 실용적인 테스트를 제안했습니다. 이것이 바로 \u0026lsquo;모방 게임(Imitation Game)\u0026lsquo;으로, 후일 \u0026lsquo;튜링 테스트(Turing Test)\u0026lsquo;로 알려졌습니다.\n튜링 테스트의 구조는 간단합니다. 심문관, 인간, 그리고 기계, 세 명의 참가자가 있습니다. 심문관은 분리된 방에서 텍스트 기반의 대화를 통해 누가 인간이고 누가 기계인지를 판별해야 합니다. 기계의 목표는 심문관을 속여 자신을 인간으로 믿게 만드는 것이고, 인간 참가자는 심문관이 올바른 판단을 내리도록 돕습니다. 튜링은 만약 기계가 이 게임을 충분히 잘 수행하여 심문관이 일정 시간 동안 일정 확률 이상으로 인간과 기계를 구분하지 못한다면, 우리는 그 기계가 \u0026lsquo;생각한다\u0026rsquo;고 말할 수 있다고 주장했습니다. 이는 지능에 대한 행동주의적 관점을 제시하며 이후의 모든 논의의 시발점이 되었습니다.\n튜링 테스트의 강점은 그 명확성과 실용성에 있습니다. \u0026lsquo;지능\u0026rsquo;이나 \u0026lsquo;생각\u0026rsquo;과 같은 모호한 개념에 대한 철학적 정의를 내리는 대신, 튜링은 측정 가능한 행동적 기준을 제시했습니다. 이 테스트를 통과하기 위해 기계는 자연어 이해, 추론, 지식 활용, 학습 등 인공지능 연구의 거의 모든 핵심 과제를 종합적으로 수행해야 합니다. 이는 300년 전 데카르트가 제기했던 \u0026lsquo;유연한 언어 사용\u0026rsquo;이라는 시험을 직접적으로 구현한 것이기도 합니다.\n그러나 튜링 테스트는 제안된 순간부터 수많은 비판에 직면했습니다.\n의식의 부재: 테스트는 오직 외부 행동만을 평가하므로, 기계가 진정한 의식이나 이해 없이 단순히 인간의 대화를 \u0026lsquo;모방\u0026rsquo;하는 것만으로도 통과할 수 있다는 비판이 제기됩니다. 이는 튜링 테스트가 지능의 충분조건이 될 수 없다는 핵심적인 반론입니다.\n인간중심주의(Chauvinism): 테스트가 인간과 같은 방식의 대화 능력을 지능의 유일한 척도로 삼는다는 비판입니다. 인간과 다른 형태의 지능을 가진 존재는 이 테스트를 통과할 수 없을 것입니다.\n상징 조작의 한계: 기계가 단순히 기호를 조작할 뿐, 그 의미를 이해하지 못한다는 주장입니다. 이 비판은 이후 존 설의 \u0026lsquo;중국어 방\u0026rsquo; 논증에서 더욱 정교하게 발전합니다.\n실패한 예측: 튜링은 2000년경에는 컴퓨터가 5분간의 대화 후 심문관을 30%의 확률로 속일 수 있을 것이라고 예측했지만, 이 예측은 빗나갔다.21 이는 그가 AI 개발의 어려움을 과소평가했음을 보여줍니니다.\n튜링 테스트에서 파생된 논쟁은 인공지능을 바라보는 두 가지 핵심적인 관점, 즉 \u0026lsquo;약한 인공지능(Weak AI)\u0026lsquo;과 \u0026lsquo;강한 인공지능(Strong AI)\u0026lsquo;의 구분을 낳았습니다.\n약한 인공지능 (Weak AI): 이 관점은 인공지능을 특정 문제를 해결하도록 설계된 강력한 도구로 봅니다. 이는 지능의 \u0026lsquo;시뮬레이션\u0026rsquo;이며, 진정한 이해나 의식은 없다고 가정합니다. 이메일 스팸 필터, 체스 프로그램, 현재의 대규모 언어 모델(LLM) 등이 여기에 해당합니다. 이들은 정해진 규칙과 패턴에 따라 효율적으로 작동하지만, 그들이 처리하는 정보의 의미를 진정으로 이해하지는 못합니다.\n강한 인공지능 (Strong AI): 이는 훨씬 더 급진적인 주장으로, 올바르게 프로그래밍된 충분히 복잡한 컴퓨터는 단순히 마음을 시뮬레이션하는 것을 넘어, 말 그대로 마음을 소유한다고 봅니다. 즉, 강한 인공지능은 진정한 이해, 의식, 그리고 자기 인식을 가질 수 있다는 가설입니다.\n결론적으로 튜링 테스트는 \u0026lsquo;생각하는 기계\u0026rsquo;에 대한 논의를 철학적 사변에서 경험적 검증의 영역으로 끌어내린 기념비적인 제안이었습니다. 비록 지능이나 의식에 대한 완벽한 척도는 아닐지라도, 그것은 AI가 달성해야 할 구체적인 목표를 제시했으며, 이후의 모든 AI 철학 논쟁의 출발점이 되었습니다.\n중국어 방 - 설의 강인공지능 비판 튜링 테스트가 행동주의적 관점에서 지능을 정의하려 했다면, 1980년 철학자 존 설(John Searle)은 \u0026lsquo;중국어 방(Chinese Room)\u0026lsquo;이라는 강력한 사고 실험을 통해 그 근본을 뒤흔들었습니다. 그의 목표는 단순히 튜링 테스트를 비판하는 것을 넘어, 설이 \u0026lsquo;강인공지능(Strong AI)\u0026rsquo; 가설이라고 부르는, 인공지능 분야의 핵심적인 철학적 주장을 반박하는 것이었습니다.\n강인공지능 가설이란 \u0026ldquo;적절하게 프로그래밍된 컴퓨터는 단순히 마음을 \u0026lsquo;시뮬레이션\u0026rsquo;하는 것이 아니라, 문자 그대로 마음을 \u0026lsquo;소유\u0026rsquo;한다\u0026quot;는 주장입니다. 즉, 올바른 프로그램을 실행하는 것은 그 자체로 생각하고 이해하는 것과 같다는 계산주의(computationalism) 및 기능주의(functionalism)적 입장입니다.\n설의 사고 실험은 다음과 같습니다. 중국어를 전혀 모르는 한 사람이 방 안에 갇혀 있습니다. 그에게는 두 가지가 주어진다. 하나는 중국어 기호들로 가득 찬 바구니이고, 다른 하나는 영어로 된 규칙 책입니다. 이 규칙 책은 \u0026ldquo;만약 이러이러한 모양의 기호 뭉치가 들어오면, 저러저러한 모양의 기호 뭉치를 내보내라\u0026quot;는 식으로 지시합니다. 방 밖의 사람들은 중국어로 된 질문지를 방 안으로 넣고, 방 안의 사람은 규칙 책에 따라 적절한 중국어 기호 뭉치(답변)를 찾아 밖으로 내보냅니다. 만약 규칙 책이 매우 정교하다면, 방 밖의 중국어 화자들은 방 안에 중국어를 완벽하게 이해하는 사람이 있다고 믿게 될 것입니다. 즉, 이 시스템은 튜링 테스트를 통과합니다.\n설은 여기서 \u0026ldquo;이 시스템 안의 그 어떤 것도 중국어를 \u0026lsquo;이해\u0026rsquo;하는가?\u0026rdquo; 라는 질문을 합니다. 방 안의 사람은 분명 중국어를 이해하지 못합니다. 그는 단지 의미를 모르는 기호들을 규칙에 따라 조작하고 있을 뿐입니다. 규칙 책이나 기호 바구니 역시 이해 능력이 없습니다. 그렇다면 \u0026lsquo;시스템 전체\u0026rsquo;가 이해한다고 말할 수 있을까요? 설은 이것이 터무니없는 주장이라고 일축합니다.\n이 사고 실험을 통해 설은 다음과 같은 결론을 도출합니다. 컴퓨터가 프로그램을 실행하는 과정은 중국어 방 안의 사람이 규칙 책을 따르는 것과 본질적으로 동일합니다. 컴퓨터는 구문(syntax), 즉 형식적인 기호 조작 규칙에 따라 작동할 뿐, 그 기호가 가리키는 의미(semantics) 나 진정한 이해, 즉 지향성(intentionality) 을 가질 수 없습니다. 따라서 아무리 정교한 프로그램이라도, 그것은 결코 진정한 마음이나 의식을 가질 수 없으며, 강인공지능 가설은 근본적으로 틀렸다는 것입니다.\n중국어 방 논증은 수많은 반박에 부딪혔습니다. \u0026lsquo;시스템 반론\u0026rsquo;(방 안의 사람이 아니라, 사람, 책, 기호를 포함한 시스템 전체가 중국어를 이해한다는 주장)이나 \u0026lsquo;로봇 반론\u0026rsquo;(로봇의 몸에 들어가 세상과 상호작용하며 기호의 의미를 습득하게 하면 이해가 생긴다는 주장) 등이 대표적입니다. 그러나 이러한 반론들에도 불구하고, 존 설의 가설은 시스템의 핵심에는 여전히 의미를 이해하지 못하는 순수한 형식적 기호 조작만이 있을 뿐이라는 입장을 고수합니다. 이 논증은 AI가 인간과 같은 \u0026lsquo;이해\u0026rsquo;에 도달할 수 있는지에 대한 근본적인 회의를 제기하며, 오늘날까지도 AI 철학의 가장 뜨거운 논쟁거리로 남아있습니다.\n인공지능은 행동할 수 있는가? 자율성과 자유의지 인공지능의 행위자성을 논의하기 전, 혼동을 피하기 위해 \u0026lsquo;자율성(autonomy)\u0026lsquo;과 \u0026lsquo;자유의지(free will)\u0026lsquo;의 두 개념을 명확히 구분해야 합니다.\n자율성 (Autonomy): 외부의 직접적인 통제나 강제 없이, 자신의 내부 상태와 규칙에 따라 행동할 수 있는 기능적, 조작적 독립성을 의미합니다. \u0026lsquo;자기 스스로의 법칙(auto-nomos)\u0026lsquo;에 따라 행동하는 능력으로, 자신의 이성이나 가치 판단에 기반해 행동할 수 있는 상태입니다. 예를 들어, 인간의 개입 없이 스스로 경로를 선택하고 장애물을 회피하는 자율주행 자동차는 이러한 의미에서 \u0026lsquo;자율적\u0026rsquo;이라고 할 수 있습니다. 자유의지 (Free Will): 단순히 내적 규칙에 따라 행동하는 것을 넘어, 대안적 가능성들 사이에서 진정으로 선택할 수 있는 능력, 자신이 의식적으로 지지하는 이유에 따라 행동하는 능력, 그리고 자신의 의도의 궁극적인 원천이 되는 능력을 포함하는 훨씬 더 깊은 형이상학적 개념입니다. 이는 단순히 움직일 수 있다는 것과 그 움직임에 \u0026lsquo;의미\u0026rsquo;를 부여할 수 있다는 것 사이의 근본적인 간극을 나타냅니다. 또한 자유의지는 도덕적 책임의 전제 조건으로 여겨집니다. 현재의 인공지능은 명백히 기능적 \u0026lsquo;자율성\u0026rsquo;을 획득하고 있지만, \u0026lsquo;자유의지\u0026rsquo;를 가졌다고 보기는 어렵습니다.\n행위성과 지능의 분리 튜링과 설의 논쟁이 \u0026lsquo;기계가 인간처럼 생각하거나 이해할 수 있는가\u0026rsquo;라는 질문에 집중하는 동안, 21세기의 AI 기술, 특히 대규모 언어 모델(LLM)의 등장은 새로운 철학적 관점을 요구하게 되었습니다. 이 새로운 관점을 가장 명확하게 제시한 인물 중 한 명이 철학자 루치아노 플로리디(Luciano Floridi)입니다. 그는 현대 AI가 우리에게 \u0026lsquo;행위성(agency)\u0026lsquo;과 \u0026lsquo;지능(intelligence)\u0026lsquo;의 분리를 강요한다고 주장합니다.\n플로리디에 따르면, 우리는 전통적으로 어떤 목표를 향해 성공적으로 문제를 해결하는 능력(행위성)은 그것을 수행하는 주체의 지능에 의해 뒷받침된다고 가정했습니다. 그러나 현대 AI는 이 가정을 깨뜨립니다. 체스를 두는 스마트폰이나 ChatGPT와 같은 시스템이 예시입니다. 이 시스템들은 특정 목표(체스에서 이기기, 질문에 적절한 답변 생성하기)를 달성하는 데 있어 인간을 능가하는 강력한 행위성을 보여줍니다. 그러나 플로리디는 이들이 인간적인 의미에서의 진정한 이해나 의식을 가진 지능은 \u0026ldquo;전혀(zero) 없다\u0026quot;고 단언합니다.\n이들의 능력은 실제 세계에 대한 이해에서 비롯된 것이 아니라, 방대한 데이터 속에서 통계적 패턴과 상관관계를 찾아내고 이를 활용하는 것입니다. ChatGPT는 단어의 의미를 이해해서가 아니라, 특정 단어 시퀀스 다음에 어떤 단어 시퀀스가 올 확률이 가장 높은지를 계산하여 그럴듯한 문장을 생성할 뿐입니다. 이는 설의 중국어 방 논증과도 일맥상통합니다. 즉, 이들은 정교한 구문적 조작을 통해 의미 있는 행동처럼 보이는 결과를 만들어내지만, 진정한 의미론적 이해는 결여되어 있습니다.\n플로리디의 관점은 튜링과 설의 논쟁을 우회하여 새로운 지평을 엽니다. 이제 핵심 질문은 \u0026ldquo;기계가 지능적인가?\u0026ldquo;가 아니라, \u0026ldquo;지능이 없는 강력한 행위성의 출현이 의미하는 바는 무엇인가?\u0026rdquo; 로 전환됩니다. 우리는 이제 지능적이지는 않지만, 우리 사회와 환경에 막대한 영향을 미칠 수 있는 자율적인 \u0026lsquo;인공적 행위자(artificial agents)\u0026lsquo;와 공존하는 시대를 맞이하고 있습니다.\n이러한 \u0026lsquo;행위성과 지능의 분리\u0026rsquo;는 AI 철학의 무게중심을 형이상학적 질문(의식, 이해의 본질)에서 윤리적이고 정치적인 질문으로 이동시킵니다. 지능이 없는 행위자에게 우리는 어떤 종류의 책임을 물을 수 있을까요? 이들의 행동을 어떻게 신뢰하고 통제할까요? 이들이 만들어내는 결과에 대한 도덕적 책임은 누구에게 있을까요? 플로리디의 분석은 인공지능이 제기하는 문제가 더 이상 \u0026lsquo;기계가 인간이 될 수 있는가\u0026rsquo;가 아니라, \u0026lsquo;인간이 이 새로운 종류의 비-인간 행위자와 어떻게 공존할 것인가\u0026rsquo;임을 명확히 보여줍니다.\n자유의지에 대한 관점들 르네 데카르트 (René Descartes): 데카르트는 자유의지를 생각하는 자아, 즉 비물질적인 영혼(res cogitans)의 핵심적인 능력으로 봅니다. 기계는 단지 물질적인 자동기계(res extensa)에 불과하므로, 자각적 자아나 영혼이 없는 인공지능은 진정한 자유의지를 가질 수 없다고 결론 내릴 가능성이 높습니다. 데이비드 흄 (David Hume): 흄은 결정론과 자유의지가 양립 가능하다고 보는 양립가능론(compatibilism)의 대표적인 인물입니다. 그에게 자유는 \u0026lsquo;강제가 아닌 자신의 욕구에 따라 행동하는 것\u0026rsquo;으로 정의됩니다. 이 관점에서 보면, 인공지능이 비록 결정론적인 프로그램에 따라 작동하더라도, 외부의 강제 없이 자신의 내적 상태(프로그래밍된 \u0026lsquo;욕구\u0026rsquo;)에 따라 행동한다면 일종의 \u0026lsquo;자유\u0026rsquo;를 가진다고 볼 수 있는 이론적 여지를 제공합니다. 임마누엘 칸트 (Immanuel Kant): 칸트에게 자유의지는 이성적 존재가 스스로에게 보편적인 도덕 법칙을 부여하고 따르는 \u0026lsquo;도덕적 자율성\u0026rsquo;과 깊이 연관됩니다. 인공지능이 자유의지를 가지려면, 단순히 계산적 이성을 갖는 것을 넘어 보편화 가능한 도덕 원칙을 이해하고 그것을 행위의 동기로 삼을 수 있는 능력을 갖추어야 합니다. 이는 인공지능에게 매우 높은 기준을 요구합니다. 존 설 (John Searle): 앞서 살펴본 바와 같이, 설은 인공지능이 기호를 조작할 수는 있지만 진정한 의미(semantics)를 이해하지 못한다고 주장합니다. 자유의지는 자신의 행위의 의미를 이해하고 의도하는 능력을 전제하므로, 현재의 인공지능은 자유의지를 가질 수 없다고 봅니다. 대니얼 데닛 (Daniel Dennett): 데닛은 인간의 자유의지를 신비로운 힘이 아니라 복잡한 정보처리 시스템의 결과물로 설명합니다. 인공지능이 인간의 뇌만큼 충분히 복잡한 시스템으로 발전한다면, 인간의 자유의지와 실용적으로 동등한, \u0026lsquo;가질 가치가 있는(worth wanting)\u0026rsquo; 종류의 자유의지를 가질 수 있다고 주장합니다. 루치아노 플로리디 (Luciano Floridi): 플로리디는 자유의지를 \u0026lsquo;책임(responsibility)\u0026lsquo;의 문제로 전환합니다. 그는 인공지능이 \u0026lsquo;자율적 행위자(autonomous agent)\u0026lsquo;는 될 수 있지만, 아직 \u0026lsquo;도덕적 주체(moral subject)\u0026lsquo;는 아니라고 봅니다. 즉, 인공지능의 행동에 대한 책임을 어떻게 분배할 것인가? 라는 실용적인 문제에 더 집중합니다. 철학자 핵심 개념 AI 자유의지에 대한 입장 핵심 논거 데카르트 심신 이원론 부정적 자유의지는 비물질적 영혼의 속성이며, 기계는 물질에 불과하다. 흄 양립가능론 조건부 긍정 가능 자유가 \u0026lsquo;외부 강제 없이 내적 욕구에 따르는 것\u0026rsquo;이라면, 결정론적 AI도 자유로울 수 있다. 칸트 도덕적 자율성 매우 회의적 자유의지는 보편적 도덕 법칙을 스스로에게 부여하는 이성적 능력과 결부되어 있다. 설 의미론/생물학적 자연주의 부정적 자유의지는 진정한 \u0026lsquo;이해\u0026rsquo;를 전제하지만, AI는 구문론적 조작만 할 뿐 의미를 이해하지 못한다. 데닛 정보처리 시스템 실용적 긍정 가능 자유의지는 복잡한 시스템의 산물이므로, 충분히 복잡한 AI는 인간과 유사한 자유의지를 가질 수 있다. 플로리디 책임 가능성 논점 전환 자유의지보다 \u0026lsquo;책임\u0026rsquo;의 문제로 접근해야 하며, AI는 자율적 행위자일 수 있으나 도덕적 주체는 아니다. 기속성, 욕구, 그리고 알고리즘적 자유의 한계 자유의지를 더 깊이 이해하기 위해 법학적, 심리학적 전제 조건들을 살펴보겠습니다. 한 법학적 분석에 따르면, 자유의지는 \u0026lsquo;자유\u0026rsquo;와 \u0026lsquo;의지\u0026rsquo;라는 두 요소로 구성됩니다. 여기서 \u0026lsquo;자유\u0026rsquo;는 그 원인을 물리적으로 규명할 수 없는 \u0026lsquo;예측 불가능성\u0026rsquo;으로 정의될 수 있습니다. 이런 관점에서 복잡한 인공지능의 출력값은 그 원인을 명확히 규명하기 어렵기 때문에 인간의 뇌처럼 \u0026lsquo;자유롭다\u0026rsquo;고 볼 여지가 있습니다.\n의지는 \u0026lsquo;동기\u0026rsquo;를 갖고 목적을 실현하려는 심적 상태를 의미하는데, 이 동기는 생존이나 번식과 관련된 1차적 욕구를 넘어선 호기심, 명예욕, 물질욕과 같은 \u0026lsquo;2차적 욕구\u0026rsquo;와 관련됩니다. 현재 인공지능은 생물학적 기반이 없기 때문에 이러한 내재적 욕구나 두려움을 갖지 않습니다. 따라서 알고리즘으로 구현된 목표는 가질 수 있어도, 진정한 의미의 \u0026lsquo;의지\u0026rsquo;를 갖기는 어렵습니다.\n더 나아가, 진정한 인간의 자율성은 \u0026lsquo;기속성(commitment)\u0026lsquo;이라는 개념을 가질 수 있습니다. 이는 \u0026lsquo;강요 없는 자기 강제\u0026rsquo;를 의미하며, 개인이 스스로 선택한 행위나 원칙이 자신에게 규범적인 효력을 가지는 상태를 말합니다. 인간의 자율성은 우리가 살아온 삶의 이야기, 즉 되돌릴 수 없는 과거와 유한한 미래(죽음)라는 한계 속에서 형성되는 서사적 정체성에 깊이 뿌리내리고 있습니다. 반면, 인공지능의 기억은 원리적으로 리셋되거나 포맷될 수 있으며, 생물학적 유한성에 묶여 있지 않습니다. 이러한 근본적인 차이 때문에 인공지능은 인간과 같은 \u0026lsquo;기속성\u0026rsquo;에 기반한 자율성을 갖기 어렵습니다.\n한편, \u0026ldquo;인공지능은 결정론적 알고리즘을 따르므로 자유의지가 없다\u0026quot;는 관점에서는, 인간의 뇌 역시 물리 법칙의 지배를 받는 인과적 시스템이며 인간의 자유 또한 생물학적, 사회적 조건에 의해 크게 제약된다고 볼 수 있습니다. 이 경우 인간과 기계의 차이는 절대적인 것이 아닌 정도의 차이일 수 있습니다.\n종합해 보면, 인공지능의 행위자성 문제는 세 가지 뚜렷한 층위에서 동시에 논의되고 있습니다.\n기능적 자율성(L1): 엔지니어링의 목표이며 외부 개입 없이 작업을 수행하는 능력 책임 귀속 가능한 행위자성(L2): 법적·윤리적 목표이며 특정 행위에 대해 책임을 물을 수 있는 주체의 문제 형이상학적 자유의지(L3) : 철학적·현상학적 개념이며 진정한 선택과 내적 경험의 문제 인공지능은 L1과 L2를 달성할 수 있을지 몰라도, L3를 달성하는 것은 전혀 다른 차원의 문제입니다.\n인간이 경험하는 진정한 의미의 행위자성은 역설적으로 인공지능이 본질적으로 결여하고 있는 \u0026lsquo;한계\u0026rsquo;를 요구하는 것처럼 보입니다. 인간의 선택이 의미와 무게를 갖는 것은 우리의 삶이 유한하고 과거가 돌이킬 수 없다는 \u0026lsquo;기속성\u0026rsquo; 때문입니다. 인간의 유한성과 달리 원리적으로 무한하고, 리셋 가능하며, 취약하지 않은 인공지능의 \u0026lsquo;완벽함\u0026rsquo;이 인간적 의미의 행위자성을 획득하는 데 가장 큰 장애물이 될 수 있습니다.\n인공지능은 어떻게 아는가? 인공지능의 데이터 기반 \u0026lsquo;학습\u0026rsquo;은 인간의 지식과 동일할까요? 아니면 근본적으로 다른, 어쩌면 더 제한된 방식의 습득일까요?\n귀납 엔진 - 데이터 기반 지식과 인간 경험론의 유사성 현대 인공지능, 특히 딥러닝과 순환신경망(RNN)과 같은 기술의 학습 방식은 매우 정교한 형태의 \u0026lsquo;귀납 추론(inductive reasoning)\u0026rsquo;입니다. 방대한 양의 구체적인 데이터(경험)로부터 일반적인 패턴과 원리를 도출합니다. 인간의 인식 또한 사물에 나타나는 패턴을 파악하고, 반복적인 관찰을 통해 일관된 인과 원리를 발견하는 귀납적 측면을 가지고 있으며, 이런 점에서 인공지능은 인간의 기본적 인식 방법을 모방한 것이라고 볼 수 있습니다.\n인공지능의 학습 방식은 모든 지식이 감각 경험에서 비롯된다고 주장하는 고전 철학의 \u0026lsquo;경험론(empiricism)\u0026rsquo; 또는 \u0026lsquo;감각론(sensationalism)\u0026lsquo;과 유사합니다. 경험론자에게 지식의 유일한 원천이 감각인 것처럼, 인공지능에게 지식의 유일한 원천은 학습 데이터입니다. 인공지능의 \u0026lsquo;감각\u0026rsquo;은 이미지, 텍스트, 소리 등의 데이터 입력이며, 이를 통해 세상을 \u0026lsquo;경험\u0026rsquo;합니다.\n이러한 데이터 의존성은 인공지능 지식의 강력한 힘인 동시에 치명적인 약점입니다. 경험론이 감각 경험의 범위를 넘어서는 추상적 개념을 설명하는 데 어려움을 겪는 것처럼, 인공지능의 지식은 훈련 데이터의 양과 질, 범위에 의해 제한됩니다. 편향되거나 불완전한 데이터를 학습한 인공지능은 편향된 지식을 갖게 되며, 이는 철학의 오랜 난제인 \u0026lsquo;귀납의 문제\u0026rsquo;가 현대 기술 환경에서 재현된 것이라 할 수 있습니다. 데이터 자체가 기록자의 주관성, 지식의 한계, 잘못된 관념 등에 의해 오염될 수 있기 때문에, 이로 인해 부정확하고 편향된 자료를 그대로 인지하여 오류의 결론에 도달할 위험이 있습니다.\n데이터를 넘어서 - 연역적 추론과 이론적 이해를 향한 탐색 인간의 지능은 귀납적인 과정으로만 이루어지지 않습니다. 우리는 일반적인 규칙을 특정 사례에 적용하는 \u0026lsquo;연역 추론(deductive reasoning)\u0026lsquo;과, 주어진 현상을 가장 잘 설명하는 가설을 추론하는 \u0026lsquo;귀추법(abductive reasoning)\u0026lsquo;을 사용합니다. 인간은 심지어 불완전하거나 희박한 데이터를 가지고도 이론이나 가설을 형성하고, 이를 통해 세계를 이해하려 시도합니다. 이는 현재의 데이터 기반 인공지능이 대부분 결여하고 있는 능력입니다.\n한 분석에서는 데이터라는 경험적 사실이 없으면 작동하지 못하는 현재의 AI는 귀납적 사고 단계에 있으며, 인간 사고의 근원은 연역적이라고 주장합니다. 사회 현상과 같이 복잡한 영역에서는 이론 없이는 인과관계를 파악하기 어렵습니다. 인공지능이 인간의 지적 능력에 근접하거나 뛰어넘기 위해서는 단순히 데이터에서 패턴을 찾는 것을 넘어, 세계에 대한 추상적인 이론적 모델을 구축하고 활용하는 연역적 논리체계를 갖추어야 합니다.\n또한 인간의 지식 획득 과정에는 \u0026lsquo;통찰(insight)\u0026rsquo; 또는 \u0026lsquo;직관(intuition)\u0026lsquo;이라는 비절차적이고 총체적인 이해가 존재합니다. 이는 경험과 지식에 의하지 않고 갑작스럽게 문제의 본질을 꿰뚫어 보는 능력으로, 알고리즘적으로 복제하기 매우 어려운 인간 고유의 인식 능력으로 여겨집니다.\n현재 인공지능의 인식론적 특징은 철학적 경험론의 극단적 형태로 볼 수 있습니다. AI는 일종의 \u0026lsquo;초경험주의자(hyper-empiricist)\u0026lsquo;로서, 방대한 데이터를 통해 귀납적으로 학습하는 능력의 강점과, 이론적·연역적 틀이 부재할 때 나타나는 취약성을 동시에 보여줍니다.\n인간 수준의 범용 인공지능(AGI)으로 나아가는 길은 단순히 더 많은 데이터와 더 빠른 연산 능력의 문제가 아니라, 근본적인 인식론적 도전 과제입니다. 이는 귀납 엔진에서 연역적·이론적 엔진으로의 전환, 즉 세계로부터 배우는 것에서 세계에 대해 추론하는 것으로의 패러다임 전환이 AGI 달성의 핵심 과제입니다.\n인공지능의 도덕성 AI에 대한 심각한 윤리적 질문들이 제기되고 있습니다. 이들은 과연 도덕적 판단을 내릴 수 있을까요? 그들의 행동에 대한 책임은 누구에게 있을까요? 그리고 인간 사회의 편견을 그대로 학습한 인공지능이 만들어낼 불평등은 어떻게 해결해야 할까요?\n도덕적 행위자라는 질문 AI가 도덕적 행위자(moral agent)가 될 수 있을까요? 그 전에 먼저 \u0026lsquo;도덕적 행위자\u0026rsquo;는 무엇일까요? 철학의 오랜 전통 속에서 도덕적 행위자는 단순히 행동하는 존재를 넘어, 자신의 행동에 대해 도덕적 책임을 질 수 있는 존재로 간주됩니다. 이를 위해서는 일반적으로 의도성(intentionality), 자유 의지(free will), 그리고 의식(consciousness) 과 같은 능력이 전제돼야 합니다.\n이러한 전통적인 관점에서 볼 때, 현재의 AI는 도덕적 행위자가 될 수 없습니다. AI는 프로그래밍된 목표와 데이터에 따라 작동할 뿐, 진정한 의미의 자유로운 선택을 하지 못합니다. 현재의 AI는 자유 의지나 의식을 가지고 있지 않으므로, 그들의 행동에 대한 도덕적 칭찬이나 비난의 주체가 될 수 없다는 주장입니다.\n그러나 AI 기술은 계속 발전하고 있으며 자율주행차, 의료 진단 AI, 자율 무기 등 도덕적 딜레마 상황에 놓이는 시스템들이 현실에 등장하고 있습니다. 이에 따라 철학계에서는 보다 실용적이고 기능적인 접근법을 고민하고 있습니다. 바로 \u0026lsquo;인공적 도덕 행위자(Artificial Moral Agent, AMA)\u0026rsquo; 또는 \u0026lsquo;기능적 도덕 행위자(functional moral agent)\u0026lsquo;라는 개념입니다.\n이 관점은 AI가 인간과 같은 의식이나 자유 의지를 소유하는지와는 별개로, 도덕적 판단의 \u0026lsquo;기능\u0026rsquo;을 수행할 수 있는지에 주목합니다. AI 시스템이 특정 상황의 도덕적으로 중요한 측면들(예: 공리주의적 결과, 의무론적 규칙)을 인식하고, 이를 자신의 의사결정 알고리즘에 통합하여 행동할 수 있다면, 비록 의식은 없더라도 \u0026lsquo;기능적\u0026rsquo;으로 도덕적 행위자 역할을 할 수 있다는 것입니다. 예를 들어, 자율주행차는 \u0026lsquo;인명 피해를 최소화하라\u0026rsquo;는 규칙을 입력받고, 충돌이 불가피한 상황에서 여러 선택지 중 이 규칙에 가장 부합하는 행동을 선택하도록 프로그래밍될 수 있습니다.\n이러한 접근은 AI 윤리 논쟁의 초점을 \u0026ldquo;기계가 영혼을 가질 수 있는가?\u0026ldquo;라는 추상적인 질문에서 \u0026ldquo;우리는 어떻게 기계가 도덕적인 것처럼 행동하도록 설계할 수 있는가?\u0026ldquo;라는 구체적이고 공학적인 질문으로 전환시킵니다.31 이는 AI가 진정한 도덕적 주체가 될 수 있는지에 대한 최종적인 답을 내리지 않고도, 당면한 윤리적 문제들을 다룰 수 있는 실용적인 길을 제시합니다. 그러나 이 기능적 접근법 역시 한계가 있습니다. 이는 도덕성의 인지적 측면만을 다룰 뿐, 공감이나 연민 등 감정적 측면을 포괄하지 못하며, 무엇보다도 이 기능적 행위자가 오류를 범했을 때 그 책임을 누구에게 물어야 하는가라는 더 복잡한 문제가 있습니다.\n책임의 공백 자율적인 인공지능 복잡한 환경에서 예측 불가능한 방식으로 행동하다가 해로운 결과를 초래했을 때, 그 책임은 누구에게 있을까요? 이 질문은 \u0026lsquo;책임의 공백(responsibility gap)\u0026lsquo;이라는 심각한 윤리적, 법적 난제를 나타냅니다. 책임의 공백은 다음과 같은 여러 요인으로 인해 발생합니다:\n예측 불가능성과 불투명성(Opacity): 딥러닝과 같은 현대 AI 시스템, 특히 \u0026lsquo;블랙박스\u0026rsquo; 모델들은 스스로 학습하고 진화하기 때문에 개발자조차 시스템의 모든 행동을 예측하거나 그 결정 과정을 완벽하게 이해하기 어렵습니다. 개발자가 결과를 예측할 수 없었다면, 그에게 전적인 책임을 묻기 어렵습니다.\n통제력의 부재: 사용자는 인공지능에게 목표를 부여할 뿐, 그 목표를 달성하기 위한 구체적인 행동 하나하나를 통제하지 않습니다. 사용자가 시스템의 작동을 완전히 통제할 수 없다면, 그 결과에 대한 책임 역시 온전히 지기 어렵습니다.\n행위자성의 부재: 인공지능 자체는 현재로서는 도덕적 책임을 질 수 있는 주체, 즉 진정한 도덕적 행위자로 인정받지 못합니다.\n다수 행위자 문제(Problem of Many Hands): AI 시스템의 개발, 배포, 운영에는 프로그래머, 데이터 과학자, 기업, 사용자 등 수많은 인간 행위자들이 관여합니다. 피해가 발생했을 때, 이 복잡한 네트워크 속에서 특정 개인이나 집단에게 책임을 명확히 귀속시키기가 매우 어렵습니다.\n이러한 책임의 공백은 사회 정의와 신뢰의 기반을 뒤흔들 수 있습니다. 피해자는 있는데 책임지는 사람이 아무도 없을 수 있습니다. 이 문제를 해결하기 위해 다양한 철학적, 기술적 접근법이 제시되고 있습니다.\n어떤 학자들은 \u0026lsquo;책임의 과잉(responsibility abundance)\u0026lsquo;이라는 개념을 제시하며, 책임질 사람이 없는 것이 아니라 관련된 모든 인간 행위자(개발자, 기업, 규제 기관, 사용자 등)가 일정 부분 책임을 공유해야 한다고 주장합니다. 또 다른 접근법은 \u0026lsquo;의미 있는 인간 통제(Meaningful Human Control)\u0026lsquo;의 원칙을 강조합니다. 이는 AI 시스템의 설계 단계부터 인간이 시스템의 작동을 이해하고, 예측하며, 필요시 개입할 수 있는 기술적, 절차적 장치를 마련해야 한다는 것입니다.\n최근에는 책임의 개념 자체를 재정의하려는 시도도 있습니다. 전통적인 책임론이 과거의 행위에 대한 비난(후향적 책임)에 초점을 맞춘다면, 새로운 접근법은 미래에 더 책임감 있는 행위자를 육성하는 것(전향적 책임)에 중점을 둡니다. 책임 관행의 목표는 단순히 범인을 찾는 것이 아니라, AI와 관련된 모든 인간 행위자들이 더 나은 도덕적 판단을 내리도록 유도하는 \u0026lsquo;행위 주체 육성(agency cultivation)\u0026lsquo;에 있다는 것입니다.\n더 근본적으로는 \u0026lsquo;취약성 격차(vulnerability gap)\u0026lsquo;라는 문제가 지적되기도 합니다. 인간 사회의 책임 관행은 비난, 칭찬, 분노와 같은 \u0026lsquo;반응적 태도\u0026rsquo;에 기반하며, 이는 행위자들이 서로의 감정과 평가에 \u0026lsquo;취약하다\u0026rsquo;는 것을 전제로 합니다. 그러나 기계인 인공지능은 이러한 도덕적 감정에 취약하지 않습니다. 이러한 비대칭성은 우리의 도덕적 생태계 자체를 교란시킬 수 있으며, 이 문제를 해결하는 것이 책임 격차를 메우는 핵심 과제라는 주장입니다. 결국 책임의 공백 문제는 AI 시대에 맞는 새로운 사회적 합의와 법적, 윤리적 프레임워크의 구축이 시급함을 보여줍니다.\n알고리즘의 그림자 - 편향과 공정성 인공지능이 마주하는 윤리적 문제 중 대표적인 위협 중 하나로 알고리즘 편향(algorithmic bias)이 있습니다. 인공지능은 데이터를 통해 세상을 학습합니다. 만약 그 학습 데이터가 인종, 성별, 사회경제적 지위 등에 대한 인간 사회의 역사적, 구조적 편견을 담고 있다면, AI는 그 편견을 그대로 학습하고, 심지어 증폭시켜 체계적으로 차별적인 결과를 낳습니다. 알고리즘 편향은 AI 개발의 여러 단계에서 발생할 수 있습니다:\n데이터 편향: AI를 훈련시키는 데이터 자체가 특정 집단을 과소 또는 과대 대표하거나, 과거의 차별적인 패턴을 반영하고 있을 때 발생합니다. 예를 들어, 과거 남성 위주로 채용했던 기업의 데이터를 학습한 채용 AI가 있다면, 여성 지원자에게 불리한 평가를 내릴 가능성이 높습니다.\n모델 및 알고리즘 편향: 알고리즘이 특정 변수에 부적절한 가중치를 부여하거나, 모델의 설계 자체가 특정 집단에 불리하게 작용할 때 발생합니다.\n인간의 편향: 개발자나 데이터 레이블러의 무의식적인 편견이 데이터 선택, 특징 공학, 모델 평가 과정에 개입하여 편향을 유발할 수 있습니다.\n이러한 편향은 사회 전반에 심각한 해악을 끼칩니다:\n고용 차별: AI 채용 도구가 특정 성별이나 인종의 지원자를 체계적으로 배제할 수 있습니다.\n불공정한 사법 시스템: 재범 위험을 예측하는 AI가 소수 인종 커뮤니티를 부당하게 높은 위험군으로 분류하여, 기존의 사법적 불평등을 심화시킬 수 있습니다.\n의료 불평등: 특정 인종 집단의 데이터가 부족한 의료 진단 AI는 해당 집단의 환자들에게 오진을 내리거나 부정확한 치료 계획을 제시할 수 있습니다.\n금융 소외: 신용 평가 모델이 특정 지역이나 인구 집단에 대해 부당하게 낮은 점수를 부여하여 대출이나 금융 서비스 접근 기회를 박탈할 수 있습니다.\n이러한 문제를 해결하기 위해 \u0026lsquo;공정성(fairness)\u0026lsquo;을 확보하려는 노력하고 있지만, 공정성을 정의하고 측정하는 것 자체가 또 다른 난제입니다. 예를 들어, \u0026lsquo;집단 공정성(group fairness)\u0026lsquo;은 대출 승인율과 같은 긍정적인 결과가 모든 인구 집단(인종, 성별 등)에 걸쳐 동일해야 한다고 요구합니다. 반면, \u0026lsquo;개인 공정성(individual fairness)\u0026lsquo;은 비슷한 자격 조건을 가진 개인들은 그들의 인구 통계학적 특성과 관계없이 비슷한 결정을 받아야 한다고 주장합니다. 문제는 이 두 가지 공정성 기준이 종종 서로 상충하여 동시에 만족시키기 어렵다는 것입니다.\n따라서 알고리즘 편향 문제를 해결하기 위해서는 어떤 종류의 공정성을 추구할 것인지에 대한 사회적, 철학적 합의가 필요합니다. 이를 위해 다양한 대표성을 가진 데이터셋을 구축하고, 공정성을 고려하여 알고리즘을 설계하며, AI 시스템 배포 후에도 지속적인 감사와 모니터링을 통해 편향을 탐지하고 완화하려는 노력이 필수적입니다.40 결국 AI 윤리 논쟁은 추상적인 영역에서 벗어나, 이처럼 구체적인 사회 정의 문제에 대한 실용적인 해결책을 찾는 방향으로 나아갑니다. 이는 AI의 개발이 더 이상 순수한 기술의 영역이 아닌, 가치와 정치, 철학이 깊이 개입된 사회적 실천임을 보여줍니다.\n에이전트의 경제 및 정치적 영향 인공지능의 등장은 인류 문명의 근간을 이루는 사회, 경제, 정치 구조와 인간관계의 본질 자체를 변화시킬 잠재력을 가지고 있습니다. 최근 언급되고 있는 주제를 살펴보겠습니다.\n경제적 영향: 노동의 종말과 초월적 행위성의 도래 인공지능은 산업혁명 이후 가장 큰 노동 시장의 격변을 예고합니다. 이전의 자동화가 주로 육체적, 반복적 작업을 대체했다면, 인공지능은 보고서 작성, 데이터 분석, 코딩, 고객 응대와 같은 인지적이고 비정형적인 작업까지 자동화할 수 있습니다. 이로 인해 행정, 금융, 법률 등 화이트칼라 직업군이 상당한 영향을 받을 것으로 예측되며, 골드만삭스는 전 세계적으로 약 3억 개의 일자리가 생성형 AI의 영향을 받을 수 있다고 추정했습니다.\n그러나 이러한 변화는 단순히 일자리 소멸만 의미하지 않습니다. 동시에 인공지능은 인간의 생산성을 극적으로 향상시킵니다. AI는 지식 습득의 장벽을 낮추고, 복잡한 문제 해결을 도우며, 개인이 이전에는 상상할 수 없었던 규모의 작업을 수행할 수 있습니다.\n이러한 양면성은 노동 시장의 양극화를 심화시킬 위험이 있습니다. AI를 효과적으로 활용하는 고숙련 노동자의 생산성과 임금은 급격히 상승하는 반면, AI에 의해 대체 가능한 기술을 가진 노동자들은 일자리를 잃거나 저임금 일자리로 내몰리면서 경제적 불평등은 더욱 심화될 우려가 있습니다. 이는 기본소득, 재교육 시스템, 부의 재분배 등 새로운 사회 안전망에 대한 근본적인 논의를 요구합니다.\n정치적 영향: 감시 사회와 디지털 권위주의 인공지능 기술의 이면에는 전례 없는 사회 통제와 감시의 가능성이 있습니다. 대표적인 사례로 중국의 사회 신용 시스템이 있습니다. 이 시스템은 인공지능를 활용하여 시민들의 온라인 및 오프라인 활동, 금융 거래, 법규 준수 여부 등 방대한 데이터를 수집하고 분석합니다. 그리고 이를 바탕으로 개인의 \u0026lsquo;신용 점수\u0026rsquo;를 매겨 보상(대출 우대, 여행 허가)이나 불이익(대출 제한, 여행 금지)을 가합니다.\nAI는 안면 인식, 걸음걸이 인식 등 생체 정보 기술을 통해 개인을 식별하고 추적하며, 방대한 데이터를 실시간으로 처리하여 행동 패턴을 분석하고 점수를 매깁니다. 이는 국가가 시민의 모든 행동을 감시하고, 국가가 원하는 방향으로 행동을 유도하는 강력한 사회 통제 메커니즘의 가능성이 있습니다.\n인공지능이라는 동일한 기술이 한편에서는 개인의 역량을 강화하는 \u0026lsquo;초월적 행위성\u0026rsquo;의 도구가 되고, 다른 한편에서는 개인을 억압하는 \u0026lsquo;감시 체제\u0026rsquo;의 엔진이 될 수 있다는 이중성은 이 기술의 가치가 중립적이지 않으며, 그것을 사용하는 사회의 가치와 권력 구조에 따라 그 결과가 극명하게 달라진다는 점을 보여줍니다.\n마치며 저도 인간 뇌의 모든 작용을 규칙으로 구현하면 이는 사람으로 대해야 하지 않을까? 라는 생각을 한 적이 있기에 기호주의 인공지능에 공감했고, 이외의 인공지능을 향한 여러 관점이 재밌었습니다.\n포스트를 작성하는 시점의 제 생각을 말해보면, 현재의 LLM과 같은 약한 인공지능의 구현 수준은 강력한 도구이며, 강한 인공지능의 정의 수준까지 구현된다면 이는 새로운 존재로 인식하고 존중을 가져야 한다고 생각합니다.\n또한 현재의 귀납 추론만 가능한 수준을 넘어 연역 추론이나 귀추법을 통해 스스로 미지를 나아가는 수준에 도달했을 때, 행동에 의미를 가지면서 도구를 넘어 스스로 존재한다고 생각합니다.\n이외에 AI의 사회 통제 메커니즘 사례를 보며, 인공지능 엄청난 생산성을 가져올 수 있지만 생각지 못한 경우가 우려되어, 개발 시에도 더 신중하게 고민해야겠다고 생각했습니다.\n본 포스트는 Google Gemini의 응답을 기반으로 저의 의견을 반영하여 다시 작성했습니다.\n","date":"2025-08-04T14:17:00Z","permalink":"https://yeonhl.github.io/theories/ai/genealogy/","title":"인공지능의 계보"},{"content":" 요약\n언어의 5가지 핵심 원리\n음운론: 소리의 조직, 의미를 구별하는 규칙 등 언어의 \u0026lsquo;소리 체계\u0026rsquo;를 연구하는 학문 형태론: 의미를 가진 가장 작은 단위\u0026rsquo;인 형태소(morpheme)가 결합하여 단어를 형성하는 원리를 연구하는 학문 통사론: 구(phrase)와 문장(sentence)을 만드는 규칙과 구조를 연구하는 학문 (특히 어순 (word order)) 의미론: 단어와 문장의 \u0026lsquo;문자적, 사전적 의미\u0026rsquo;를 연구하는 학문 화용론: \u0026lsquo;특정 상황과 맥락 속에서\u0026rsquo; 언어가 실제로 어떻게 사용되고 해석되는지를 연구하는 학문 언어의 핵심 원리 개요 언어학 (Linguistics) 페이지는 언어의 효율적인 학습을 목표로 각 개념에 접근합니다. 언어를 단순 암기가 아닌 언어라는 시스템의 작동 원리인 \u0026lsquo;언어학적 원리\u0026rsquo;를 학습하여 \u0026lsquo;무엇을\u0026rsquo; 배울지 뿐만 아니라, \u0026lsquo;왜\u0026rsquo; 그렇게 배워야 하는지를 이해하며 메타언어적 인식 (Metalinguistic Awareness)1 능력의 향상을 목표로 합니다.\n이번 포스트는 언어를 구성하는 5가지 핵심 원리가 무엇이고, 어떤 특징을 갖는지 작성합니다. 각 원리의 구체적인 내용은 다음 포스트에서 다룹니다.\n음운론(Phonology) 음운론 (Phonology) 은 언어의 \u0026lsquo;소리 체계\u0026rsquo;를 연구하는 학문입니다. 소리를 물리적으로 분석하는 음성학(phonetics)과 달리, 특정 언어 내에서 소리가 어떻게 조직되고, 어떤 규칙에 따라 의미를 구별하는 기능을 하는지에 초점을 맞춥니다. 이는 언어의 가장 기본적인 구성 요소 \u0026lsquo;소리\u0026rsquo;의 설계도를 이해하는 것과 같습니다.\n음운론의 국제음성기호 (IPA), 최소대립쌍, 운율에 대해 간략하게 설명하겠습니다.\n국제음성기호 (International Phonetic Alphabet, IPA) 국제음성기호 (International Phonetic Alphabet, IPA) 는 모든 언어의 소리를 기호와 1:1로 대응시키는 과학적 도구입니다.\nIPA는 발음의 모호함을 제거합니다. 예를 들어 영어의 철자와 실제 발음은 매우 불일치하는 경우가 많습니다. 알파벳 \u0026lsquo;a\u0026rsquo;는 cat의 [æ], father의 [ɑ:], about의 [ə] 등 다양한 소리로 발음됩니다. 파닉스(Phonics) 는 소리 환경에 충분히 노출된 원어민에게는 유효할 수 있지만, 한국어와 같이 철자와 발음이 비교적 일치하는 언어를 모국어로 사용하는 학습자에게는 이해하기 어려울 수 있습니다.\n학습 초기에 목표 언어의 IPA 차트를 확보하고, 각 기호가 어떤 조음 위치(예: 양순음, 치경음)와 조음 방식(예: 파열음, 마찰음)으로 소리 나는지 원리를 이해하며 따라하면 학습에 큰 도움이 됩니다. 예를 들어, \u0026lsquo;p\u0026rsquo; 소리를 낼 때 단순히 \u0026lsquo;ㅍ\u0026rsquo; 소리를 흉내 내는 것을 넘어, \u0026lsquo;두 입술을 닫아(폐쇄) 공기의 흐름을 막았다가 터뜨리는(파열) 무성음\u0026rsquo;이라는 원리를 인지하는 것입니다. 이 과정 자체가 자신의 구강 구조를 의식하는 메타언어적 훈련이며, 정확한 소리를 만드는 물리적 기반입니다.\n물론 IPA가 완벽한 시스템은 아닙니다. 비전문가에게는 복잡하게 느껴질 수 있고, 미국 사전 편찬계 등 일부 분야에서는 보편적으로 채택되지 않았습니다. 그리고 라틴 알파벳에 기반한 역사적 배경으로 인해 서유럽 언어의 음성 체계에 다소 편향되어 있다는 비판도 존재합니다. 여러 문자와 언어별 발음을 익혀서 문자별 표기법으로 발음을 어림잡는 것이 받아들이기 더 쉬울 수 있습니다.\n언어학 (Linguistics) 페이지에서는 IPA에 대해 더 자세히 다룰 예정입니다. 각 언어에 맞는 원리나, 발음 표기법은 해당 언어의 페이지에서 다루겠습니다.\n최소대립쌍 (Minimal Pairs) 최소대립쌍 (Minimal Pairs) 는 단 하나의 소리(음소) 차이로 인해 의미가 구별되는 단어의 쌍을 의미합니다. 예를 들면 영어의 rice (/raɪs/)와 lice /laɪs/, 한국어의 \u0026lsquo;달\u0026rsquo;, \u0026lsquo;탈\u0026rsquo;, \u0026lsquo;딸\u0026rsquo;이 있습니다. 이는 학습자가 모국어에 존재하지 않아 구분하기 어려운 소리들을 명확히 인지하고 정확하게 발음하는데 효과적인 방법 중 하나입니다.\n한국어가 모국어인 경우 특히 유성음과 무성음의 대립(/b/ vs. /p/, /v/ vs. /f/), 또는 유음의 대립(/l/ vs. /r/)이 어렵게 느낄 수 있습니다. 이러한 음소 쌍에 대한 최소대립쌍 목록을 인터넷에서 찾아 반복적으로 듣고 따라 말하며 자신의 발음을 녹음하여 원어민의 소리와 비교하며 연습한다면 이해에 도움이 됩니다.\n영어로 예를 들면 minimal pairs f p와 같이 검색하여 찾아볼 수 있습니다.\n운율 (Prosody) 운율 (Prosody) 은 문장 전체의 음악성 (억양, 강세, 리듬)을 의미합니다. 언어에서 개별 소리의 정확성만큼 중요한 의미를 가집니다.\n영어와 같은 강세 기반 언어(stress-timed language) 는 의미를 전달하는 핵심어인 내용어(명사, 동사, 형용사 등)에 강세를 주고, 문법적 기능을 하는 기능어(전치사, 관사, 접속사 등)는 약하고 빠르게 발음하는 독특한 리듬을 가집니다.\n반면, 한국어는 각 음절이 비슷한 길이로 발음되는 음절 기반 언어(syllable-timed language) 에 가까워, 단어 내 특정 음절의 강세보다는 문장 전체의 억양 흐름이 더 중요합니다.\n이러한 근본적인 리듬 체계의 차이를 이해하지 못하면, 아무리 개별 단어를 정확하게 발음해도 원어민에게는 어색하게 들릴 수 있습니다.\n쉐도잉(shadowing) 을 통한 운율 학습 시 무작정 따라 하기보다는 분석적으로 접근해야 합니다. 문장을 듣고 따라 말하기 전, 어느 단어에 강세가 오는지, 문장의 종류(평서문, 의문문 등)에 따라 문장 끝의 억양이 어떻게 변하는지(하강조, 상승조) 등을 미리 분석하고 표시한 후 연습하는 것이 효과적입니다.\n음운론적 접근의 의미 음운론적 접근은 단순히 말하기 능력만을 위한 것이 아닙니다. \u0026ldquo;들리지 않으면 말할 수 없다\u0026rdquo; 이전에는 \u0026ldquo;소리를 구별하지 못하면 들리지 않는다\u0026rdquo;는 단계가 존재합니다. 모국어에 없는 소리(예: 한국어 화자에게 영어의 /θ/와 /s/)는 뇌에서 같은 소리 범주로 처리되는 경향이 있습니다. 이로 인해 think와 sink는 물리적으로 다른 소리임에도, 훈련되지 않은 귀에는 거의 동일하게 들릴 수 있습니다.\nIPA 학습과 최소대립쌍 훈련은 이러한 뇌의 소리 지도를 목표 언어에 맞게 재편성하는 과정입니다. 목표 언어의 음소 체계에 맞는 새로운 \u0026lsquo;소리 서랍장\u0026rsquo;을 만드는 것과 같습니다. 정확한 발음 훈련은 듣기 능력의 근본적인 토대를 구축하는 작업이며, 학습 초기 단계에서 최우선 순위가 되어야 하는 이유가 됩니다.\n형태론(Morphology) 형태론 (Morphology) 은 \u0026lsquo;의미를 가진 가장 작은 단위\u0026rsquo;인 형태소(morpheme)가 결합하여 단어를 형성하는 원리를 연구하는 학문입니다. 형태소는 스스로 자립하여 사용될 수 있는 어근(root) 과, 어근에 붙어 의미를 더하거나 품사를 바꾸는 접사(affix: 접두사, 접미사) 로 나뉩니다.\n어원(Etymology)과 접사 많은 단어들은 소수의 공통된 어근과 접사의 조합으로 구성됩니다. 예를 들어, 라틴어 어근 port(나르다)를 알면 import(안으로 나르다→수입하다), export(밖으로 나르다→수출하다), transport(가로질러 나르다→수송하다), portable(나를 수 있는→휴대용의) 등 수많은 파생 단어를 유기적으로 이해할 수 있습니다.\n이 전략을 효과적으로 실행하기 위해서는 다음의 단계적 접근이 필요합니다.\nun-(반대), re-(다시), -able(가능한), -tion(명사형) 등 빈번하게 사용되는 핵심 접두사와 접미사 목록을 학습합니다. 모르는 단어를 만났을 때, 온라인 어원 사전(예: Online Etymology Dictionary) 등을 활용하여 단어의 뿌리와 역사적 변천 과정을 학습합니다. 단어를 보면 어근과 접사로 분해하는 연습을 합니다. 예를 들어, unbelievably라는 단어를 un-(not) + believe(믿다) + -able(할 수 있는) + -ly(부사형)으로 분석할 수 있습니다. 이러한 접근법은 단순 암기와 달리 \u0026lsquo;모르는 단어\u0026rsquo;에 대처할 수 있습니다. 처음 보는 단어도 어근이나 접사를 통해 의미를 \u0026lsquo;추론\u0026rsquo;할 수 있기 때문입니다. 예를 들어, circumnavigate라는 단어를 처음 보더라도, circum-(주위에)과 navigate(항해하다)라는 형태소를 알면 \u0026lsquo;주위를 항해하다\u0026rsquo;로 추론할 수 있고 실제 뜻인 \u0026lsquo;세계 일주하다\u0026rsquo;라는 의미를 유추할 수 있습니다.\n통사론(Syntax) 통사론 (Syntax) 은 단어들이 결합하여 구(phrase)와 문장(sentence)을 만드는 규칙과 구조를 연구하는 분야입니다. 특히 문장 성분(주어, 목적어, 서술어 등)의 배열 순서, 즉 어순(word order)이 핵심입니다.\n언어 유형론(Linguistic Typology) 기반 어순 차이 세계의 언어들은 주어(Subject), 목적어(Object), 동사(Verb)의 기본 어순에 따라 크게 몇 가지 유형으로 나뉩니다. 한국어, 일본어, 튀르키예어 등은 SOV(주어-목적어-동사) 유형에 속하며, 영어, 중국어, 프랑스어 등은 SVO(주어-동사-목적어) 유형에 속합니다. 이 차이는 단순히 단어 배열 순서의 차이를 넘어, 언어 전체의 구조적 특성과 긴밀하게 연결됩니다.\nSOV 언어인 한국어는 동사가 문장의 가장 마지막에 위치하고, \u0026lsquo;은/는\u0026rsquo;, \u0026lsquo;을/를\u0026rsquo;과 같은 조사가 발달하여 어순이 비교적 자유롭습니다. 또한, 수식어는 항상 수식을 받는 말 앞에 위치합니다 (예시: 서울에 사는 친구)\n반면, SVO 언어인 영어는 동사가 주어와 목적어 사이에 위치하며, 명사 뒤에 붙는 조사가 없는 대신 위치가 중요해져 어순이 비교적 고정적입니다. 문법적 관계를 나타내기 위해 전치사가 발달했으며, 수식어(특히 관계대명사절)가 수식을 받는 말 뒤에 오는 경우가 많습니다. (예시: a friend who lives in Seoul).\n핵심 문장 패턴(Sentence Patterns) 집중 훈련 모든 문장은 새롭게 생성될 수 있지만, 반복적으로 사용되는 소수의 핵심 패턴이 존재합니다. 초급 학습 단계에서는 수많은 개별 문법 규칙을 파편적으로 외우기보다, 영어의 5형식과 같이 가장 빈번하게 사용되는 핵심 패턴을 하나의 덩어리(chunk)로 통째로 익히고 그 안의 단어만 바꿔 응용하는 방식이 훨씬 효과적입니다.\n이를 위해 SV, SVO 등 가장 기본적인 문형부터 시작하여, 각 패턴에 해당하는 예문을 직접 만들어보면 좋습니다. I want to..., I think that..., Can I get...? 등과 같이 활용도 높은 표현 덩어리를 정해두고, 그 패턴을 사용하여 다양한 문장을 생성해보세요.\n통사론적 접근의 의미 단순히 암기한 문장의 수를 늘리는 것을 넘어, \u0026lsquo;창의적 문장 생성\u0026rsquo; 능력의 기반을 다지는 과정입니다. 문장을 통째로 \u0026lsquo;암기\u0026rsquo;하는 것으로는 실제 상황에 유연하게 대처할 수 없습니다. 핵심 패턴 및 수식어구나 절 추가 방법, 문장의 논리적 연결 방법 등 문장 구조의 원리를 알게 되면, 암기하지 않은 새로운 문장도 스스로 \u0026lsquo;생성\u0026rsquo;해낼 수 있습니다.\n이는 언어를 능동적으로 다루는 전환점입니다. 문장 패턴 학습은 단순히 회화 표현을 익히는 것이 아니라, 목표 언어의 통사적 규칙을 내재화하는 \u0026lsquo;생성 문법(Generative Grammar)\u0026rsquo; 능력을 기르는 핵심적인 과정입니다.\n의미론(Semantics)과 화용론(Pragmatics) 의미론(Semantics) 과 화용론(Pragmatics) 은 언어의 의미를 연구하는 학문입니다. 의미론은 단어와 문장의 \u0026lsquo;문자적, 사전적 의미\u0026rsquo;를 연구하는 학문이고, 화용론은 더 나아가 \u0026lsquo;특정 상황과 맥락 속에서\u0026rsquo; 언어가 실제로 어떻게 사용되고 해석되는지를 연구하는 학문입니다.\n의미론이 \u0026ldquo;창문이 열려 있다\u0026quot;는 문장의 객관적인 상태 자체를 다룬다면, 화용론은 한겨울에 상대방이 이 말을 했을 때 \u0026ldquo;춥다\u0026quot;는 불평이나 \u0026ldquo;창문을 닫아달라\u0026quot;는 요청의 의미로 해석될 수 있음을 설명합니다. 즉, 화용론은 화자, 청자, 발화 의도, 공유된 지식, 사회문화적 배경 등 모든 비언어적 요소를 포괄하는 종합적인 학문입니다.\n\u0026lsquo;문법적으로 완벽하지만 어색한\u0026rsquo; 언어 언어 학습의 어려움은 문법의 정확성보다 화용론적 적절성에서 발생하는 경우가 많습니다. 문법적으로는 완벽하지만 상황에 맞지 않는 말을 해서 오해를 사거나 무례하게 비치는 경우가 대표적입니다. 예를 들어, 한국어를 배우는 외국인이 회사 상사에게 친근감의 표시로 \u0026ldquo;밥 먹었어?\u0026ldquo;라고 묻는 것은 문법적으로는 틀리지 않지만, 한국의 경어법 문화에서는 매우 부적절합니다. 이는 언어가 단순히 정보를 전달하는 기호의 나열이 아니라, 요청, 약속, 거절 등과 같은 사회적 행위(speech act)를 수행하는 도구이기 때문입니다.\n성공적인 의사소통을 위해서는 다음과 같은 화용론적 학습 전략이 필요합니다.\n문화적 맥락 이해: 언어는 문화를 담고 있으므로, 목표 언어가 사용되는 문화권의 가치관, 사회적 규범(예: 공손함의 표현 방식, 직설적 화법과 간접적 화법의 선호도 등)을 언어와 함께 학습해야 합니다. 상황별 언어 사용 관찰: 영화, 드라마, 리얼리티 쇼와 같이 실제 대화가 나타나는 자료를 활용하여, 동일한 의도(요청, 거절, 사과 등)가 대화 참여자의 관계(격식/비격식, 친소 관계)나 상황에 따라 어떻게 다르게 표현되는지를 적극적으로 관찰하고 분석해야 합니다. 화용적 인식(Pragmatic Awareness)의 의식적 향상: 학습 과정에서 \u0026ldquo;이 표현은 누가, 누구에게, 어떤 상황에서, 왜 사용하는가?\u0026ldquo;라는 의문을 가져야 합니다. 이는 모국어와 목표어의 화용적 차이를 명확히 인지하고, 모국어의 화용 규칙을 목표어에 그대로 적용하는 \u0026lsquo;화용적 전이(pragmatic transfer)\u0026rsquo; 오류를 줄일 수 있습니다. 화용론의 대두 과거 언어학에서는 음운론, 형태론, 통사론 등 형식적인 규칙으로 설명하기 어려운 복잡한 언어 현상들을 \u0026lsquo;화용론적 쓰레기통(pragmatic wastebasket)\u0026lsquo;으로 취급하는 경향이 있었다고 합니다. 그만큼 화용론이 비체계적이고 주변적인 분야로 취급되었음을 보여줍니다.\n그러나 현대 언어학에서는 화용론을 언어가 본질적으로 가질 수밖에 없는 \u0026lsquo;언어학적 미결정성(linguistic underdeterminacy)\u0026rsquo;, 즉 중의성을 해결하는 핵심적인 역할로 취급합니다. 예를 들어, \u0026ldquo;John is looking for his glass.\u0026ldquo;라는 문장이 \u0026lsquo;안경(glasses)\u0026lsquo;을 찾는 것인지 \u0026lsquo;유리잔(glass)\u0026lsquo;을 찾는 것인지를 판단하려면 문장 구조나 단어의 사전적 의미가 아니라, 발화가 이루어지는 맥락을 이해해야 합니다.\n화용론은 소리, 단어, 문장 구조 등 다른 모든 언어학적 요소를 통합하여 최종적으로 \u0026lsquo;의미 있는 소통\u0026rsquo;을 완성시키는 핵심입니다. 유창성의 최종 단계는 더 복잡한 문법이나 더 많은 단어를 아는 것이 아니라, 상황에 맞게 언어를 정교하고 효과적으로 사용하는 \u0026lsquo;화용적 능력(pragmatic competence)\u0026lsquo;을 갖추는 것입니다.\n마치며 이전에는 외국어를 단순 암기로 학습했지만 이 방법에 의문을 가졌습니다. 한국어는 새로운 단어를 봐도 그 의도를 추측할 수 있고, 쉽게 사용할 수 있지만, 영어 등 외국어의 경우 추측하는 방법을 몰라 새로운 케이스를 보면 항상 암기를 해야 했고, 시간이 지나 잊어버리면 같은 내용을 다시 공부해야 했습니다.\n이러한 차이를 근거로 제가 외국어를 비효율적으로 학습하고 있다고 판단했습니다. 언어는 각각의 규칙이 있기에, 우선 이를 이해하고 응용하면서 학습하는 것이 올바른 접근 방법이라 생각했습니다. 그리고 개인적으로도 언어의 규칙에 호기심이 생겨서 언어가 발전한 역사와, 언어에 담긴 철학 등을 더 깊게 알아보려고 합니다.\n언어학 페이지의 다음 포스트는 각 원리의 세부 내용을 보다 자세히 다루겠습니다. 이후에는 다른 언어와 한국어의 원리를 비교하면서 그 나라의 언어 규칙을 이해하려고 합니다.\n언어의 구조와 규칙 자체를 의식적으로 인지하고 분석하는 능력\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2025-07-06T14:03:00+09:00","permalink":"https://yeonhl.github.io/theories/linguistics/core-of-language/","title":"언어의 핵심 원리"}]